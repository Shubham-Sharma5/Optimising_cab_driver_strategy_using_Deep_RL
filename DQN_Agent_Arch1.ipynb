{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import itertools\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store tracked state and action pairs\n",
    "states_to_track = collections.defaultdict(dict)\n",
    "\n",
    "states = [(0,0,0), (1,0,0), (2,0,0), (3,0,0)]\n",
    "for state in states:\n",
    "    states_to_track[state] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0, 0, 0), (1, 0, 0), (2, 0, 0), (3, 0, 0)])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_track.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        #hyperparameters for the DQN\n",
    "        self.discount_factor = 0.9\n",
    "        self.learning_rate = 0.01       \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            layers.Dense(32, input_dim = self.state_size, activation = \"relu\"),\n",
    "            layers.Dense(32, activation = \"relu\"),\n",
    "            layers.Dense(self.action_size, activation = \"relu\")\n",
    "        ])\n",
    "        #Compile the Model        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state, time, possible_actions_index, possible_actions):\n",
    "        action_index = 0\n",
    "        epsilon = np.exp(-0.0009*time) #The value of epsilon will decay with each episode \n",
    "        #Picking random action\n",
    "        if np.random.random_sample() <= epsilon:\n",
    "            action_index = random.choice(possible_actions_index)\n",
    "        #Picking the action that maximizes the Q-value\n",
    "        else:\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1,self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            possible_q_values = [q_value[0][i] for i in possible_actions_index]\n",
    "            action_index = possible_actions_index[np.argmax(possible_q_values)]\n",
    "        \n",
    "        return action_index, epsilon\n",
    "     \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        if len(self.memory) == 2000:\n",
    "            self.memory.popleft() #Removing the first element everytime a new experience is added once the memory is full\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "    \n",
    "    # Function to train the model\n",
    "    def train_model(self):\n",
    "        \n",
    "        #Sampling only after the number of experiences in the memory exceed batch size\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action_index, reward, next_state, done_bool = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                actions.append(action_index)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_bool)\n",
    "                \n",
    "            # Write your code from here\n",
    "            #Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)\n",
    "\n",
    "            # Get the target for the Q-network\n",
    "            target_q_val = self.model.predict(update_output)\n",
    "            #Updating the target matrix\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor*np.max(target_q_val[i])\n",
    "\n",
    "                \n",
    "            # Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size = self.batch_size, epochs = 1, verbose = 0)\n",
    "    \n",
    "    #Method to save the q-values for tracked states\n",
    "    def save_tracking_states(self):\n",
    "        for state in states_to_track.keys():\n",
    "            # Use the model to predict the q_value of the state we are tacking.\n",
    "            state_vector = env.state_encod_arch1(state).reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state_vector)\n",
    "            # Grab the q_value of the action index that we are tracking.\n",
    "            states_to_track[state].append(q_value[0][0])\n",
    "    \n",
    "    #Saving the model\n",
    "    def save(self, name):\n",
    "        self.model.save(os.getcwd()+\"\\\\\"+name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 5000 #Number of episodes to run over\n",
    "episode_len = 24*30 #Number of hours in an episode assuming each episode ends after 30 days\n",
    "m = 5 #Locations\n",
    "t = 24 #Hours in a day\n",
    "d = 7 #Days in a week\n",
    "rewards_per_episode, episodes = [], [] #Lists to store the rewards per episode\n",
    "agent = DQNAgent(state_size, action_size) #Creating the QDNAgent class object\n",
    "epsilon = 0 #Initializing epsilon as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward 0, memory_length 145, epsilon 1.0, total time 733, loss [881.7510986328125], compute time 8.94\n",
      "Saving model for episode: 0\n",
      "episode 1, reward -312, memory_length 273, epsilon 0.9991, total time 721, loss [275.43267822265625], compute time 9.83\n",
      "episode 2, reward -90, memory_length 427, epsilon 0.9982, total time 733, loss [569.2400512695312], compute time 11.76\n",
      "episode 3, reward -93, memory_length 566, epsilon 0.9973, total time 724, loss [1577.4189453125], compute time 10.47\n",
      "episode 4, reward -388, memory_length 690, epsilon 0.99641, total time 722, loss [781.27783203125], compute time 9.56\n",
      "episode 5, reward 32, memory_length 836, epsilon 0.99551, total time 730, loss [1314.5621337890625], compute time 11.13\n",
      "episode 6, reward 145, memory_length 982, epsilon 0.99461, total time 729, loss [645.2808837890625], compute time 11.1\n",
      "episode 7, reward -56, memory_length 1124, epsilon 0.99372, total time 722, loss [761.8148193359375], compute time 10.81\n",
      "episode 8, reward -296, memory_length 1270, epsilon 0.99283, total time 726, loss [569.9566650390625], compute time 10.96\n",
      "episode 9, reward -132, memory_length 1391, epsilon 0.99193, total time 722, loss [556.0537109375], compute time 9.3\n",
      "episode 10, reward -390, memory_length 1526, epsilon 0.99104, total time 724, loss [788.199951171875], compute time 10.29\n",
      "Saving model for episode: 10\n",
      "episode 11, reward -179, memory_length 1674, epsilon 0.99015, total time 724, loss [827.411865234375], compute time 11.27\n",
      "episode 12, reward -225, memory_length 1822, epsilon 0.98926, total time 723, loss [485.6033630371094], compute time 11.31\n",
      "episode 13, reward -274, memory_length 1955, epsilon 0.98837, total time 724, loss [1027.3377685546875], compute time 10.31\n",
      "episode 14, reward -116, memory_length 2000, epsilon 0.98748, total time 722, loss [871.2315673828125], compute time 11.91\n",
      "episode 15, reward -128, memory_length 2000, epsilon 0.98659, total time 723, loss [1125.9063720703125], compute time 9.22\n",
      "episode 16, reward -156, memory_length 2000, epsilon 0.9857, total time 721, loss [978.3299560546875], compute time 10.5\n",
      "episode 17, reward -336, memory_length 2000, epsilon 0.98482, total time 723, loss [1025.0186767578125], compute time 11.88\n",
      "episode 18, reward -5, memory_length 2000, epsilon 0.98393, total time 727, loss [850.20166015625], compute time 10.88\n",
      "episode 19, reward -107, memory_length 2000, epsilon 0.98305, total time 728, loss [1982.6490478515625], compute time 10.37\n",
      "episode 20, reward -168, memory_length 2000, epsilon 0.98216, total time 723, loss [439.21441650390625], compute time 9.67\n",
      "Saving model for episode: 20\n",
      "episode 21, reward -375, memory_length 2000, epsilon 0.98128, total time 723, loss [972.5484619140625], compute time 10.86\n",
      "episode 22, reward -202, memory_length 2000, epsilon 0.98039, total time 721, loss [878.8073120117188], compute time 11.19\n",
      "episode 23, reward 16, memory_length 2000, epsilon 0.97951, total time 724, loss [1037.365234375], compute time 10.69\n",
      "episode 24, reward -36, memory_length 2000, epsilon 0.97863, total time 725, loss [1110.12060546875], compute time 10.61\n",
      "episode 25, reward 93, memory_length 2000, epsilon 0.97775, total time 725, loss [895.3207397460938], compute time 10.06\n",
      "episode 26, reward 118, memory_length 2000, epsilon 0.97687, total time 723, loss [804.6495971679688], compute time 10.97\n",
      "episode 27, reward 91, memory_length 2000, epsilon 0.97599, total time 724, loss [1477.18017578125], compute time 10.88\n",
      "episode 28, reward -237, memory_length 2000, epsilon 0.97511, total time 732, loss [957.5584716796875], compute time 10.65\n",
      "episode 29, reward -205, memory_length 2000, epsilon 0.97424, total time 722, loss [1414.77294921875], compute time 12.46\n",
      "episode 30, reward -254, memory_length 2000, epsilon 0.97336, total time 730, loss [491.79779052734375], compute time 12.17\n",
      "Saving model for episode: 30\n",
      "episode 31, reward -157, memory_length 2000, epsilon 0.97249, total time 732, loss [768.0924682617188], compute time 12.21\n",
      "episode 32, reward -184, memory_length 2000, epsilon 0.97161, total time 721, loss [1246.21337890625], compute time 9.71\n",
      "episode 33, reward -119, memory_length 2000, epsilon 0.97074, total time 724, loss [1185.641845703125], compute time 12.09\n",
      "episode 34, reward -45, memory_length 2000, epsilon 0.96986, total time 723, loss [1905.421142578125], compute time 10.82\n",
      "episode 35, reward -105, memory_length 2000, epsilon 0.96899, total time 722, loss [1491.4910888671875], compute time 12.66\n",
      "episode 36, reward 159, memory_length 2000, epsilon 0.96812, total time 728, loss [980.915771484375], compute time 10.09\n",
      "episode 37, reward 188, memory_length 2000, epsilon 0.96725, total time 721, loss [559.4577026367188], compute time 11.17\n",
      "episode 38, reward -80, memory_length 2000, epsilon 0.96638, total time 729, loss [1083.601806640625], compute time 11.9\n",
      "episode 39, reward -144, memory_length 2000, epsilon 0.96551, total time 721, loss [1003.4990844726562], compute time 10.32\n",
      "episode 40, reward -110, memory_length 2000, epsilon 0.96464, total time 723, loss [1297.3349609375], compute time 11.01\n",
      "Saving model for episode: 40\n",
      "episode 41, reward 211, memory_length 2000, epsilon 0.96377, total time 724, loss [628.0498046875], compute time 10.84\n",
      "episode 42, reward 197, memory_length 2000, epsilon 0.96291, total time 721, loss [1246.60986328125], compute time 10.66\n",
      "episode 43, reward -278, memory_length 2000, epsilon 0.96204, total time 733, loss [1157.2613525390625], compute time 12.17\n",
      "episode 44, reward 132, memory_length 2000, epsilon 0.96117, total time 724, loss [775.3242797851562], compute time 10.38\n",
      "episode 45, reward -28, memory_length 2000, epsilon 0.96031, total time 721, loss [559.335205078125], compute time 10.99\n",
      "episode 46, reward -144, memory_length 2000, epsilon 0.95945, total time 724, loss [617.464599609375], compute time 12.49\n",
      "episode 47, reward -73, memory_length 2000, epsilon 0.95858, total time 725, loss [738.2094116210938], compute time 13.0\n",
      "episode 48, reward -114, memory_length 2000, epsilon 0.95772, total time 726, loss [1055.832763671875], compute time 10.18\n",
      "episode 49, reward 99, memory_length 2000, epsilon 0.95686, total time 726, loss -, compute time 11.67\n",
      "episode 50, reward -47, memory_length 2000, epsilon 0.956, total time 722, loss [745.0234375], compute time 10.77\n",
      "Saving model for episode: 50\n",
      "episode 51, reward 5, memory_length 2000, epsilon 0.95514, total time 725, loss [866.7508544921875], compute time 10.72\n",
      "episode 52, reward -315, memory_length 2000, epsilon 0.95428, total time 726, loss [985.7837524414062], compute time 11.65\n",
      "episode 53, reward 306, memory_length 2000, epsilon 0.95342, total time 721, loss [1241.383544921875], compute time 10.87\n",
      "episode 54, reward -236, memory_length 2000, epsilon 0.95256, total time 723, loss [1115.951416015625], compute time 11.38\n",
      "episode 55, reward 32, memory_length 2000, epsilon 0.95171, total time 721, loss [1086.6514892578125], compute time 10.58\n",
      "episode 56, reward -325, memory_length 2000, epsilon 0.95085, total time 725, loss [1411.7989501953125], compute time 10.45\n",
      "episode 57, reward -281, memory_length 2000, epsilon 0.94999, total time 721, loss [1181.2239990234375], compute time 9.97\n",
      "episode 58, reward -231, memory_length 2000, epsilon 0.94914, total time 728, loss [960.8701171875], compute time 10.93\n",
      "episode 59, reward 171, memory_length 2000, epsilon 0.94829, total time 721, loss [556.5396728515625], compute time 10.99\n",
      "episode 60, reward 24, memory_length 2000, epsilon 0.94743, total time 728, loss [2020.1455078125], compute time 10.49\n",
      "Saving model for episode: 60\n",
      "episode 61, reward -84, memory_length 2000, epsilon 0.94658, total time 726, loss [1759.1419677734375], compute time 10.18\n",
      "episode 62, reward -227, memory_length 2000, epsilon 0.94573, total time 725, loss [1474.310791015625], compute time 11.19\n",
      "episode 63, reward -10, memory_length 2000, epsilon 0.94488, total time 725, loss [1076.0640869140625], compute time 10.36\n",
      "episode 64, reward -179, memory_length 2000, epsilon 0.94403, total time 721, loss [1120.0703125], compute time 10.11\n",
      "episode 65, reward -414, memory_length 2000, epsilon 0.94318, total time 732, loss [968.4951782226562], compute time 10.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 66, reward -196, memory_length 2000, epsilon 0.94233, total time 728, loss [958.240478515625], compute time 12.05\n",
      "episode 67, reward -274, memory_length 2000, epsilon 0.94148, total time 722, loss [997.9073486328125], compute time 10.03\n",
      "episode 68, reward -108, memory_length 2000, epsilon 0.94064, total time 724, loss [883.5355834960938], compute time 12.49\n",
      "episode 69, reward 244, memory_length 2000, epsilon 0.93979, total time 732, loss [1028.28515625], compute time 10.06\n",
      "episode 70, reward -20, memory_length 2000, epsilon 0.93894, total time 730, loss [1455.226806640625], compute time 11.69\n",
      "Saving model for episode: 70\n",
      "episode 71, reward -140, memory_length 2000, epsilon 0.9381, total time 725, loss [1134.7967529296875], compute time 11.9\n",
      "episode 72, reward -133, memory_length 2000, epsilon 0.93725, total time 726, loss -, compute time 12.71\n",
      "episode 73, reward 343, memory_length 2000, epsilon 0.93641, total time 722, loss [921.762939453125], compute time 12.02\n",
      "episode 74, reward -123, memory_length 2000, epsilon 0.93557, total time 725, loss [1450.6204833984375], compute time 11.45\n",
      "episode 75, reward -311, memory_length 2000, epsilon 0.93473, total time 722, loss [927.629150390625], compute time 10.12\n",
      "episode 76, reward -266, memory_length 2000, epsilon 0.93389, total time 723, loss [1392.603271484375], compute time 11.28\n",
      "episode 77, reward 46, memory_length 2000, epsilon 0.93305, total time 721, loss [1718.2833251953125], compute time 10.65\n",
      "episode 78, reward -224, memory_length 2000, epsilon 0.93221, total time 724, loss [1156.21240234375], compute time 12.01\n",
      "episode 79, reward -92, memory_length 2000, epsilon 0.93137, total time 728, loss [873.7388916015625], compute time 11.38\n",
      "episode 80, reward -390, memory_length 2000, epsilon 0.93053, total time 729, loss [1041.4261474609375], compute time 11.27\n",
      "Saving model for episode: 80\n",
      "episode 81, reward -100, memory_length 2000, epsilon 0.92969, total time 725, loss [1035.7451171875], compute time 12.14\n",
      "episode 82, reward -197, memory_length 2000, epsilon 0.92886, total time 722, loss [718.2095947265625], compute time 12.87\n",
      "episode 83, reward -427, memory_length 2000, epsilon 0.92802, total time 729, loss [1437.6185302734375], compute time 12.42\n",
      "episode 84, reward 55, memory_length 2000, epsilon 0.92719, total time 723, loss [893.6163330078125], compute time 10.53\n",
      "episode 85, reward 164, memory_length 2000, epsilon 0.92635, total time 729, loss [805.137939453125], compute time 10.62\n",
      "episode 86, reward -98, memory_length 2000, epsilon 0.92552, total time 721, loss [879.0578002929688], compute time 10.71\n",
      "episode 87, reward 126, memory_length 2000, epsilon 0.92469, total time 723, loss [721.330078125], compute time 10.82\n",
      "episode 88, reward 16, memory_length 2000, epsilon 0.92386, total time 733, loss [887.9813232421875], compute time 12.39\n",
      "episode 89, reward 19, memory_length 2000, epsilon 0.92302, total time 721, loss -, compute time 12.6\n",
      "episode 90, reward -245, memory_length 2000, epsilon 0.92219, total time 724, loss -, compute time 11.98\n",
      "Saving model for episode: 90\n",
      "episode 91, reward -126, memory_length 2000, epsilon 0.92136, total time 728, loss [859.6104125976562], compute time 10.99\n",
      "episode 92, reward -80, memory_length 2000, epsilon 0.92054, total time 723, loss [1145.4149169921875], compute time 11.46\n",
      "episode 93, reward 83, memory_length 2000, epsilon 0.91971, total time 721, loss [774.5675048828125], compute time 11.55\n",
      "episode 94, reward -241, memory_length 2000, epsilon 0.91888, total time 729, loss [955.1032104492188], compute time 10.96\n",
      "episode 95, reward 127, memory_length 2000, epsilon 0.91805, total time 724, loss [1036.784423828125], compute time 11.08\n",
      "episode 96, reward -139, memory_length 2000, epsilon 0.91723, total time 722, loss [917.9851684570312], compute time 11.4\n",
      "episode 97, reward -305, memory_length 2000, epsilon 0.9164, total time 726, loss -, compute time 11.88\n",
      "episode 98, reward -126, memory_length 2000, epsilon 0.91558, total time 726, loss [648.49853515625], compute time 11.09\n",
      "episode 99, reward -119, memory_length 2000, epsilon 0.91475, total time 722, loss [769.760986328125], compute time 11.79\n",
      "episode 100, reward 95, memory_length 2000, epsilon 0.91393, total time 721, loss [1560.0670166015625], compute time 11.11\n",
      "Saving model for episode: 100\n",
      "episode 101, reward -166, memory_length 2000, epsilon 0.91311, total time 722, loss [997.8468017578125], compute time 12.43\n",
      "episode 102, reward 5, memory_length 2000, epsilon 0.91229, total time 721, loss -, compute time 12.1\n",
      "episode 103, reward -53, memory_length 2000, epsilon 0.91147, total time 721, loss [1286.6572265625], compute time 10.26\n",
      "episode 104, reward -225, memory_length 2000, epsilon 0.91065, total time 736, loss [615.6572265625], compute time 10.86\n",
      "episode 105, reward 108, memory_length 2000, epsilon 0.90983, total time 722, loss [826.9715576171875], compute time 10.52\n",
      "episode 106, reward 23, memory_length 2000, epsilon 0.90901, total time 727, loss [1013.054443359375], compute time 10.07\n",
      "episode 107, reward -92, memory_length 2000, epsilon 0.90819, total time 729, loss [1144.756103515625], compute time 10.88\n",
      "episode 108, reward 71, memory_length 2000, epsilon 0.90737, total time 726, loss [1426.6983642578125], compute time 10.45\n",
      "episode 109, reward -45, memory_length 2000, epsilon 0.90656, total time 723, loss [1033.002685546875], compute time 9.59\n",
      "episode 110, reward -125, memory_length 2000, epsilon 0.90574, total time 728, loss [647.168701171875], compute time 10.11\n",
      "Saving model for episode: 110\n",
      "episode 111, reward 217, memory_length 2000, epsilon 0.90493, total time 731, loss -, compute time 10.2\n",
      "episode 112, reward 61, memory_length 2000, epsilon 0.90411, total time 725, loss [1534.49072265625], compute time 11.1\n",
      "episode 113, reward 54, memory_length 2000, epsilon 0.9033, total time 725, loss [847.3433227539062], compute time 10.53\n",
      "episode 114, reward -145, memory_length 2000, epsilon 0.90249, total time 728, loss [987.5657958984375], compute time 10.43\n",
      "episode 115, reward -217, memory_length 2000, epsilon 0.90168, total time 726, loss [1089.900634765625], compute time 12.07\n",
      "episode 116, reward -195, memory_length 2000, epsilon 0.90086, total time 726, loss [866.88427734375], compute time 11.53\n",
      "episode 117, reward 37, memory_length 2000, epsilon 0.90005, total time 725, loss [1083.8095703125], compute time 11.25\n",
      "episode 118, reward -151, memory_length 2000, epsilon 0.89924, total time 724, loss [1125.772705078125], compute time 10.7\n",
      "episode 119, reward 144, memory_length 2000, epsilon 0.89844, total time 721, loss [1173.319091796875], compute time 10.86\n",
      "episode 120, reward 72, memory_length 2000, epsilon 0.89763, total time 723, loss [932.3079223632812], compute time 12.26\n",
      "Saving model for episode: 120\n",
      "episode 121, reward 204, memory_length 2000, epsilon 0.89682, total time 721, loss [1327.36376953125], compute time 10.96\n",
      "episode 122, reward -48, memory_length 2000, epsilon 0.89601, total time 723, loss [969.4365234375], compute time 11.2\n",
      "episode 123, reward -108, memory_length 2000, epsilon 0.89521, total time 722, loss [1628.013916015625], compute time 10.64\n",
      "episode 124, reward 392, memory_length 2000, epsilon 0.8944, total time 728, loss [1020.14794921875], compute time 10.15\n",
      "episode 125, reward 128, memory_length 2000, epsilon 0.8936, total time 727, loss -, compute time 11.93\n",
      "episode 126, reward -223, memory_length 2000, epsilon 0.89279, total time 727, loss [1235.7032470703125], compute time 12.16\n",
      "episode 127, reward 122, memory_length 2000, epsilon 0.89199, total time 733, loss -, compute time 11.12\n",
      "episode 128, reward 199, memory_length 2000, epsilon 0.89119, total time 721, loss [1644.57763671875], compute time 11.41\n",
      "episode 129, reward 195, memory_length 2000, epsilon 0.89039, total time 726, loss [1346.552734375], compute time 11.81\n",
      "episode 130, reward -83, memory_length 2000, epsilon 0.88959, total time 728, loss [866.410888671875], compute time 10.96\n",
      "Saving model for episode: 130\n",
      "episode 131, reward -21, memory_length 2000, epsilon 0.88878, total time 726, loss [1144.179443359375], compute time 10.4\n",
      "episode 132, reward 279, memory_length 2000, epsilon 0.88799, total time 726, loss [1322.9617919921875], compute time 12.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 133, reward 100, memory_length 2000, epsilon 0.88719, total time 725, loss [1113.806396484375], compute time 12.26\n",
      "episode 134, reward -11, memory_length 2000, epsilon 0.88639, total time 723, loss [890.3985595703125], compute time 11.71\n",
      "episode 135, reward 201, memory_length 2000, epsilon 0.88559, total time 722, loss [545.4559326171875], compute time 11.64\n",
      "episode 136, reward -76, memory_length 2000, epsilon 0.88479, total time 727, loss [788.1127319335938], compute time 12.63\n",
      "episode 137, reward -445, memory_length 2000, epsilon 0.884, total time 722, loss [982.9224853515625], compute time 12.42\n",
      "episode 138, reward 99, memory_length 2000, epsilon 0.8832, total time 731, loss [681.645263671875], compute time 10.48\n",
      "episode 139, reward 77, memory_length 2000, epsilon 0.88241, total time 727, loss [1049.560302734375], compute time 11.12\n",
      "episode 140, reward -143, memory_length 2000, epsilon 0.88161, total time 726, loss [1817.3416748046875], compute time 13.77\n",
      "Saving model for episode: 140\n",
      "episode 141, reward -40, memory_length 2000, epsilon 0.88082, total time 723, loss [1109.9912109375], compute time 10.2\n",
      "episode 142, reward 166, memory_length 2000, epsilon 0.88003, total time 723, loss [1175.859619140625], compute time 11.69\n",
      "episode 143, reward 318, memory_length 2000, epsilon 0.87924, total time 725, loss [945.4271240234375], compute time 10.99\n",
      "episode 144, reward 108, memory_length 2000, epsilon 0.87845, total time 722, loss [1072.216552734375], compute time 11.06\n",
      "episode 145, reward 111, memory_length 2000, epsilon 0.87766, total time 724, loss [1551.9556884765625], compute time 10.93\n",
      "episode 146, reward -2, memory_length 2000, epsilon 0.87687, total time 725, loss -, compute time 10.33\n",
      "episode 147, reward -94, memory_length 2000, epsilon 0.87608, total time 722, loss [1062.92529296875], compute time 10.15\n",
      "episode 148, reward 259, memory_length 2000, epsilon 0.87529, total time 722, loss -, compute time 11.43\n",
      "episode 149, reward -79, memory_length 2000, epsilon 0.8745, total time 722, loss [1198.65087890625], compute time 12.16\n",
      "episode 150, reward 95, memory_length 2000, epsilon 0.87372, total time 734, loss [994.1024169921875], compute time 13.17\n",
      "Saving model for episode: 150\n",
      "episode 151, reward -162, memory_length 2000, epsilon 0.87293, total time 726, loss [480.3838195800781], compute time 11.55\n",
      "episode 152, reward -94, memory_length 2000, epsilon 0.87214, total time 731, loss [935.7371826171875], compute time 9.62\n",
      "episode 153, reward 285, memory_length 2000, epsilon 0.87136, total time 728, loss [837.3843383789062], compute time 9.88\n",
      "episode 154, reward 297, memory_length 2000, epsilon 0.87058, total time 726, loss -, compute time 9.98\n",
      "episode 155, reward -45, memory_length 2000, epsilon 0.86979, total time 731, loss -, compute time 11.04\n",
      "episode 156, reward 306, memory_length 2000, epsilon 0.86901, total time 725, loss [1413.1422119140625], compute time 10.68\n",
      "episode 157, reward 396, memory_length 2000, epsilon 0.86823, total time 723, loss [1256.4295654296875], compute time 11.56\n",
      "episode 158, reward -102, memory_length 2000, epsilon 0.86745, total time 722, loss [601.8743286132812], compute time 12.52\n",
      "episode 159, reward -3, memory_length 2000, epsilon 0.86667, total time 726, loss [890.5008544921875], compute time 12.04\n",
      "episode 160, reward -155, memory_length 2000, epsilon 0.86589, total time 723, loss [830.762939453125], compute time 9.96\n",
      "Saving model for episode: 160\n",
      "episode 161, reward -67, memory_length 2000, epsilon 0.86511, total time 730, loss [640.923828125], compute time 12.35\n",
      "episode 162, reward 18, memory_length 2000, epsilon 0.86433, total time 723, loss [833.068359375], compute time 10.92\n",
      "episode 163, reward -10, memory_length 2000, epsilon 0.86355, total time 721, loss [665.70361328125], compute time 10.99\n",
      "episode 164, reward -13, memory_length 2000, epsilon 0.86278, total time 728, loss [726.1742553710938], compute time 11.54\n",
      "episode 165, reward 111, memory_length 2000, epsilon 0.862, total time 724, loss [1000.545654296875], compute time 10.82\n",
      "episode 166, reward 90, memory_length 2000, epsilon 0.86122, total time 725, loss [691.704345703125], compute time 11.9\n",
      "episode 167, reward -362, memory_length 2000, epsilon 0.86045, total time 727, loss [866.9716186523438], compute time 11.5\n",
      "episode 168, reward -252, memory_length 2000, epsilon 0.85968, total time 721, loss [797.2694091796875], compute time 11.12\n",
      "episode 169, reward 200, memory_length 2000, epsilon 0.8589, total time 723, loss [933.4729614257812], compute time 10.81\n",
      "episode 170, reward 96, memory_length 2000, epsilon 0.85813, total time 721, loss -, compute time 11.28\n",
      "Saving model for episode: 170\n",
      "episode 171, reward -27, memory_length 2000, epsilon 0.85736, total time 721, loss [1199.618408203125], compute time 10.81\n",
      "episode 172, reward 270, memory_length 2000, epsilon 0.85659, total time 728, loss [1149.2161865234375], compute time 10.64\n",
      "episode 173, reward -79, memory_length 2000, epsilon 0.85582, total time 725, loss [1592.5682373046875], compute time 11.11\n",
      "episode 174, reward -486, memory_length 2000, epsilon 0.85505, total time 724, loss -, compute time 11.66\n",
      "episode 175, reward -9, memory_length 2000, epsilon 0.85428, total time 728, loss [941.3373413085938], compute time 11.72\n",
      "episode 176, reward 115, memory_length 2000, epsilon 0.85351, total time 732, loss [752.7923583984375], compute time 12.4\n",
      "episode 177, reward 125, memory_length 2000, epsilon 0.85274, total time 722, loss [1220.3033447265625], compute time 12.48\n",
      "episode 178, reward 75, memory_length 2000, epsilon 0.85197, total time 722, loss -, compute time 10.55\n",
      "episode 179, reward 70, memory_length 2000, epsilon 0.85121, total time 723, loss -, compute time 11.41\n",
      "episode 180, reward 263, memory_length 2000, epsilon 0.85044, total time 725, loss -, compute time 10.71\n",
      "Saving model for episode: 180\n",
      "episode 181, reward 243, memory_length 2000, epsilon 0.84968, total time 721, loss [1012.8984375], compute time 12.24\n",
      "episode 182, reward -35, memory_length 2000, epsilon 0.84891, total time 727, loss [1392.50341796875], compute time 11.18\n",
      "episode 183, reward 179, memory_length 2000, epsilon 0.84815, total time 723, loss [844.226318359375], compute time 11.15\n",
      "episode 184, reward 391, memory_length 2000, epsilon 0.84739, total time 721, loss [820.387451171875], compute time 11.63\n",
      "episode 185, reward 24, memory_length 2000, epsilon 0.84662, total time 723, loss -, compute time 11.85\n",
      "episode 186, reward -19, memory_length 2000, epsilon 0.84586, total time 721, loss [1539.0751953125], compute time 10.67\n",
      "episode 187, reward -54, memory_length 2000, epsilon 0.8451, total time 721, loss [855.49169921875], compute time 11.02\n",
      "episode 188, reward 210, memory_length 2000, epsilon 0.84434, total time 723, loss -, compute time 10.63\n",
      "episode 189, reward 172, memory_length 2000, epsilon 0.84358, total time 726, loss [845.053466796875], compute time 11.23\n",
      "episode 190, reward 226, memory_length 2000, epsilon 0.84282, total time 729, loss [790.3587646484375], compute time 11.12\n",
      "Saving model for episode: 190\n",
      "episode 191, reward 270, memory_length 2000, epsilon 0.84206, total time 726, loss -, compute time 11.18\n",
      "episode 192, reward 37, memory_length 2000, epsilon 0.84131, total time 723, loss [1075.146484375], compute time 11.63\n",
      "episode 193, reward 109, memory_length 2000, epsilon 0.84055, total time 727, loss [1338.185791015625], compute time 12.34\n",
      "episode 194, reward 19, memory_length 2000, epsilon 0.83979, total time 722, loss [1032.75634765625], compute time 10.83\n",
      "episode 195, reward -189, memory_length 2000, epsilon 0.83904, total time 722, loss [760.6861572265625], compute time 10.78\n",
      "episode 196, reward -20, memory_length 2000, epsilon 0.83828, total time 721, loss [1249.2442626953125], compute time 11.89\n",
      "episode 197, reward 75, memory_length 2000, epsilon 0.83753, total time 728, loss [683.0322265625], compute time 10.15\n",
      "episode 198, reward 461, memory_length 2000, epsilon 0.83678, total time 727, loss [1176.40283203125], compute time 10.57\n",
      "episode 199, reward 213, memory_length 2000, epsilon 0.83602, total time 723, loss [992.530029296875], compute time 11.87\n",
      "episode 200, reward 252, memory_length 2000, epsilon 0.83527, total time 721, loss [1048.397216796875], compute time 10.48\n",
      "Saving model for episode: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 201, reward -7, memory_length 2000, epsilon 0.83452, total time 725, loss [1337.9248046875], compute time 9.63\n",
      "episode 202, reward 124, memory_length 2000, epsilon 0.83377, total time 722, loss [1458.678466796875], compute time 12.33\n",
      "episode 203, reward -201, memory_length 2000, epsilon 0.83302, total time 725, loss [1483.322265625], compute time 11.38\n",
      "episode 204, reward -30, memory_length 2000, epsilon 0.83227, total time 729, loss [956.483154296875], compute time 12.11\n",
      "episode 205, reward 131, memory_length 2000, epsilon 0.83152, total time 723, loss [790.513916015625], compute time 11.33\n",
      "episode 206, reward 212, memory_length 2000, epsilon 0.83077, total time 725, loss [1339.0045166015625], compute time 11.52\n",
      "episode 207, reward 180, memory_length 2000, epsilon 0.83002, total time 721, loss [1201.6221923828125], compute time 11.18\n",
      "episode 208, reward 272, memory_length 2000, epsilon 0.82928, total time 728, loss [898.0107421875], compute time 12.14\n",
      "episode 209, reward -85, memory_length 2000, epsilon 0.82853, total time 732, loss [579.578369140625], compute time 10.99\n",
      "episode 210, reward -81, memory_length 2000, epsilon 0.82779, total time 726, loss -, compute time 10.34\n",
      "Saving model for episode: 210\n",
      "episode 211, reward -13, memory_length 2000, epsilon 0.82704, total time 727, loss [861.08984375], compute time 10.72\n",
      "episode 212, reward 131, memory_length 2000, epsilon 0.8263, total time 722, loss [1090.1475830078125], compute time 10.55\n",
      "episode 213, reward 178, memory_length 2000, epsilon 0.82555, total time 723, loss [654.7970581054688], compute time 12.77\n",
      "episode 214, reward 341, memory_length 2000, epsilon 0.82481, total time 722, loss [2051.898193359375], compute time 10.47\n",
      "episode 215, reward 216, memory_length 2000, epsilon 0.82407, total time 722, loss [953.483154296875], compute time 11.58\n",
      "episode 216, reward 0, memory_length 2000, epsilon 0.82333, total time 724, loss -, compute time 10.95\n",
      "episode 217, reward -288, memory_length 2000, epsilon 0.82259, total time 721, loss [986.0904541015625], compute time 11.9\n",
      "episode 218, reward 90, memory_length 2000, epsilon 0.82185, total time 724, loss [1106.3017578125], compute time 10.27\n",
      "episode 219, reward 140, memory_length 2000, epsilon 0.82111, total time 723, loss [974.1170043945312], compute time 10.82\n",
      "episode 220, reward 266, memory_length 2000, epsilon 0.82037, total time 728, loss -, compute time 12.14\n",
      "Saving model for episode: 220\n",
      "episode 221, reward -8, memory_length 2000, epsilon 0.81963, total time 722, loss [1257.2314453125], compute time 11.63\n",
      "episode 222, reward 144, memory_length 2000, epsilon 0.81889, total time 728, loss [1565.7685546875], compute time 11.09\n",
      "episode 223, reward -18, memory_length 2000, epsilon 0.81816, total time 730, loss [892.3787231445312], compute time 10.85\n",
      "episode 224, reward 564, memory_length 2000, epsilon 0.81742, total time 727, loss [1108.1629638671875], compute time 11.7\n",
      "episode 225, reward 305, memory_length 2000, epsilon 0.81669, total time 730, loss [1315.888671875], compute time 10.11\n",
      "episode 226, reward 299, memory_length 2000, epsilon 0.81595, total time 725, loss -, compute time 13.14\n",
      "episode 227, reward 433, memory_length 2000, epsilon 0.81522, total time 732, loss -, compute time 10.42\n",
      "episode 228, reward 231, memory_length 2000, epsilon 0.81448, total time 722, loss [1608.8916015625], compute time 10.76\n",
      "episode 229, reward 36, memory_length 2000, epsilon 0.81375, total time 726, loss -, compute time 11.83\n",
      "episode 230, reward 248, memory_length 2000, epsilon 0.81302, total time 722, loss [769.0643920898438], compute time 10.39\n",
      "Saving model for episode: 230\n",
      "episode 231, reward 282, memory_length 2000, epsilon 0.81229, total time 722, loss -, compute time 10.69\n",
      "episode 232, reward 182, memory_length 2000, epsilon 0.81156, total time 721, loss [1215.80224609375], compute time 13.59\n",
      "episode 233, reward 55, memory_length 2000, epsilon 0.81083, total time 722, loss -, compute time 11.43\n",
      "episode 234, reward 495, memory_length 2000, epsilon 0.8101, total time 729, loss [770.8413696289062], compute time 10.52\n",
      "episode 235, reward -18, memory_length 2000, epsilon 0.80937, total time 722, loss [1524.3795166015625], compute time 12.53\n",
      "episode 236, reward -224, memory_length 2000, epsilon 0.80864, total time 721, loss [648.3325805664062], compute time 10.88\n",
      "episode 237, reward 65, memory_length 2000, epsilon 0.80791, total time 723, loss [1386.458251953125], compute time 10.97\n",
      "episode 238, reward -210, memory_length 2000, epsilon 0.80719, total time 721, loss [621.7156372070312], compute time 10.07\n",
      "episode 239, reward 159, memory_length 2000, epsilon 0.80646, total time 721, loss -, compute time 11.69\n",
      "episode 240, reward 204, memory_length 2000, epsilon 0.80574, total time 724, loss [474.9052429199219], compute time 11.9\n",
      "Saving model for episode: 240\n",
      "episode 241, reward 83, memory_length 2000, epsilon 0.80501, total time 728, loss [1015.6466064453125], compute time 11.74\n",
      "episode 242, reward 345, memory_length 2000, epsilon 0.80429, total time 726, loss [1063.486572265625], compute time 11.06\n",
      "episode 243, reward 261, memory_length 2000, epsilon 0.80356, total time 726, loss [764.6312255859375], compute time 13.2\n",
      "episode 244, reward 331, memory_length 2000, epsilon 0.80284, total time 724, loss [1063.731201171875], compute time 10.95\n",
      "episode 245, reward 127, memory_length 2000, epsilon 0.80212, total time 726, loss [729.1444702148438], compute time 10.98\n",
      "episode 246, reward 370, memory_length 2000, epsilon 0.8014, total time 722, loss [844.429443359375], compute time 10.79\n",
      "episode 247, reward 306, memory_length 2000, epsilon 0.80068, total time 721, loss [694.32763671875], compute time 10.93\n",
      "episode 248, reward 99, memory_length 2000, epsilon 0.79995, total time 729, loss [517.0340576171875], compute time 11.71\n",
      "episode 249, reward 295, memory_length 2000, epsilon 0.79924, total time 726, loss [1048.7564697265625], compute time 12.04\n",
      "episode 250, reward 135, memory_length 2000, epsilon 0.79852, total time 725, loss -, compute time 9.96\n",
      "Saving model for episode: 250\n",
      "episode 251, reward 30, memory_length 2000, epsilon 0.7978, total time 723, loss [845.4923095703125], compute time 10.49\n",
      "episode 252, reward 381, memory_length 2000, epsilon 0.79708, total time 724, loss [777.2293090820312], compute time 11.44\n",
      "episode 253, reward 511, memory_length 2000, epsilon 0.79636, total time 724, loss [656.8790283203125], compute time 11.02\n",
      "episode 254, reward -62, memory_length 2000, epsilon 0.79565, total time 725, loss -, compute time 11.4\n",
      "episode 255, reward 109, memory_length 2000, epsilon 0.79493, total time 722, loss [723.577880859375], compute time 10.77\n",
      "episode 256, reward 301, memory_length 2000, epsilon 0.79422, total time 722, loss [899.4913330078125], compute time 10.87\n",
      "episode 257, reward -294, memory_length 2000, epsilon 0.7935, total time 722, loss [1032.321533203125], compute time 11.93\n",
      "episode 258, reward 52, memory_length 2000, epsilon 0.79279, total time 729, loss [416.19586181640625], compute time 12.4\n",
      "episode 259, reward 105, memory_length 2000, epsilon 0.79207, total time 723, loss [1134.91259765625], compute time 11.45\n",
      "episode 260, reward -22, memory_length 2000, epsilon 0.79136, total time 732, loss [741.18408203125], compute time 11.38\n",
      "Saving model for episode: 260\n",
      "episode 261, reward 32, memory_length 2000, epsilon 0.79065, total time 728, loss [768.584228515625], compute time 10.16\n",
      "episode 262, reward 270, memory_length 2000, epsilon 0.78994, total time 729, loss [985.1133422851562], compute time 10.5\n",
      "episode 263, reward 140, memory_length 2000, epsilon 0.78923, total time 722, loss [782.3148193359375], compute time 10.47\n",
      "episode 264, reward 182, memory_length 2000, epsilon 0.78852, total time 722, loss [961.8946533203125], compute time 10.4\n",
      "episode 265, reward 171, memory_length 2000, epsilon 0.78781, total time 721, loss [1305.143310546875], compute time 9.73\n",
      "episode 266, reward 167, memory_length 2000, epsilon 0.7871, total time 732, loss -, compute time 10.94\n",
      "episode 267, reward 153, memory_length 2000, epsilon 0.78639, total time 727, loss [1031.37939453125], compute time 10.09\n",
      "episode 268, reward 549, memory_length 2000, epsilon 0.78568, total time 723, loss [932.6629638671875], compute time 11.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 269, reward -83, memory_length 2000, epsilon 0.78498, total time 721, loss [949.195556640625], compute time 11.47\n",
      "episode 270, reward 280, memory_length 2000, epsilon 0.78427, total time 724, loss -, compute time 10.41\n",
      "Saving model for episode: 270\n",
      "episode 271, reward 304, memory_length 2000, epsilon 0.78357, total time 722, loss [1627.54052734375], compute time 10.17\n",
      "episode 272, reward 58, memory_length 2000, epsilon 0.78286, total time 721, loss [966.070556640625], compute time 10.8\n",
      "episode 273, reward -250, memory_length 2000, epsilon 0.78216, total time 722, loss [1188.38134765625], compute time 10.38\n",
      "episode 274, reward 167, memory_length 2000, epsilon 0.78145, total time 725, loss [933.5184936523438], compute time 10.95\n",
      "episode 275, reward 108, memory_length 2000, epsilon 0.78075, total time 729, loss [511.847900390625], compute time 10.96\n",
      "episode 276, reward 65, memory_length 2000, epsilon 0.78005, total time 722, loss [430.37384033203125], compute time 10.85\n",
      "episode 277, reward -46, memory_length 2000, epsilon 0.77935, total time 721, loss -, compute time 11.0\n",
      "episode 278, reward 234, memory_length 2000, epsilon 0.77865, total time 722, loss [698.3187255859375], compute time 10.56\n",
      "episode 279, reward 74, memory_length 2000, epsilon 0.77794, total time 722, loss [876.2867431640625], compute time 10.53\n",
      "episode 280, reward 109, memory_length 2000, epsilon 0.77724, total time 735, loss [796.05810546875], compute time 10.91\n",
      "Saving model for episode: 280\n",
      "episode 281, reward 459, memory_length 2000, epsilon 0.77655, total time 725, loss [932.9181518554688], compute time 10.86\n",
      "episode 282, reward 366, memory_length 2000, epsilon 0.77585, total time 721, loss [678.6787719726562], compute time 12.03\n",
      "episode 283, reward 81, memory_length 2000, epsilon 0.77515, total time 730, loss [1266.13720703125], compute time 11.36\n",
      "episode 284, reward -205, memory_length 2000, epsilon 0.77445, total time 731, loss [945.428466796875], compute time 10.19\n",
      "episode 285, reward 99, memory_length 2000, epsilon 0.77375, total time 721, loss [906.1751098632812], compute time 11.35\n",
      "episode 286, reward 93, memory_length 2000, epsilon 0.77306, total time 722, loss [938.7169189453125], compute time 11.11\n",
      "episode 287, reward 282, memory_length 2000, epsilon 0.77236, total time 724, loss -, compute time 10.67\n",
      "episode 288, reward 303, memory_length 2000, epsilon 0.77167, total time 721, loss -, compute time 10.92\n",
      "episode 289, reward 453, memory_length 2000, epsilon 0.77097, total time 727, loss -, compute time 11.3\n",
      "episode 290, reward 334, memory_length 2000, epsilon 0.77028, total time 722, loss [1275.331298828125], compute time 10.62\n",
      "Saving model for episode: 290\n",
      "episode 291, reward 206, memory_length 2000, epsilon 0.76959, total time 722, loss -, compute time 11.8\n",
      "episode 292, reward 141, memory_length 2000, epsilon 0.7689, total time 725, loss [505.1141357421875], compute time 11.14\n",
      "episode 293, reward 492, memory_length 2000, epsilon 0.7682, total time 725, loss -, compute time 12.26\n",
      "episode 294, reward 369, memory_length 2000, epsilon 0.76751, total time 723, loss -, compute time 12.22\n",
      "episode 295, reward 414, memory_length 2000, epsilon 0.76682, total time 741, loss [1272.3359375], compute time 11.96\n",
      "episode 296, reward 774, memory_length 2000, epsilon 0.76613, total time 722, loss [959.1800537109375], compute time 10.2\n",
      "episode 297, reward 342, memory_length 2000, epsilon 0.76544, total time 721, loss [515.833984375], compute time 11.29\n",
      "episode 298, reward -147, memory_length 2000, epsilon 0.76475, total time 722, loss [798.0011596679688], compute time 11.68\n",
      "episode 299, reward 166, memory_length 2000, epsilon 0.76407, total time 726, loss [1678.23779296875], compute time 10.96\n",
      "episode 300, reward 225, memory_length 2000, epsilon 0.76338, total time 731, loss [1043.6563720703125], compute time 10.89\n",
      "Saving model for episode: 300\n",
      "episode 301, reward 16, memory_length 2000, epsilon 0.76269, total time 724, loss [1594.9017333984375], compute time 11.7\n",
      "episode 302, reward -22, memory_length 2000, epsilon 0.76201, total time 726, loss [531.2210693359375], compute time 11.74\n",
      "episode 303, reward 104, memory_length 2000, epsilon 0.76132, total time 725, loss [829.9588012695312], compute time 13.07\n",
      "episode 304, reward 345, memory_length 2000, epsilon 0.76064, total time 722, loss [973.7880859375], compute time 10.59\n",
      "episode 305, reward 153, memory_length 2000, epsilon 0.75995, total time 723, loss [899.4093017578125], compute time 12.08\n",
      "episode 306, reward 396, memory_length 2000, epsilon 0.75927, total time 727, loss [768.947265625], compute time 10.18\n",
      "episode 307, reward 205, memory_length 2000, epsilon 0.75859, total time 724, loss [1073.790283203125], compute time 10.32\n",
      "episode 308, reward 81, memory_length 2000, epsilon 0.7579, total time 726, loss -, compute time 10.39\n",
      "episode 309, reward 394, memory_length 2000, epsilon 0.75722, total time 721, loss [1210.887451171875], compute time 11.69\n",
      "episode 310, reward 265, memory_length 2000, epsilon 0.75654, total time 723, loss -, compute time 11.42\n",
      "Saving model for episode: 310\n",
      "episode 311, reward 137, memory_length 2000, epsilon 0.75586, total time 724, loss -, compute time 12.42\n",
      "episode 312, reward 33, memory_length 2000, epsilon 0.75518, total time 732, loss [1251.33740234375], compute time 10.97\n",
      "episode 313, reward 507, memory_length 2000, epsilon 0.7545, total time 722, loss -, compute time 11.46\n",
      "episode 314, reward 291, memory_length 2000, epsilon 0.75382, total time 729, loss [952.7861328125], compute time 11.23\n",
      "episode 315, reward 128, memory_length 2000, epsilon 0.75314, total time 727, loss [927.474853515625], compute time 11.54\n",
      "episode 316, reward 627, memory_length 2000, epsilon 0.75247, total time 723, loss [1092.2720947265625], compute time 11.36\n",
      "episode 317, reward -48, memory_length 2000, epsilon 0.75179, total time 721, loss -, compute time 12.35\n",
      "episode 318, reward 60, memory_length 2000, epsilon 0.75111, total time 722, loss [858.6761474609375], compute time 10.27\n",
      "episode 319, reward 321, memory_length 2000, epsilon 0.75044, total time 725, loss [842.2825927734375], compute time 12.53\n",
      "episode 320, reward 139, memory_length 2000, epsilon 0.74976, total time 721, loss -, compute time 11.32\n",
      "Saving model for episode: 320\n",
      "episode 321, reward 118, memory_length 2000, epsilon 0.74909, total time 723, loss [622.1578369140625], compute time 11.15\n",
      "episode 322, reward 154, memory_length 2000, epsilon 0.74841, total time 732, loss [539.4639892578125], compute time 10.03\n",
      "episode 323, reward 344, memory_length 2000, epsilon 0.74774, total time 721, loss -, compute time 11.8\n",
      "episode 324, reward 285, memory_length 2000, epsilon 0.74707, total time 726, loss -, compute time 11.43\n",
      "episode 325, reward 264, memory_length 2000, epsilon 0.7464, total time 724, loss -, compute time 11.97\n",
      "episode 326, reward 55, memory_length 2000, epsilon 0.74572, total time 722, loss -, compute time 9.71\n",
      "episode 327, reward 4, memory_length 2000, epsilon 0.74505, total time 721, loss [1259.57275390625], compute time 9.85\n",
      "episode 328, reward 190, memory_length 2000, epsilon 0.74438, total time 728, loss -, compute time 10.11\n",
      "episode 329, reward 307, memory_length 2000, epsilon 0.74371, total time 728, loss [1122.781005859375], compute time 10.93\n",
      "episode 330, reward 326, memory_length 2000, epsilon 0.74304, total time 724, loss [1237.8704833984375], compute time 9.97\n",
      "Saving model for episode: 330\n",
      "episode 331, reward 47, memory_length 2000, epsilon 0.74238, total time 721, loss [843.8935546875], compute time 11.38\n",
      "episode 332, reward 383, memory_length 2000, epsilon 0.74171, total time 723, loss -, compute time 12.32\n",
      "episode 333, reward 208, memory_length 2000, epsilon 0.74104, total time 722, loss [1360.74853515625], compute time 10.73\n",
      "episode 334, reward 410, memory_length 2000, epsilon 0.74037, total time 730, loss [1249.251220703125], compute time 12.32\n",
      "episode 335, reward 391, memory_length 2000, epsilon 0.73971, total time 721, loss [926.189697265625], compute time 11.72\n",
      "episode 336, reward -39, memory_length 2000, epsilon 0.73904, total time 730, loss [147.70254516601562], compute time 12.0\n",
      "episode 337, reward 167, memory_length 2000, epsilon 0.73838, total time 727, loss [430.91278076171875], compute time 11.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 338, reward 356, memory_length 2000, epsilon 0.73771, total time 726, loss [401.10321044921875], compute time 10.89\n",
      "episode 339, reward 258, memory_length 2000, epsilon 0.73705, total time 721, loss [338.9935607910156], compute time 11.58\n",
      "episode 340, reward 266, memory_length 2000, epsilon 0.73639, total time 721, loss -, compute time 11.94\n",
      "Saving model for episode: 340\n",
      "episode 341, reward 333, memory_length 2000, epsilon 0.73572, total time 724, loss [379.08355712890625], compute time 11.0\n",
      "episode 342, reward 50, memory_length 2000, epsilon 0.73506, total time 721, loss [308.8642578125], compute time 12.46\n",
      "episode 343, reward 56, memory_length 2000, epsilon 0.7344, total time 721, loss -, compute time 11.55\n",
      "episode 344, reward 270, memory_length 2000, epsilon 0.73374, total time 727, loss -, compute time 12.03\n",
      "episode 345, reward 518, memory_length 2000, epsilon 0.73308, total time 723, loss [300.88262939453125], compute time 10.69\n",
      "episode 346, reward -34, memory_length 2000, epsilon 0.73242, total time 728, loss [250.098388671875], compute time 11.74\n",
      "episode 347, reward 513, memory_length 2000, epsilon 0.73176, total time 723, loss -, compute time 11.25\n",
      "episode 348, reward 131, memory_length 2000, epsilon 0.7311, total time 721, loss -, compute time 10.9\n",
      "episode 349, reward 420, memory_length 2000, epsilon 0.73045, total time 724, loss -, compute time 13.2\n",
      "episode 350, reward 88, memory_length 2000, epsilon 0.72979, total time 722, loss [200.61871337890625], compute time 11.92\n",
      "Saving model for episode: 350\n",
      "episode 351, reward 36, memory_length 2000, epsilon 0.72913, total time 724, loss [505.31036376953125], compute time 10.01\n",
      "episode 352, reward 139, memory_length 2000, epsilon 0.72848, total time 723, loss -, compute time 11.73\n",
      "episode 353, reward 250, memory_length 2000, epsilon 0.72782, total time 721, loss [365.98077392578125], compute time 11.53\n",
      "episode 354, reward 266, memory_length 2000, epsilon 0.72717, total time 721, loss [274.34161376953125], compute time 12.83\n",
      "episode 355, reward 14, memory_length 2000, epsilon 0.72651, total time 721, loss -, compute time 11.51\n",
      "episode 356, reward -75, memory_length 2000, epsilon 0.72586, total time 722, loss [216.70062255859375], compute time 13.02\n",
      "episode 357, reward 161, memory_length 2000, epsilon 0.72521, total time 726, loss [336.877197265625], compute time 11.76\n",
      "episode 358, reward 464, memory_length 2000, epsilon 0.72455, total time 722, loss [597.3612060546875], compute time 11.42\n",
      "episode 359, reward 556, memory_length 2000, epsilon 0.7239, total time 725, loss [271.0294494628906], compute time 11.5\n",
      "episode 360, reward 729, memory_length 2000, epsilon 0.72325, total time 728, loss [562.4614868164062], compute time 11.68\n",
      "Saving model for episode: 360\n",
      "episode 361, reward 289, memory_length 2000, epsilon 0.7226, total time 729, loss -, compute time 10.91\n",
      "episode 362, reward 268, memory_length 2000, epsilon 0.72195, total time 722, loss -, compute time 11.12\n",
      "episode 363, reward 163, memory_length 2000, epsilon 0.7213, total time 724, loss [2.4502806663513184], compute time 10.97\n",
      "episode 364, reward 360, memory_length 2000, epsilon 0.72065, total time 729, loss -, compute time 12.05\n",
      "episode 365, reward 531, memory_length 2000, epsilon 0.72, total time 727, loss [291.73468017578125], compute time 11.75\n",
      "episode 366, reward 367, memory_length 2000, epsilon 0.71936, total time 722, loss [257.5150451660156], compute time 12.26\n",
      "episode 367, reward 331, memory_length 2000, epsilon 0.71871, total time 723, loss [157.28805541992188], compute time 13.06\n",
      "episode 368, reward 511, memory_length 2000, epsilon 0.71806, total time 727, loss [386.5712890625], compute time 11.21\n",
      "episode 369, reward 468, memory_length 2000, epsilon 0.71742, total time 721, loss [391.5807800292969], compute time 12.2\n",
      "episode 370, reward 225, memory_length 2000, epsilon 0.71677, total time 721, loss [454.41644287109375], compute time 10.18\n",
      "Saving model for episode: 370\n",
      "episode 371, reward 41, memory_length 2000, epsilon 0.71613, total time 724, loss [215.36941528320312], compute time 11.5\n",
      "episode 372, reward 546, memory_length 2000, epsilon 0.71548, total time 731, loss [273.7745666503906], compute time 11.6\n",
      "episode 373, reward 73, memory_length 2000, epsilon 0.71484, total time 724, loss [202.88833618164062], compute time 10.16\n",
      "episode 374, reward 7, memory_length 2000, epsilon 0.71419, total time 725, loss [266.5341491699219], compute time 9.69\n",
      "episode 375, reward 484, memory_length 2000, epsilon 0.71355, total time 724, loss [272.18701171875], compute time 10.89\n",
      "episode 376, reward 358, memory_length 2000, epsilon 0.71291, total time 722, loss [363.6997375488281], compute time 10.95\n",
      "episode 377, reward 450, memory_length 2000, epsilon 0.71227, total time 727, loss -, compute time 11.03\n",
      "episode 378, reward 412, memory_length 2000, epsilon 0.71163, total time 723, loss [243.11024475097656], compute time 10.65\n",
      "episode 379, reward 519, memory_length 2000, epsilon 0.71099, total time 725, loss [370.2402648925781], compute time 11.3\n",
      "episode 380, reward 300, memory_length 2000, epsilon 0.71035, total time 722, loss -, compute time 11.21\n",
      "Saving model for episode: 380\n",
      "episode 381, reward 59, memory_length 2000, epsilon 0.70971, total time 724, loss -, compute time 12.4\n",
      "episode 382, reward 347, memory_length 2000, epsilon 0.70907, total time 726, loss [441.7294616699219], compute time 12.7\n",
      "episode 383, reward 41, memory_length 2000, epsilon 0.70843, total time 724, loss [83.27156066894531], compute time 12.21\n",
      "episode 384, reward 547, memory_length 2000, epsilon 0.7078, total time 721, loss -, compute time 11.14\n",
      "episode 385, reward 334, memory_length 2000, epsilon 0.70716, total time 729, loss -, compute time 11.01\n",
      "episode 386, reward 496, memory_length 2000, epsilon 0.70652, total time 732, loss [286.14337158203125], compute time 12.16\n",
      "episode 387, reward 64, memory_length 2000, epsilon 0.70589, total time 726, loss [430.0591735839844], compute time 11.13\n",
      "episode 388, reward 448, memory_length 2000, epsilon 0.70525, total time 723, loss -, compute time 10.31\n",
      "episode 389, reward 203, memory_length 2000, epsilon 0.70462, total time 725, loss [442.12225341796875], compute time 11.43\n",
      "episode 390, reward 283, memory_length 2000, epsilon 0.70398, total time 724, loss [171.57017517089844], compute time 10.98\n",
      "Saving model for episode: 390\n",
      "episode 391, reward 554, memory_length 2000, epsilon 0.70335, total time 730, loss [2.4554121494293213], compute time 11.24\n",
      "episode 392, reward 618, memory_length 2000, epsilon 0.70272, total time 725, loss [3.059814691543579], compute time 11.5\n",
      "episode 393, reward 578, memory_length 2000, epsilon 0.70209, total time 728, loss [111.94070434570312], compute time 10.58\n",
      "episode 394, reward 609, memory_length 2000, epsilon 0.70145, total time 725, loss [604.9866943359375], compute time 10.81\n",
      "episode 395, reward 353, memory_length 2000, epsilon 0.70082, total time 727, loss [99.8780517578125], compute time 11.17\n",
      "episode 396, reward 452, memory_length 2000, epsilon 0.70019, total time 726, loss [577.816650390625], compute time 11.87\n",
      "episode 397, reward 658, memory_length 2000, epsilon 0.69956, total time 726, loss [106.71378326416016], compute time 11.49\n",
      "episode 398, reward 216, memory_length 2000, epsilon 0.69893, total time 721, loss [291.00396728515625], compute time 12.12\n",
      "episode 399, reward -67, memory_length 2000, epsilon 0.6983, total time 725, loss -, compute time 11.15\n",
      "episode 400, reward 438, memory_length 2000, epsilon 0.69768, total time 726, loss [385.0386962890625], compute time 11.27\n",
      "Saving model for episode: 400\n",
      "episode 401, reward 362, memory_length 2000, epsilon 0.69705, total time 725, loss -, compute time 11.97\n",
      "episode 402, reward 650, memory_length 2000, epsilon 0.69642, total time 723, loss [446.85791015625], compute time 10.94\n",
      "episode 403, reward 556, memory_length 2000, epsilon 0.6958, total time 722, loss [355.76702880859375], compute time 12.25\n",
      "episode 404, reward 506, memory_length 2000, epsilon 0.69517, total time 725, loss [197.86471557617188], compute time 9.05\n",
      "episode 405, reward 40, memory_length 2000, epsilon 0.69454, total time 723, loss -, compute time 11.08\n",
      "episode 406, reward 261, memory_length 2000, epsilon 0.69392, total time 725, loss [194.38746643066406], compute time 11.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 407, reward 235, memory_length 2000, epsilon 0.69329, total time 724, loss [204.52114868164062], compute time 11.6\n",
      "episode 408, reward 254, memory_length 2000, epsilon 0.69267, total time 727, loss [412.2796325683594], compute time 12.16\n",
      "episode 409, reward 529, memory_length 2000, epsilon 0.69205, total time 725, loss -, compute time 11.74\n",
      "episode 410, reward 204, memory_length 2000, epsilon 0.69143, total time 723, loss [217.65972900390625], compute time 11.37\n",
      "Saving model for episode: 410\n",
      "episode 411, reward 329, memory_length 2000, epsilon 0.6908, total time 728, loss -, compute time 10.44\n",
      "episode 412, reward 119, memory_length 2000, epsilon 0.69018, total time 726, loss [224.8780059814453], compute time 12.39\n",
      "episode 413, reward 223, memory_length 2000, epsilon 0.68956, total time 725, loss -, compute time 12.28\n",
      "episode 414, reward 352, memory_length 2000, epsilon 0.68894, total time 729, loss -, compute time 12.06\n",
      "episode 415, reward 351, memory_length 2000, epsilon 0.68832, total time 725, loss [454.95806884765625], compute time 10.06\n",
      "episode 416, reward 484, memory_length 2000, epsilon 0.6877, total time 732, loss [85.37357330322266], compute time 11.28\n",
      "episode 417, reward 260, memory_length 2000, epsilon 0.68708, total time 728, loss [74.77303314208984], compute time 12.28\n",
      "episode 418, reward 273, memory_length 2000, epsilon 0.68647, total time 723, loss -, compute time 10.12\n",
      "episode 419, reward 63, memory_length 2000, epsilon 0.68585, total time 721, loss -, compute time 10.36\n",
      "episode 420, reward 234, memory_length 2000, epsilon 0.68523, total time 726, loss [184.10011291503906], compute time 11.53\n",
      "Saving model for episode: 420\n",
      "episode 421, reward 415, memory_length 2000, epsilon 0.68461, total time 727, loss [96.81330871582031], compute time 10.3\n",
      "episode 422, reward 555, memory_length 2000, epsilon 0.684, total time 724, loss [409.726806640625], compute time 11.22\n",
      "episode 423, reward 700, memory_length 2000, epsilon 0.68338, total time 725, loss [345.28302001953125], compute time 11.61\n",
      "episode 424, reward 236, memory_length 2000, epsilon 0.68277, total time 723, loss -, compute time 12.02\n",
      "episode 425, reward 73, memory_length 2000, epsilon 0.68215, total time 724, loss -, compute time 10.21\n",
      "episode 426, reward 288, memory_length 2000, epsilon 0.68154, total time 721, loss [303.2838134765625], compute time 10.58\n",
      "episode 427, reward 168, memory_length 2000, epsilon 0.68093, total time 721, loss -, compute time 9.79\n",
      "episode 428, reward 737, memory_length 2000, epsilon 0.68031, total time 721, loss -, compute time 12.2\n",
      "episode 429, reward 291, memory_length 2000, epsilon 0.6797, total time 723, loss [445.4536437988281], compute time 10.83\n",
      "episode 430, reward 129, memory_length 2000, epsilon 0.67909, total time 721, loss [268.9191589355469], compute time 11.21\n",
      "Saving model for episode: 430\n",
      "episode 431, reward 797, memory_length 2000, epsilon 0.67848, total time 726, loss -, compute time 11.65\n",
      "episode 432, reward 369, memory_length 2000, epsilon 0.67787, total time 726, loss [199.2235870361328], compute time 11.59\n",
      "episode 433, reward 447, memory_length 2000, epsilon 0.67726, total time 721, loss -, compute time 11.61\n",
      "episode 434, reward 702, memory_length 2000, epsilon 0.67665, total time 722, loss -, compute time 10.96\n",
      "episode 435, reward 372, memory_length 2000, epsilon 0.67604, total time 721, loss [231.18975830078125], compute time 10.56\n",
      "episode 436, reward 612, memory_length 2000, epsilon 0.67543, total time 730, loss [326.053955078125], compute time 12.1\n",
      "episode 437, reward 287, memory_length 2000, epsilon 0.67483, total time 726, loss [314.71044921875], compute time 11.5\n",
      "episode 438, reward 730, memory_length 2000, epsilon 0.67422, total time 723, loss [561.6217651367188], compute time 12.55\n",
      "episode 439, reward 424, memory_length 2000, epsilon 0.67361, total time 724, loss [569.8430786132812], compute time 13.34\n",
      "episode 440, reward 249, memory_length 2000, epsilon 0.67301, total time 726, loss [599.2544555664062], compute time 11.65\n",
      "Saving model for episode: 440\n",
      "episode 441, reward 176, memory_length 2000, epsilon 0.6724, total time 721, loss [233.6237335205078], compute time 11.12\n",
      "episode 442, reward 160, memory_length 2000, epsilon 0.6718, total time 721, loss [145.65989685058594], compute time 13.24\n",
      "episode 443, reward 576, memory_length 2000, epsilon 0.67119, total time 724, loss [433.13885498046875], compute time 10.57\n",
      "episode 444, reward 509, memory_length 2000, epsilon 0.67059, total time 723, loss [334.1781005859375], compute time 10.19\n",
      "episode 445, reward 219, memory_length 2000, epsilon 0.66998, total time 724, loss -, compute time 11.81\n",
      "episode 446, reward 322, memory_length 2000, epsilon 0.66938, total time 721, loss -, compute time 10.28\n",
      "episode 447, reward 825, memory_length 2000, epsilon 0.66878, total time 726, loss [542.898193359375], compute time 10.97\n",
      "episode 448, reward 365, memory_length 2000, epsilon 0.66818, total time 721, loss [250.12054443359375], compute time 10.86\n",
      "episode 449, reward 337, memory_length 2000, epsilon 0.66758, total time 722, loss [695.441162109375], compute time 10.89\n",
      "episode 450, reward 276, memory_length 2000, epsilon 0.66698, total time 729, loss [384.9692687988281], compute time 12.93\n",
      "Saving model for episode: 450\n",
      "episode 451, reward 44, memory_length 2000, epsilon 0.66638, total time 723, loss [417.24554443359375], compute time 10.39\n",
      "episode 452, reward 173, memory_length 2000, epsilon 0.66578, total time 724, loss [395.564208984375], compute time 11.19\n",
      "episode 453, reward 156, memory_length 2000, epsilon 0.66518, total time 726, loss [365.7445373535156], compute time 13.64\n",
      "episode 454, reward 515, memory_length 2000, epsilon 0.66458, total time 722, loss [218.83401489257812], compute time 13.67\n",
      "episode 455, reward 566, memory_length 2000, epsilon 0.66398, total time 721, loss [312.3351745605469], compute time 11.22\n",
      "episode 456, reward 438, memory_length 2000, epsilon 0.66338, total time 728, loss [321.5935363769531], compute time 10.63\n",
      "episode 457, reward 336, memory_length 2000, epsilon 0.66279, total time 722, loss -, compute time 11.66\n",
      "episode 458, reward 293, memory_length 2000, epsilon 0.66219, total time 724, loss [557.3927001953125], compute time 12.57\n",
      "episode 459, reward 538, memory_length 2000, epsilon 0.6616, total time 726, loss [291.8656921386719], compute time 10.43\n",
      "episode 460, reward 406, memory_length 2000, epsilon 0.661, total time 721, loss [216.061767578125], compute time 11.23\n",
      "Saving model for episode: 460\n",
      "episode 461, reward 601, memory_length 2000, epsilon 0.66041, total time 725, loss [220.27442932128906], compute time 12.27\n",
      "episode 462, reward 454, memory_length 2000, epsilon 0.65981, total time 721, loss -, compute time 10.65\n",
      "episode 463, reward 231, memory_length 2000, epsilon 0.65922, total time 725, loss [370.57232666015625], compute time 10.06\n",
      "episode 464, reward 572, memory_length 2000, epsilon 0.65863, total time 727, loss -, compute time 11.39\n",
      "episode 465, reward 292, memory_length 2000, epsilon 0.65803, total time 725, loss [192.74761962890625], compute time 10.64\n",
      "episode 466, reward 227, memory_length 2000, epsilon 0.65744, total time 728, loss [295.7249450683594], compute time 11.87\n",
      "episode 467, reward 429, memory_length 2000, epsilon 0.65685, total time 725, loss -, compute time 9.99\n",
      "episode 468, reward -84, memory_length 2000, epsilon 0.65626, total time 728, loss -, compute time 10.97\n",
      "episode 469, reward -54, memory_length 2000, epsilon 0.65567, total time 726, loss [221.67222595214844], compute time 12.5\n",
      "episode 470, reward 189, memory_length 2000, epsilon 0.65508, total time 722, loss -, compute time 14.04\n",
      "Saving model for episode: 470\n",
      "episode 471, reward 547, memory_length 2000, epsilon 0.65449, total time 724, loss -, compute time 12.81\n",
      "episode 472, reward 307, memory_length 2000, epsilon 0.6539, total time 723, loss [278.4798889160156], compute time 11.94\n",
      "episode 473, reward 289, memory_length 2000, epsilon 0.65331, total time 724, loss -, compute time 11.12\n",
      "episode 474, reward 109, memory_length 2000, epsilon 0.65272, total time 722, loss [266.51806640625], compute time 10.76\n",
      "episode 475, reward 392, memory_length 2000, epsilon 0.65214, total time 729, loss -, compute time 11.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 476, reward 646, memory_length 2000, epsilon 0.65155, total time 724, loss -, compute time 10.95\n",
      "episode 477, reward 329, memory_length 2000, epsilon 0.65096, total time 723, loss [97.5779037475586], compute time 11.71\n",
      "episode 478, reward 385, memory_length 2000, epsilon 0.65038, total time 721, loss -, compute time 10.74\n",
      "episode 479, reward 496, memory_length 2000, epsilon 0.64979, total time 723, loss -, compute time 10.73\n",
      "episode 480, reward 767, memory_length 2000, epsilon 0.64921, total time 726, loss [82.23078155517578], compute time 11.43\n",
      "Saving model for episode: 480\n",
      "episode 481, reward 279, memory_length 2000, epsilon 0.64863, total time 727, loss -, compute time 12.06\n",
      "episode 482, reward 367, memory_length 2000, epsilon 0.64804, total time 728, loss -, compute time 11.41\n",
      "episode 483, reward 135, memory_length 2000, epsilon 0.64746, total time 721, loss [557.5509643554688], compute time 11.72\n",
      "episode 484, reward 417, memory_length 2000, epsilon 0.64688, total time 723, loss [274.41046142578125], compute time 12.03\n",
      "episode 485, reward 261, memory_length 2000, epsilon 0.64629, total time 721, loss [82.4624252319336], compute time 11.76\n",
      "episode 486, reward 417, memory_length 2000, epsilon 0.64571, total time 721, loss -, compute time 10.93\n",
      "episode 487, reward 771, memory_length 2000, epsilon 0.64513, total time 728, loss [314.8970642089844], compute time 11.74\n",
      "episode 488, reward 86, memory_length 2000, epsilon 0.64455, total time 723, loss [542.0341186523438], compute time 11.65\n",
      "episode 489, reward 297, memory_length 2000, epsilon 0.64397, total time 730, loss -, compute time 11.27\n",
      "episode 490, reward 306, memory_length 2000, epsilon 0.64339, total time 724, loss -, compute time 12.47\n",
      "Saving model for episode: 490\n",
      "episode 491, reward 374, memory_length 2000, epsilon 0.64281, total time 724, loss -, compute time 11.13\n",
      "episode 492, reward 369, memory_length 2000, epsilon 0.64224, total time 721, loss -, compute time 11.38\n",
      "episode 493, reward 337, memory_length 2000, epsilon 0.64166, total time 727, loss [114.34181213378906], compute time 10.78\n",
      "episode 494, reward 227, memory_length 2000, epsilon 0.64108, total time 722, loss -, compute time 12.63\n",
      "episode 495, reward 454, memory_length 2000, epsilon 0.6405, total time 723, loss -, compute time 11.86\n",
      "episode 496, reward 405, memory_length 2000, epsilon 0.63993, total time 721, loss [490.2396240234375], compute time 11.66\n",
      "episode 497, reward 356, memory_length 2000, epsilon 0.63935, total time 724, loss -, compute time 12.33\n",
      "episode 498, reward 634, memory_length 2000, epsilon 0.63878, total time 722, loss [253.8916473388672], compute time 11.64\n",
      "episode 499, reward 439, memory_length 2000, epsilon 0.6382, total time 723, loss -, compute time 10.15\n",
      "episode 500, reward 334, memory_length 2000, epsilon 0.63763, total time 725, loss [520.263916015625], compute time 10.56\n",
      "Saving model for episode: 500\n",
      "episode 501, reward 351, memory_length 2000, epsilon 0.63705, total time 722, loss [262.63653564453125], compute time 11.32\n",
      "episode 502, reward 190, memory_length 2000, epsilon 0.63648, total time 728, loss [433.34063720703125], compute time 13.21\n",
      "episode 503, reward 342, memory_length 2000, epsilon 0.63591, total time 727, loss [281.8008728027344], compute time 11.2\n",
      "episode 504, reward 362, memory_length 2000, epsilon 0.63534, total time 722, loss [386.792724609375], compute time 9.83\n",
      "episode 505, reward 550, memory_length 2000, epsilon 0.63477, total time 726, loss [307.69036865234375], compute time 12.07\n",
      "episode 506, reward 216, memory_length 2000, epsilon 0.63419, total time 726, loss [267.9197998046875], compute time 10.92\n",
      "episode 507, reward 311, memory_length 2000, epsilon 0.63362, total time 722, loss [229.28704833984375], compute time 12.77\n",
      "episode 508, reward 150, memory_length 2000, epsilon 0.63305, total time 725, loss [239.78065490722656], compute time 15.02\n",
      "episode 509, reward 244, memory_length 2000, epsilon 0.63248, total time 726, loss [625.9598999023438], compute time 13.14\n",
      "episode 510, reward 312, memory_length 2000, epsilon 0.63192, total time 728, loss [464.2462158203125], compute time 12.35\n",
      "Saving model for episode: 510\n",
      "episode 511, reward 222, memory_length 2000, epsilon 0.63135, total time 725, loss -, compute time 12.01\n",
      "episode 512, reward 123, memory_length 2000, epsilon 0.63078, total time 723, loss [73.77252197265625], compute time 12.07\n",
      "episode 513, reward 714, memory_length 2000, epsilon 0.63021, total time 725, loss [403.6967468261719], compute time 12.21\n",
      "episode 514, reward 5, memory_length 2000, epsilon 0.62964, total time 724, loss [198.08937072753906], compute time 11.07\n",
      "episode 515, reward 499, memory_length 2000, epsilon 0.62908, total time 721, loss -, compute time 14.91\n",
      "episode 516, reward 559, memory_length 2000, epsilon 0.62851, total time 726, loss -, compute time 13.22\n",
      "episode 517, reward 604, memory_length 2000, epsilon 0.62795, total time 725, loss [479.7767333984375], compute time 12.59\n",
      "episode 518, reward 331, memory_length 2000, epsilon 0.62738, total time 724, loss [345.3360290527344], compute time 12.35\n",
      "episode 519, reward 525, memory_length 2000, epsilon 0.62682, total time 724, loss [332.8786926269531], compute time 11.9\n",
      "episode 520, reward 437, memory_length 2000, epsilon 0.62625, total time 721, loss [167.4968719482422], compute time 12.96\n",
      "Saving model for episode: 520\n",
      "episode 521, reward 674, memory_length 2000, epsilon 0.62569, total time 723, loss -, compute time 13.42\n",
      "episode 522, reward 477, memory_length 2000, epsilon 0.62513, total time 728, loss -, compute time 12.32\n",
      "episode 523, reward 520, memory_length 2000, epsilon 0.62456, total time 724, loss -, compute time 13.36\n",
      "episode 524, reward 442, memory_length 2000, epsilon 0.624, total time 722, loss -, compute time 15.29\n",
      "episode 525, reward 540, memory_length 2000, epsilon 0.62344, total time 734, loss [302.4114990234375], compute time 13.39\n",
      "episode 526, reward 774, memory_length 2000, epsilon 0.62288, total time 721, loss [407.9929504394531], compute time 12.53\n",
      "episode 527, reward 85, memory_length 2000, epsilon 0.62232, total time 721, loss -, compute time 11.56\n",
      "episode 528, reward 524, memory_length 2000, epsilon 0.62176, total time 728, loss [2.6930832862854004], compute time 11.27\n",
      "episode 529, reward 277, memory_length 2000, epsilon 0.6212, total time 729, loss [338.9881591796875], compute time 10.98\n",
      "episode 530, reward 482, memory_length 2000, epsilon 0.62064, total time 723, loss [383.00421142578125], compute time 17.94\n",
      "Saving model for episode: 530\n",
      "episode 531, reward 378, memory_length 2000, epsilon 0.62008, total time 729, loss [207.03045654296875], compute time 17.56\n",
      "episode 532, reward 312, memory_length 2000, epsilon 0.61953, total time 729, loss [431.0465393066406], compute time 13.86\n",
      "episode 533, reward 865, memory_length 2000, epsilon 0.61897, total time 727, loss -, compute time 11.8\n",
      "episode 534, reward 525, memory_length 2000, epsilon 0.61841, total time 724, loss -, compute time 12.47\n",
      "episode 535, reward 383, memory_length 2000, epsilon 0.61786, total time 732, loss [563.8062744140625], compute time 11.59\n",
      "episode 536, reward 536, memory_length 2000, epsilon 0.6173, total time 725, loss [288.3583068847656], compute time 11.54\n",
      "episode 537, reward 619, memory_length 2000, epsilon 0.61674, total time 725, loss -, compute time 11.31\n",
      "episode 538, reward 565, memory_length 2000, epsilon 0.61619, total time 723, loss [317.96807861328125], compute time 11.96\n",
      "episode 539, reward 611, memory_length 2000, epsilon 0.61564, total time 726, loss -, compute time 12.09\n",
      "episode 540, reward 361, memory_length 2000, epsilon 0.61508, total time 733, loss [195.2900848388672], compute time 11.26\n",
      "Saving model for episode: 540\n",
      "episode 541, reward 511, memory_length 2000, epsilon 0.61453, total time 724, loss -, compute time 12.57\n",
      "episode 542, reward 194, memory_length 2000, epsilon 0.61398, total time 721, loss [322.1025390625], compute time 10.4\n",
      "episode 543, reward 389, memory_length 2000, epsilon 0.61342, total time 726, loss -, compute time 11.61\n",
      "episode 544, reward 315, memory_length 2000, epsilon 0.61287, total time 722, loss -, compute time 11.79\n",
      "episode 545, reward 470, memory_length 2000, epsilon 0.61232, total time 723, loss [2.045248508453369], compute time 11.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 546, reward 425, memory_length 2000, epsilon 0.61177, total time 722, loss [212.71170043945312], compute time 12.67\n",
      "episode 547, reward 80, memory_length 2000, epsilon 0.61122, total time 724, loss -, compute time 12.55\n",
      "episode 548, reward 320, memory_length 2000, epsilon 0.61067, total time 727, loss -, compute time 12.6\n",
      "episode 549, reward 359, memory_length 2000, epsilon 0.61012, total time 732, loss [453.7476806640625], compute time 11.94\n",
      "episode 550, reward 649, memory_length 2000, epsilon 0.60957, total time 728, loss [483.76239013671875], compute time 12.44\n",
      "Saving model for episode: 550\n",
      "episode 551, reward 661, memory_length 2000, epsilon 0.60902, total time 722, loss [3.4193837642669678], compute time 11.49\n",
      "episode 552, reward 249, memory_length 2000, epsilon 0.60847, total time 723, loss [209.24603271484375], compute time 10.76\n",
      "episode 553, reward 597, memory_length 2000, epsilon 0.60793, total time 727, loss -, compute time 9.83\n",
      "episode 554, reward 405, memory_length 2000, epsilon 0.60738, total time 721, loss [155.89157104492188], compute time 11.87\n",
      "episode 555, reward 540, memory_length 2000, epsilon 0.60683, total time 729, loss -, compute time 13.01\n",
      "episode 556, reward 734, memory_length 2000, epsilon 0.60629, total time 732, loss -, compute time 14.1\n",
      "episode 557, reward 704, memory_length 2000, epsilon 0.60574, total time 722, loss [496.8857421875], compute time 13.1\n",
      "episode 558, reward 459, memory_length 2000, epsilon 0.6052, total time 729, loss [541.8670654296875], compute time 15.08\n",
      "episode 559, reward 351, memory_length 2000, epsilon 0.60465, total time 726, loss [312.3304443359375], compute time 14.98\n",
      "episode 560, reward 456, memory_length 2000, epsilon 0.60411, total time 725, loss [355.84490966796875], compute time 15.98\n",
      "Saving model for episode: 560\n",
      "episode 561, reward 426, memory_length 2000, epsilon 0.60357, total time 725, loss -, compute time 14.54\n",
      "episode 562, reward 645, memory_length 2000, epsilon 0.60302, total time 728, loss [184.21669006347656], compute time 13.82\n",
      "episode 563, reward 515, memory_length 2000, epsilon 0.60248, total time 722, loss [116.05219268798828], compute time 15.46\n",
      "episode 564, reward 356, memory_length 2000, epsilon 0.60194, total time 724, loss -, compute time 14.2\n",
      "episode 565, reward 513, memory_length 2000, epsilon 0.6014, total time 721, loss -, compute time 15.52\n",
      "episode 566, reward 694, memory_length 2000, epsilon 0.60086, total time 721, loss -, compute time 13.53\n",
      "episode 567, reward 640, memory_length 2000, epsilon 0.60032, total time 722, loss [334.8361511230469], compute time 15.03\n",
      "episode 568, reward 441, memory_length 2000, epsilon 0.59978, total time 729, loss -, compute time 13.77\n",
      "episode 569, reward 485, memory_length 2000, epsilon 0.59924, total time 723, loss -, compute time 13.21\n",
      "episode 570, reward 793, memory_length 2000, epsilon 0.5987, total time 722, loss [127.76094818115234], compute time 13.88\n",
      "Saving model for episode: 570\n",
      "episode 571, reward 245, memory_length 2000, epsilon 0.59816, total time 723, loss -, compute time 12.74\n",
      "episode 572, reward 195, memory_length 2000, epsilon 0.59762, total time 723, loss [443.72576904296875], compute time 12.24\n",
      "episode 573, reward 581, memory_length 2000, epsilon 0.59708, total time 730, loss -, compute time 13.59\n",
      "episode 574, reward 681, memory_length 2000, epsilon 0.59655, total time 723, loss [108.32264709472656], compute time 14.28\n",
      "episode 575, reward 504, memory_length 2000, epsilon 0.59601, total time 726, loss -, compute time 12.34\n",
      "episode 576, reward 266, memory_length 2000, epsilon 0.59547, total time 721, loss [89.07159423828125], compute time 11.39\n",
      "episode 577, reward 347, memory_length 2000, epsilon 0.59494, total time 731, loss [530.1295166015625], compute time 12.76\n",
      "episode 578, reward 334, memory_length 2000, epsilon 0.5944, total time 721, loss [433.5320739746094], compute time 13.71\n",
      "episode 579, reward 663, memory_length 2000, epsilon 0.59387, total time 725, loss [192.25421142578125], compute time 11.71\n",
      "episode 580, reward 550, memory_length 2000, epsilon 0.59333, total time 722, loss [203.78396606445312], compute time 11.98\n",
      "Saving model for episode: 580\n",
      "episode 581, reward 518, memory_length 2000, epsilon 0.5928, total time 721, loss -, compute time 12.41\n",
      "episode 582, reward 674, memory_length 2000, epsilon 0.59227, total time 721, loss -, compute time 12.93\n",
      "episode 583, reward 688, memory_length 2000, epsilon 0.59173, total time 723, loss [226.30245971679688], compute time 12.26\n",
      "episode 584, reward 613, memory_length 2000, epsilon 0.5912, total time 725, loss -, compute time 14.47\n",
      "episode 585, reward 631, memory_length 2000, epsilon 0.59067, total time 724, loss [601.2119140625], compute time 12.65\n",
      "episode 586, reward 843, memory_length 2000, epsilon 0.59014, total time 728, loss -, compute time 13.15\n",
      "episode 587, reward 672, memory_length 2000, epsilon 0.58961, total time 726, loss [501.9527587890625], compute time 12.85\n",
      "episode 588, reward 639, memory_length 2000, epsilon 0.58908, total time 727, loss [475.7478942871094], compute time 10.86\n",
      "episode 589, reward 101, memory_length 2000, epsilon 0.58855, total time 731, loss -, compute time 11.18\n",
      "episode 590, reward 881, memory_length 2000, epsilon 0.58802, total time 724, loss [158.60984802246094], compute time 12.57\n",
      "Saving model for episode: 590\n",
      "episode 591, reward 546, memory_length 2000, epsilon 0.58749, total time 726, loss [427.39862060546875], compute time 12.58\n",
      "episode 592, reward 663, memory_length 2000, epsilon 0.58696, total time 723, loss [112.9236831665039], compute time 12.3\n",
      "episode 593, reward 779, memory_length 2000, epsilon 0.58643, total time 725, loss -, compute time 12.94\n",
      "episode 594, reward 596, memory_length 2000, epsilon 0.5859, total time 724, loss -, compute time 12.59\n",
      "episode 595, reward 402, memory_length 2000, epsilon 0.58538, total time 726, loss [323.4482421875], compute time 12.06\n",
      "episode 596, reward 829, memory_length 2000, epsilon 0.58485, total time 721, loss [364.588134765625], compute time 13.41\n",
      "episode 597, reward 561, memory_length 2000, epsilon 0.58432, total time 724, loss -, compute time 12.82\n",
      "episode 598, reward 705, memory_length 2000, epsilon 0.5838, total time 725, loss -, compute time 13.25\n",
      "episode 599, reward 423, memory_length 2000, epsilon 0.58327, total time 721, loss -, compute time 12.09\n",
      "episode 600, reward 805, memory_length 2000, epsilon 0.58275, total time 724, loss [528.654052734375], compute time 10.78\n",
      "Saving model for episode: 600\n",
      "episode 601, reward 581, memory_length 2000, epsilon 0.58222, total time 721, loss [118.31298065185547], compute time 11.33\n",
      "episode 602, reward 314, memory_length 2000, epsilon 0.5817, total time 723, loss [3.0306854248046875], compute time 10.51\n",
      "episode 603, reward 699, memory_length 2000, epsilon 0.58118, total time 727, loss [295.4624328613281], compute time 12.25\n",
      "episode 604, reward 343, memory_length 2000, epsilon 0.58065, total time 726, loss [83.41012573242188], compute time 11.38\n",
      "episode 605, reward 600, memory_length 2000, epsilon 0.58013, total time 725, loss [306.55035400390625], compute time 11.76\n",
      "episode 606, reward 360, memory_length 2000, epsilon 0.57961, total time 721, loss -, compute time 13.16\n",
      "episode 607, reward 613, memory_length 2000, epsilon 0.57909, total time 724, loss [219.25491333007812], compute time 12.7\n",
      "episode 608, reward 609, memory_length 2000, epsilon 0.57857, total time 729, loss -, compute time 12.14\n",
      "episode 609, reward 767, memory_length 2000, epsilon 0.57805, total time 725, loss -, compute time 12.1\n",
      "episode 610, reward 298, memory_length 2000, epsilon 0.57753, total time 721, loss [412.22821044921875], compute time 11.96\n",
      "Saving model for episode: 610\n",
      "episode 611, reward 384, memory_length 2000, epsilon 0.57701, total time 726, loss [710.2201538085938], compute time 12.03\n",
      "episode 612, reward 511, memory_length 2000, epsilon 0.57649, total time 728, loss [96.37417602539062], compute time 11.69\n",
      "episode 613, reward 662, memory_length 2000, epsilon 0.57597, total time 722, loss -, compute time 13.32\n",
      "episode 614, reward 788, memory_length 2000, epsilon 0.57545, total time 723, loss -, compute time 12.69\n",
      "episode 615, reward 1113, memory_length 2000, epsilon 0.57493, total time 732, loss -, compute time 13.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 616, reward 987, memory_length 2000, epsilon 0.57442, total time 728, loss -, compute time 13.57\n",
      "episode 617, reward 635, memory_length 2000, epsilon 0.5739, total time 729, loss [324.85443115234375], compute time 11.87\n",
      "episode 618, reward 513, memory_length 2000, epsilon 0.57338, total time 722, loss [2.4007835388183594], compute time 10.67\n",
      "episode 619, reward 412, memory_length 2000, epsilon 0.57287, total time 723, loss -, compute time 11.37\n",
      "episode 620, reward 415, memory_length 2000, epsilon 0.57235, total time 722, loss [2.056788444519043], compute time 12.24\n",
      "Saving model for episode: 620\n",
      "episode 621, reward 879, memory_length 2000, epsilon 0.57184, total time 725, loss [174.8433837890625], compute time 11.45\n",
      "episode 622, reward 959, memory_length 2000, epsilon 0.57132, total time 724, loss -, compute time 12.41\n",
      "episode 623, reward 963, memory_length 2000, epsilon 0.57081, total time 725, loss [91.84794616699219], compute time 13.11\n",
      "episode 624, reward 662, memory_length 2000, epsilon 0.5703, total time 729, loss -, compute time 12.0\n",
      "episode 625, reward 635, memory_length 2000, epsilon 0.56978, total time 732, loss [205.24757385253906], compute time 11.34\n",
      "episode 626, reward 214, memory_length 2000, epsilon 0.56927, total time 723, loss [103.22034454345703], compute time 10.07\n",
      "episode 627, reward 509, memory_length 2000, epsilon 0.56876, total time 730, loss [350.0841064453125], compute time 13.34\n",
      "episode 628, reward 372, memory_length 2000, epsilon 0.56825, total time 725, loss -, compute time 12.06\n",
      "episode 629, reward 695, memory_length 2000, epsilon 0.56774, total time 728, loss -, compute time 12.75\n",
      "episode 630, reward 436, memory_length 2000, epsilon 0.56722, total time 722, loss [111.05119323730469], compute time 12.51\n",
      "Saving model for episode: 630\n",
      "episode 631, reward 329, memory_length 2000, epsilon 0.56671, total time 727, loss [75.86639404296875], compute time 11.79\n",
      "episode 632, reward 504, memory_length 2000, epsilon 0.5662, total time 726, loss [302.91119384765625], compute time 12.93\n",
      "episode 633, reward 608, memory_length 2000, epsilon 0.5657, total time 734, loss [365.73291015625], compute time 12.33\n",
      "episode 634, reward 596, memory_length 2000, epsilon 0.56519, total time 730, loss -, compute time 12.03\n",
      "episode 635, reward 601, memory_length 2000, epsilon 0.56468, total time 727, loss [1.909277081489563], compute time 11.6\n",
      "episode 636, reward 518, memory_length 2000, epsilon 0.56417, total time 725, loss [184.29635620117188], compute time 11.49\n",
      "episode 637, reward 379, memory_length 2000, epsilon 0.56366, total time 727, loss -, compute time 11.16\n",
      "episode 638, reward 612, memory_length 2000, epsilon 0.56316, total time 722, loss -, compute time 13.27\n",
      "episode 639, reward 549, memory_length 2000, epsilon 0.56265, total time 722, loss [213.6656951904297], compute time 12.43\n",
      "episode 640, reward 208, memory_length 2000, epsilon 0.56214, total time 731, loss -, compute time 11.39\n",
      "Saving model for episode: 640\n",
      "episode 641, reward 799, memory_length 2000, epsilon 0.56164, total time 730, loss [77.71015930175781], compute time 11.52\n",
      "episode 642, reward 584, memory_length 2000, epsilon 0.56113, total time 722, loss -, compute time 11.19\n",
      "episode 643, reward 388, memory_length 2000, epsilon 0.56063, total time 726, loss [464.10894775390625], compute time 11.74\n",
      "episode 644, reward 648, memory_length 2000, epsilon 0.56012, total time 721, loss -, compute time 11.96\n",
      "episode 645, reward 538, memory_length 2000, epsilon 0.55962, total time 730, loss [93.31839752197266], compute time 12.12\n",
      "episode 646, reward 374, memory_length 2000, epsilon 0.55912, total time 729, loss -, compute time 13.01\n",
      "episode 647, reward 262, memory_length 2000, epsilon 0.55861, total time 724, loss -, compute time 11.46\n",
      "episode 648, reward 540, memory_length 2000, epsilon 0.55811, total time 727, loss -, compute time 12.37\n",
      "episode 649, reward 482, memory_length 2000, epsilon 0.55761, total time 723, loss -, compute time 11.69\n",
      "episode 650, reward 671, memory_length 2000, epsilon 0.55711, total time 728, loss -, compute time 11.07\n",
      "Saving model for episode: 650\n",
      "episode 651, reward 874, memory_length 2000, epsilon 0.5566, total time 724, loss [210.12692260742188], compute time 12.04\n",
      "episode 652, reward 581, memory_length 2000, epsilon 0.5561, total time 733, loss [320.7486572265625], compute time 11.47\n",
      "episode 653, reward 744, memory_length 2000, epsilon 0.5556, total time 721, loss -, compute time 13.21\n",
      "episode 654, reward 505, memory_length 2000, epsilon 0.5551, total time 725, loss [2.709441661834717], compute time 11.4\n",
      "episode 655, reward 528, memory_length 2000, epsilon 0.5546, total time 721, loss [113.61814880371094], compute time 11.1\n",
      "episode 656, reward 491, memory_length 2000, epsilon 0.55411, total time 722, loss -, compute time 12.23\n",
      "episode 657, reward 828, memory_length 2000, epsilon 0.55361, total time 721, loss [84.46934509277344], compute time 11.31\n",
      "episode 658, reward 522, memory_length 2000, epsilon 0.55311, total time 721, loss [210.1227569580078], compute time 12.31\n",
      "episode 659, reward 828, memory_length 2000, epsilon 0.55261, total time 721, loss [131.1820831298828], compute time 12.64\n",
      "episode 660, reward 325, memory_length 2000, epsilon 0.55211, total time 723, loss [109.13752746582031], compute time 10.32\n",
      "Saving model for episode: 660\n",
      "episode 661, reward 607, memory_length 2000, epsilon 0.55162, total time 721, loss -, compute time 11.91\n",
      "episode 662, reward 596, memory_length 2000, epsilon 0.55112, total time 722, loss -, compute time 11.91\n",
      "episode 663, reward 450, memory_length 2000, epsilon 0.55063, total time 722, loss -, compute time 11.77\n",
      "episode 664, reward 456, memory_length 2000, epsilon 0.55013, total time 729, loss -, compute time 10.47\n",
      "episode 665, reward 402, memory_length 2000, epsilon 0.54964, total time 724, loss -, compute time 11.34\n",
      "episode 666, reward 817, memory_length 2000, epsilon 0.54914, total time 721, loss [2.017730951309204], compute time 12.96\n",
      "episode 667, reward 675, memory_length 2000, epsilon 0.54865, total time 724, loss [103.83203125], compute time 12.35\n",
      "episode 668, reward 578, memory_length 2000, epsilon 0.54815, total time 726, loss [205.45437622070312], compute time 12.52\n",
      "episode 669, reward 884, memory_length 2000, epsilon 0.54766, total time 722, loss [443.14630126953125], compute time 12.68\n",
      "episode 670, reward 497, memory_length 2000, epsilon 0.54717, total time 725, loss [5.024255752563477], compute time 10.48\n",
      "Saving model for episode: 670\n",
      "episode 671, reward 648, memory_length 2000, epsilon 0.54668, total time 725, loss [106.97349548339844], compute time 11.4\n",
      "episode 672, reward 579, memory_length 2000, epsilon 0.54618, total time 722, loss -, compute time 11.58\n",
      "episode 673, reward 301, memory_length 2000, epsilon 0.54569, total time 727, loss -, compute time 10.83\n",
      "episode 674, reward 596, memory_length 2000, epsilon 0.5452, total time 726, loss [126.34001159667969], compute time 11.75\n",
      "episode 675, reward 478, memory_length 2000, epsilon 0.54471, total time 733, loss [427.1259765625], compute time 11.32\n",
      "episode 676, reward 460, memory_length 2000, epsilon 0.54422, total time 729, loss -, compute time 12.92\n",
      "episode 677, reward 848, memory_length 2000, epsilon 0.54373, total time 722, loss [106.0814208984375], compute time 12.66\n",
      "episode 678, reward 392, memory_length 2000, epsilon 0.54324, total time 726, loss [218.48329162597656], compute time 12.45\n",
      "episode 679, reward 612, memory_length 2000, epsilon 0.54275, total time 724, loss [3.9245431423187256], compute time 11.11\n",
      "episode 680, reward 482, memory_length 2000, epsilon 0.54227, total time 723, loss [4.936256408691406], compute time 12.35\n",
      "Saving model for episode: 680\n",
      "episode 681, reward 853, memory_length 2000, epsilon 0.54178, total time 721, loss -, compute time 11.24\n",
      "episode 682, reward 333, memory_length 2000, epsilon 0.54129, total time 722, loss [133.9454803466797], compute time 11.71\n",
      "episode 683, reward 585, memory_length 2000, epsilon 0.5408, total time 722, loss [518.7597045898438], compute time 12.71\n",
      "episode 684, reward 684, memory_length 2000, epsilon 0.54032, total time 729, loss -, compute time 12.88\n",
      "episode 685, reward 499, memory_length 2000, epsilon 0.53983, total time 722, loss -, compute time 11.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 686, reward 947, memory_length 2000, epsilon 0.53934, total time 724, loss [112.2437515258789], compute time 13.38\n",
      "episode 687, reward 758, memory_length 2000, epsilon 0.53886, total time 722, loss [130.28407287597656], compute time 11.85\n",
      "episode 688, reward 282, memory_length 2000, epsilon 0.53837, total time 721, loss -, compute time 12.92\n",
      "episode 689, reward 825, memory_length 2000, epsilon 0.53789, total time 721, loss -, compute time 12.29\n",
      "episode 690, reward 676, memory_length 2000, epsilon 0.53741, total time 732, loss [2.229306221008301], compute time 12.07\n",
      "Saving model for episode: 690\n",
      "episode 691, reward 853, memory_length 2000, epsilon 0.53692, total time 724, loss -, compute time 11.92\n",
      "episode 692, reward 331, memory_length 2000, epsilon 0.53644, total time 721, loss [2.816516399383545], compute time 13.35\n",
      "episode 693, reward 642, memory_length 2000, epsilon 0.53596, total time 722, loss -, compute time 12.1\n",
      "episode 694, reward 819, memory_length 2000, epsilon 0.53548, total time 726, loss [1.8135061264038086], compute time 12.01\n",
      "episode 695, reward 250, memory_length 2000, epsilon 0.53499, total time 723, loss [181.35699462890625], compute time 12.76\n",
      "episode 696, reward 662, memory_length 2000, epsilon 0.53451, total time 730, loss -, compute time 13.93\n",
      "episode 697, reward 307, memory_length 2000, epsilon 0.53403, total time 733, loss [128.74513244628906], compute time 12.28\n",
      "episode 698, reward 828, memory_length 2000, epsilon 0.53355, total time 722, loss -, compute time 13.18\n",
      "episode 699, reward 405, memory_length 2000, epsilon 0.53307, total time 729, loss -, compute time 13.24\n",
      "episode 700, reward 1004, memory_length 2000, epsilon 0.53259, total time 723, loss -, compute time 12.04\n",
      "Saving model for episode: 700\n",
      "episode 701, reward 489, memory_length 2000, epsilon 0.53211, total time 722, loss [116.526123046875], compute time 13.34\n",
      "episode 702, reward 139, memory_length 2000, epsilon 0.53163, total time 724, loss -, compute time 12.08\n",
      "episode 703, reward 641, memory_length 2000, epsilon 0.53116, total time 721, loss -, compute time 11.83\n",
      "episode 704, reward 725, memory_length 2000, epsilon 0.53068, total time 721, loss [140.71092224121094], compute time 10.55\n",
      "episode 705, reward 622, memory_length 2000, epsilon 0.5302, total time 725, loss [369.0235595703125], compute time 11.13\n",
      "episode 706, reward 866, memory_length 2000, epsilon 0.52972, total time 721, loss -, compute time 12.52\n",
      "episode 707, reward 312, memory_length 2000, epsilon 0.52925, total time 726, loss -, compute time 12.66\n",
      "episode 708, reward 675, memory_length 2000, epsilon 0.52877, total time 721, loss [213.4706573486328], compute time 12.72\n",
      "episode 709, reward 311, memory_length 2000, epsilon 0.5283, total time 726, loss [4.402044773101807], compute time 11.78\n",
      "episode 710, reward 855, memory_length 2000, epsilon 0.52782, total time 721, loss [104.72917175292969], compute time 12.76\n",
      "Saving model for episode: 710\n",
      "episode 711, reward 893, memory_length 2000, epsilon 0.52735, total time 725, loss -, compute time 11.93\n",
      "episode 712, reward 678, memory_length 2000, epsilon 0.52687, total time 725, loss [229.6367950439453], compute time 12.07\n",
      "episode 713, reward 425, memory_length 2000, epsilon 0.5264, total time 722, loss -, compute time 10.97\n",
      "episode 714, reward 581, memory_length 2000, epsilon 0.52592, total time 726, loss [108.97293853759766], compute time 12.47\n",
      "episode 715, reward 638, memory_length 2000, epsilon 0.52545, total time 723, loss -, compute time 12.24\n",
      "episode 716, reward 707, memory_length 2000, epsilon 0.52498, total time 723, loss [3.2181732654571533], compute time 11.52\n",
      "episode 717, reward 311, memory_length 2000, epsilon 0.52451, total time 725, loss -, compute time 11.3\n",
      "episode 718, reward 564, memory_length 2000, epsilon 0.52403, total time 722, loss [165.58770751953125], compute time 11.96\n",
      "episode 719, reward 747, memory_length 2000, epsilon 0.52356, total time 721, loss [306.45196533203125], compute time 11.73\n",
      "episode 720, reward 597, memory_length 2000, epsilon 0.52309, total time 722, loss [101.14338684082031], compute time 12.03\n",
      "Saving model for episode: 720\n",
      "episode 721, reward 805, memory_length 2000, epsilon 0.52262, total time 722, loss -, compute time 12.16\n",
      "episode 722, reward 582, memory_length 2000, epsilon 0.52215, total time 721, loss -, compute time 12.56\n",
      "episode 723, reward 556, memory_length 2000, epsilon 0.52168, total time 726, loss [364.6559753417969], compute time 12.31\n",
      "episode 724, reward 522, memory_length 2000, epsilon 0.52121, total time 726, loss -, compute time 12.9\n",
      "episode 725, reward 540, memory_length 2000, epsilon 0.52074, total time 727, loss -, compute time 12.06\n",
      "episode 726, reward 914, memory_length 2000, epsilon 0.52027, total time 728, loss -, compute time 12.98\n",
      "episode 727, reward 423, memory_length 2000, epsilon 0.51981, total time 726, loss [3.1332101821899414], compute time 11.17\n",
      "episode 728, reward 860, memory_length 2000, epsilon 0.51934, total time 727, loss -, compute time 11.8\n",
      "episode 729, reward 568, memory_length 2000, epsilon 0.51887, total time 724, loss [258.54425048828125], compute time 12.1\n",
      "episode 730, reward 513, memory_length 2000, epsilon 0.5184, total time 726, loss -, compute time 11.06\n",
      "Saving model for episode: 730\n",
      "episode 731, reward 497, memory_length 2000, epsilon 0.51794, total time 729, loss [95.3904800415039], compute time 12.06\n",
      "episode 732, reward 537, memory_length 2000, epsilon 0.51747, total time 726, loss -, compute time 12.77\n",
      "episode 733, reward 607, memory_length 2000, epsilon 0.51701, total time 725, loss [1.827553629875183], compute time 13.49\n",
      "episode 734, reward 572, memory_length 2000, epsilon 0.51654, total time 721, loss [249.7740478515625], compute time 13.16\n",
      "episode 735, reward 822, memory_length 2000, epsilon 0.51608, total time 723, loss -, compute time 12.25\n",
      "episode 736, reward 663, memory_length 2000, epsilon 0.51561, total time 722, loss [91.64567565917969], compute time 12.61\n",
      "episode 737, reward 620, memory_length 2000, epsilon 0.51515, total time 723, loss -, compute time 11.63\n",
      "episode 738, reward 746, memory_length 2000, epsilon 0.51469, total time 722, loss [192.01708984375], compute time 13.54\n",
      "episode 739, reward 887, memory_length 2000, epsilon 0.51422, total time 727, loss -, compute time 11.46\n",
      "episode 740, reward 736, memory_length 2000, epsilon 0.51376, total time 722, loss [2.8878345489501953], compute time 13.14\n",
      "Saving model for episode: 740\n",
      "episode 741, reward 810, memory_length 2000, epsilon 0.5133, total time 723, loss [324.2035217285156], compute time 12.54\n",
      "episode 742, reward 662, memory_length 2000, epsilon 0.51284, total time 725, loss [2.806941270828247], compute time 12.67\n",
      "episode 743, reward 772, memory_length 2000, epsilon 0.51237, total time 723, loss -, compute time 11.26\n",
      "episode 744, reward 784, memory_length 2000, epsilon 0.51191, total time 724, loss -, compute time 12.17\n",
      "episode 745, reward 1069, memory_length 2000, epsilon 0.51145, total time 721, loss -, compute time 13.16\n",
      "episode 746, reward 950, memory_length 2000, epsilon 0.51099, total time 727, loss -, compute time 12.11\n",
      "episode 747, reward 668, memory_length 2000, epsilon 0.51053, total time 722, loss [88.22406768798828], compute time 11.43\n",
      "episode 748, reward 648, memory_length 2000, epsilon 0.51007, total time 726, loss -, compute time 12.11\n",
      "episode 749, reward 420, memory_length 2000, epsilon 0.50961, total time 721, loss -, compute time 11.18\n",
      "episode 750, reward 702, memory_length 2000, epsilon 0.50916, total time 725, loss -, compute time 12.3\n",
      "Saving model for episode: 750\n",
      "episode 751, reward 918, memory_length 2000, epsilon 0.5087, total time 728, loss -, compute time 10.99\n",
      "episode 752, reward 821, memory_length 2000, epsilon 0.50824, total time 722, loss -, compute time 12.15\n",
      "episode 753, reward 701, memory_length 2000, epsilon 0.50778, total time 728, loss [169.55857849121094], compute time 10.79\n",
      "episode 754, reward 557, memory_length 2000, epsilon 0.50733, total time 723, loss -, compute time 11.6\n",
      "episode 755, reward 703, memory_length 2000, epsilon 0.50687, total time 726, loss [205.15869140625], compute time 12.3\n",
      "episode 756, reward 736, memory_length 2000, epsilon 0.50641, total time 722, loss [281.4068908691406], compute time 13.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 757, reward 674, memory_length 2000, epsilon 0.50596, total time 726, loss -, compute time 12.09\n",
      "episode 758, reward 1209, memory_length 2000, epsilon 0.5055, total time 723, loss [2.278658866882324], compute time 12.84\n",
      "episode 759, reward 540, memory_length 2000, epsilon 0.50505, total time 721, loss -, compute time 12.1\n",
      "episode 760, reward 761, memory_length 2000, epsilon 0.50459, total time 723, loss -, compute time 12.03\n",
      "Saving model for episode: 760\n",
      "episode 761, reward 635, memory_length 2000, epsilon 0.50414, total time 721, loss -, compute time 11.37\n",
      "episode 762, reward 838, memory_length 2000, epsilon 0.50369, total time 728, loss -, compute time 10.77\n",
      "episode 763, reward 742, memory_length 2000, epsilon 0.50323, total time 723, loss [245.59823608398438], compute time 11.3\n",
      "episode 764, reward 505, memory_length 2000, epsilon 0.50278, total time 728, loss -, compute time 12.06\n",
      "episode 765, reward 520, memory_length 2000, epsilon 0.50233, total time 724, loss [500.074951171875], compute time 11.58\n",
      "episode 766, reward 530, memory_length 2000, epsilon 0.50188, total time 723, loss -, compute time 11.1\n",
      "episode 767, reward 903, memory_length 2000, epsilon 0.50143, total time 722, loss -, compute time 13.08\n",
      "episode 768, reward 1008, memory_length 2000, epsilon 0.50097, total time 730, loss -, compute time 12.46\n",
      "episode 769, reward 885, memory_length 2000, epsilon 0.50052, total time 725, loss [108.24231719970703], compute time 12.65\n",
      "episode 770, reward 650, memory_length 2000, epsilon 0.50007, total time 724, loss [216.4641876220703], compute time 13.37\n",
      "Saving model for episode: 770\n",
      "episode 771, reward 509, memory_length 2000, epsilon 0.49962, total time 721, loss -, compute time 12.1\n",
      "episode 772, reward 693, memory_length 2000, epsilon 0.49917, total time 727, loss [441.845947265625], compute time 12.28\n",
      "episode 773, reward 689, memory_length 2000, epsilon 0.49873, total time 728, loss -, compute time 13.08\n",
      "episode 774, reward 563, memory_length 2000, epsilon 0.49828, total time 725, loss [243.61282348632812], compute time 11.87\n",
      "episode 775, reward 691, memory_length 2000, epsilon 0.49783, total time 725, loss -, compute time 10.84\n",
      "episode 776, reward 462, memory_length 2000, epsilon 0.49738, total time 722, loss -, compute time 13.03\n",
      "episode 777, reward 735, memory_length 2000, epsilon 0.49693, total time 726, loss -, compute time 12.03\n",
      "episode 778, reward 752, memory_length 2000, epsilon 0.49649, total time 724, loss [3.5767269134521484], compute time 12.95\n",
      "episode 779, reward 698, memory_length 2000, epsilon 0.49604, total time 725, loss -, compute time 12.47\n",
      "episode 780, reward 727, memory_length 2000, epsilon 0.49559, total time 722, loss [3.1956429481506348], compute time 12.32\n",
      "Saving model for episode: 780\n",
      "episode 781, reward 940, memory_length 2000, epsilon 0.49515, total time 723, loss -, compute time 11.64\n",
      "episode 782, reward 648, memory_length 2000, epsilon 0.4947, total time 723, loss [209.84132385253906], compute time 11.4\n",
      "episode 783, reward 696, memory_length 2000, epsilon 0.49426, total time 724, loss -, compute time 12.27\n",
      "episode 784, reward 612, memory_length 2000, epsilon 0.49381, total time 726, loss [114.29965209960938], compute time 10.84\n",
      "episode 785, reward 907, memory_length 2000, epsilon 0.49337, total time 723, loss -, compute time 12.57\n",
      "episode 786, reward 545, memory_length 2000, epsilon 0.49292, total time 721, loss [68.73736572265625], compute time 11.79\n",
      "episode 787, reward 601, memory_length 2000, epsilon 0.49248, total time 723, loss [188.98837280273438], compute time 12.51\n",
      "episode 788, reward 638, memory_length 2000, epsilon 0.49204, total time 723, loss [339.6863708496094], compute time 11.39\n",
      "episode 789, reward 510, memory_length 2000, epsilon 0.4916, total time 723, loss [209.73757934570312], compute time 10.74\n",
      "episode 790, reward 653, memory_length 2000, epsilon 0.49115, total time 726, loss [385.00836181640625], compute time 13.33\n",
      "Saving model for episode: 790\n",
      "episode 791, reward 511, memory_length 2000, epsilon 0.49071, total time 722, loss -, compute time 11.43\n",
      "episode 792, reward 581, memory_length 2000, epsilon 0.49027, total time 721, loss [411.13885498046875], compute time 10.93\n",
      "episode 793, reward 894, memory_length 2000, epsilon 0.48983, total time 721, loss [191.83135986328125], compute time 13.34\n",
      "episode 794, reward 455, memory_length 2000, epsilon 0.48939, total time 727, loss [320.5210266113281], compute time 11.56\n",
      "episode 795, reward 806, memory_length 2000, epsilon 0.48895, total time 727, loss -, compute time 11.56\n",
      "episode 796, reward 477, memory_length 2000, epsilon 0.48851, total time 725, loss -, compute time 11.98\n",
      "episode 797, reward 228, memory_length 2000, epsilon 0.48807, total time 723, loss -, compute time 12.97\n",
      "episode 798, reward 662, memory_length 2000, epsilon 0.48763, total time 725, loss -, compute time 10.62\n",
      "episode 799, reward 693, memory_length 2000, epsilon 0.48719, total time 721, loss -, compute time 11.72\n",
      "episode 800, reward 500, memory_length 2000, epsilon 0.48675, total time 725, loss [168.82167053222656], compute time 10.93\n",
      "Saving model for episode: 800\n",
      "episode 801, reward 313, memory_length 2000, epsilon 0.48631, total time 723, loss -, compute time 12.22\n",
      "episode 802, reward 729, memory_length 2000, epsilon 0.48588, total time 723, loss [213.2234344482422], compute time 13.29\n",
      "episode 803, reward 441, memory_length 2000, epsilon 0.48544, total time 727, loss [6.039327621459961], compute time 13.08\n",
      "episode 804, reward 768, memory_length 2000, epsilon 0.485, total time 723, loss -, compute time 12.1\n",
      "episode 805, reward 438, memory_length 2000, epsilon 0.48457, total time 721, loss -, compute time 12.92\n",
      "episode 806, reward 563, memory_length 2000, epsilon 0.48413, total time 721, loss [473.4547424316406], compute time 12.48\n",
      "episode 807, reward 593, memory_length 2000, epsilon 0.4837, total time 724, loss [75.72074127197266], compute time 11.75\n",
      "episode 808, reward 896, memory_length 2000, epsilon 0.48326, total time 725, loss -, compute time 12.36\n",
      "episode 809, reward 973, memory_length 2000, epsilon 0.48283, total time 733, loss -, compute time 11.12\n",
      "episode 810, reward 811, memory_length 2000, epsilon 0.48239, total time 726, loss -, compute time 12.04\n",
      "Saving model for episode: 810\n",
      "episode 811, reward 700, memory_length 2000, epsilon 0.48196, total time 723, loss [97.4892578125], compute time 11.59\n",
      "episode 812, reward 653, memory_length 2000, epsilon 0.48152, total time 721, loss [632.8731689453125], compute time 11.77\n",
      "episode 813, reward 448, memory_length 2000, epsilon 0.48109, total time 721, loss -, compute time 12.78\n",
      "episode 814, reward 739, memory_length 2000, epsilon 0.48066, total time 727, loss [103.29442596435547], compute time 13.15\n",
      "episode 815, reward 623, memory_length 2000, epsilon 0.48023, total time 724, loss -, compute time 12.21\n",
      "episode 816, reward 1073, memory_length 2000, epsilon 0.47979, total time 724, loss -, compute time 12.34\n",
      "episode 817, reward 776, memory_length 2000, epsilon 0.47936, total time 724, loss -, compute time 13.99\n",
      "episode 818, reward 612, memory_length 2000, epsilon 0.47893, total time 734, loss [207.29598999023438], compute time 12.95\n",
      "episode 819, reward 977, memory_length 2000, epsilon 0.4785, total time 722, loss -, compute time 11.91\n",
      "episode 820, reward 1089, memory_length 2000, epsilon 0.47807, total time 728, loss [202.9093017578125], compute time 13.37\n",
      "Saving model for episode: 820\n",
      "episode 821, reward 631, memory_length 2000, epsilon 0.47764, total time 730, loss [93.68405151367188], compute time 12.57\n",
      "episode 822, reward 552, memory_length 2000, epsilon 0.47721, total time 721, loss -, compute time 11.39\n",
      "episode 823, reward 794, memory_length 2000, epsilon 0.47678, total time 721, loss [183.27713012695312], compute time 11.94\n",
      "episode 824, reward 882, memory_length 2000, epsilon 0.47635, total time 731, loss -, compute time 12.58\n",
      "episode 825, reward 702, memory_length 2000, epsilon 0.47592, total time 721, loss -, compute time 13.21\n",
      "episode 826, reward 626, memory_length 2000, epsilon 0.47549, total time 730, loss [191.75851440429688], compute time 12.58\n",
      "episode 827, reward 836, memory_length 2000, epsilon 0.47507, total time 723, loss -, compute time 14.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 828, reward 869, memory_length 2000, epsilon 0.47464, total time 729, loss [252.8213348388672], compute time 13.09\n",
      "episode 829, reward 491, memory_length 2000, epsilon 0.47421, total time 729, loss -, compute time 10.58\n",
      "episode 830, reward 324, memory_length 2000, epsilon 0.47379, total time 721, loss -, compute time 13.34\n",
      "Saving model for episode: 830\n",
      "episode 831, reward 666, memory_length 2000, epsilon 0.47336, total time 721, loss -, compute time 12.21\n",
      "episode 832, reward 904, memory_length 2000, epsilon 0.47293, total time 722, loss -, compute time 13.25\n",
      "episode 833, reward 773, memory_length 2000, epsilon 0.47251, total time 724, loss [132.5283966064453], compute time 11.33\n",
      "episode 834, reward 788, memory_length 2000, epsilon 0.47208, total time 724, loss -, compute time 13.37\n",
      "episode 835, reward 921, memory_length 2000, epsilon 0.47166, total time 724, loss -, compute time 13.08\n",
      "episode 836, reward 1132, memory_length 2000, epsilon 0.47123, total time 724, loss -, compute time 12.2\n",
      "episode 837, reward 946, memory_length 2000, epsilon 0.47081, total time 725, loss -, compute time 12.19\n",
      "episode 838, reward 874, memory_length 2000, epsilon 0.47039, total time 727, loss -, compute time 13.08\n",
      "episode 839, reward 1206, memory_length 2000, epsilon 0.46996, total time 721, loss [169.1737823486328], compute time 11.69\n",
      "episode 840, reward 605, memory_length 2000, epsilon 0.46954, total time 723, loss [215.22262573242188], compute time 13.16\n",
      "Saving model for episode: 840\n",
      "episode 841, reward 857, memory_length 2000, epsilon 0.46912, total time 729, loss [82.1699447631836], compute time 12.69\n",
      "episode 842, reward 750, memory_length 2000, epsilon 0.4687, total time 723, loss [4.692661285400391], compute time 11.55\n",
      "episode 843, reward 496, memory_length 2000, epsilon 0.46827, total time 724, loss [195.74339294433594], compute time 11.73\n",
      "episode 844, reward 788, memory_length 2000, epsilon 0.46785, total time 725, loss -, compute time 14.03\n",
      "episode 845, reward 818, memory_length 2000, epsilon 0.46743, total time 729, loss [249.79531860351562], compute time 13.6\n",
      "episode 846, reward 764, memory_length 2000, epsilon 0.46701, total time 721, loss -, compute time 13.36\n",
      "episode 847, reward 803, memory_length 2000, epsilon 0.46659, total time 721, loss [178.1782989501953], compute time 12.69\n",
      "episode 848, reward 892, memory_length 2000, epsilon 0.46617, total time 723, loss -, compute time 12.71\n",
      "episode 849, reward 648, memory_length 2000, epsilon 0.46575, total time 721, loss [232.62362670898438], compute time 12.36\n",
      "episode 850, reward 298, memory_length 2000, epsilon 0.46533, total time 721, loss -, compute time 11.57\n",
      "Saving model for episode: 850\n",
      "episode 851, reward 1043, memory_length 2000, epsilon 0.46492, total time 721, loss -, compute time 13.28\n",
      "episode 852, reward 747, memory_length 2000, epsilon 0.4645, total time 721, loss [112.12947082519531], compute time 11.93\n",
      "episode 853, reward 789, memory_length 2000, epsilon 0.46408, total time 726, loss [3.78902268409729], compute time 11.51\n",
      "episode 854, reward 962, memory_length 2000, epsilon 0.46366, total time 724, loss [77.9886703491211], compute time 13.35\n",
      "episode 855, reward 788, memory_length 2000, epsilon 0.46324, total time 721, loss [292.2400817871094], compute time 11.19\n",
      "episode 856, reward 484, memory_length 2000, epsilon 0.46283, total time 725, loss -, compute time 12.07\n",
      "episode 857, reward 793, memory_length 2000, epsilon 0.46241, total time 724, loss -, compute time 12.94\n",
      "episode 858, reward 621, memory_length 2000, epsilon 0.462, total time 722, loss [105.72481536865234], compute time 11.58\n",
      "episode 859, reward 668, memory_length 2000, epsilon 0.46158, total time 725, loss -, compute time 12.03\n",
      "episode 860, reward 1220, memory_length 2000, epsilon 0.46116, total time 723, loss -, compute time 11.89\n",
      "Saving model for episode: 860\n",
      "episode 861, reward 978, memory_length 2000, epsilon 0.46075, total time 723, loss -, compute time 13.11\n",
      "episode 862, reward 1107, memory_length 2000, epsilon 0.46034, total time 722, loss [200.60409545898438], compute time 12.64\n",
      "episode 863, reward 834, memory_length 2000, epsilon 0.45992, total time 728, loss [291.3306579589844], compute time 12.66\n",
      "episode 864, reward 577, memory_length 2000, epsilon 0.45951, total time 725, loss -, compute time 11.93\n",
      "episode 865, reward 763, memory_length 2000, epsilon 0.45909, total time 722, loss -, compute time 12.87\n",
      "episode 866, reward 754, memory_length 2000, epsilon 0.45868, total time 723, loss [2.469041109085083], compute time 11.36\n",
      "episode 867, reward 457, memory_length 2000, epsilon 0.45827, total time 721, loss [118.35612487792969], compute time 12.18\n",
      "episode 868, reward 887, memory_length 2000, epsilon 0.45786, total time 724, loss [314.7972717285156], compute time 12.75\n",
      "episode 869, reward 830, memory_length 2000, epsilon 0.45744, total time 725, loss [297.0556640625], compute time 10.94\n",
      "episode 870, reward 594, memory_length 2000, epsilon 0.45703, total time 728, loss [220.17123413085938], compute time 12.46\n",
      "Saving model for episode: 870\n",
      "episode 871, reward 1076, memory_length 2000, epsilon 0.45662, total time 726, loss -, compute time 12.51\n",
      "episode 872, reward 910, memory_length 2000, epsilon 0.45621, total time 725, loss -, compute time 11.88\n",
      "episode 873, reward 732, memory_length 2000, epsilon 0.4558, total time 723, loss [109.47541046142578], compute time 11.92\n",
      "episode 874, reward 467, memory_length 2000, epsilon 0.45539, total time 723, loss -, compute time 11.31\n",
      "episode 875, reward 299, memory_length 2000, epsilon 0.45498, total time 724, loss [2.8713173866271973], compute time 11.62\n",
      "episode 876, reward 948, memory_length 2000, epsilon 0.45457, total time 725, loss -, compute time 12.29\n",
      "episode 877, reward 721, memory_length 2000, epsilon 0.45416, total time 722, loss [192.8531494140625], compute time 12.7\n",
      "episode 878, reward 662, memory_length 2000, epsilon 0.45375, total time 731, loss -, compute time 12.43\n",
      "episode 879, reward 763, memory_length 2000, epsilon 0.45335, total time 729, loss -, compute time 12.79\n",
      "episode 880, reward 887, memory_length 2000, epsilon 0.45294, total time 724, loss [110.24604797363281], compute time 12.86\n",
      "Saving model for episode: 880\n",
      "episode 881, reward 559, memory_length 2000, epsilon 0.45253, total time 726, loss -, compute time 12.87\n",
      "episode 882, reward 1036, memory_length 2000, epsilon 0.45212, total time 722, loss -, compute time 11.08\n",
      "episode 883, reward 441, memory_length 2000, epsilon 0.45172, total time 727, loss -, compute time 11.11\n",
      "episode 884, reward 681, memory_length 2000, epsilon 0.45131, total time 722, loss -, compute time 12.23\n",
      "episode 885, reward 786, memory_length 2000, epsilon 0.4509, total time 727, loss -, compute time 12.95\n",
      "episode 886, reward 653, memory_length 2000, epsilon 0.4505, total time 728, loss -, compute time 12.8\n",
      "episode 887, reward 905, memory_length 2000, epsilon 0.45009, total time 722, loss -, compute time 12.81\n",
      "episode 888, reward 791, memory_length 2000, epsilon 0.44969, total time 723, loss [1.4267096519470215], compute time 11.31\n",
      "episode 889, reward 751, memory_length 2000, epsilon 0.44928, total time 721, loss [94.83448791503906], compute time 11.87\n",
      "episode 890, reward 819, memory_length 2000, epsilon 0.44888, total time 724, loss -, compute time 14.73\n",
      "Saving model for episode: 890\n",
      "episode 891, reward 968, memory_length 2000, epsilon 0.44848, total time 725, loss -, compute time 11.92\n",
      "episode 892, reward 533, memory_length 2000, epsilon 0.44807, total time 722, loss -, compute time 10.62\n",
      "episode 893, reward 964, memory_length 2000, epsilon 0.44767, total time 725, loss -, compute time 11.59\n",
      "episode 894, reward 824, memory_length 2000, epsilon 0.44727, total time 730, loss -, compute time 13.57\n",
      "episode 895, reward 1066, memory_length 2000, epsilon 0.44686, total time 721, loss -, compute time 12.55\n",
      "episode 896, reward 1053, memory_length 2000, epsilon 0.44646, total time 731, loss -, compute time 12.05\n",
      "episode 897, reward 795, memory_length 2000, epsilon 0.44606, total time 722, loss -, compute time 12.04\n",
      "episode 898, reward 773, memory_length 2000, epsilon 0.44566, total time 721, loss [180.73226928710938], compute time 11.8\n",
      "episode 899, reward 961, memory_length 2000, epsilon 0.44526, total time 726, loss -, compute time 13.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 900, reward 708, memory_length 2000, epsilon 0.44486, total time 726, loss -, compute time 13.42\n",
      "Saving model for episode: 900\n",
      "episode 901, reward 1116, memory_length 2000, epsilon 0.44446, total time 721, loss -, compute time 12.73\n",
      "episode 902, reward 747, memory_length 2000, epsilon 0.44406, total time 722, loss [319.11761474609375], compute time 11.77\n",
      "episode 903, reward 576, memory_length 2000, epsilon 0.44366, total time 721, loss -, compute time 12.24\n",
      "episode 904, reward 512, memory_length 2000, epsilon 0.44326, total time 727, loss [108.12428283691406], compute time 13.23\n",
      "episode 905, reward 864, memory_length 2000, epsilon 0.44286, total time 721, loss [3.1194534301757812], compute time 11.24\n",
      "episode 906, reward 1324, memory_length 2000, epsilon 0.44246, total time 724, loss -, compute time 12.09\n",
      "episode 907, reward 985, memory_length 2000, epsilon 0.44206, total time 723, loss -, compute time 12.6\n",
      "episode 908, reward 1116, memory_length 2000, epsilon 0.44167, total time 726, loss [204.13436889648438], compute time 12.37\n",
      "episode 909, reward 898, memory_length 2000, epsilon 0.44127, total time 724, loss -, compute time 12.66\n",
      "episode 910, reward 1078, memory_length 2000, epsilon 0.44087, total time 723, loss -, compute time 12.04\n",
      "Saving model for episode: 910\n",
      "episode 911, reward 1016, memory_length 2000, epsilon 0.44048, total time 724, loss -, compute time 12.8\n",
      "episode 912, reward 1179, memory_length 2000, epsilon 0.44008, total time 725, loss [128.6845703125], compute time 13.19\n",
      "episode 913, reward 1027, memory_length 2000, epsilon 0.43968, total time 727, loss -, compute time 13.1\n",
      "episode 914, reward 1136, memory_length 2000, epsilon 0.43929, total time 726, loss [383.10986328125], compute time 12.98\n",
      "episode 915, reward 317, memory_length 2000, epsilon 0.43889, total time 721, loss -, compute time 12.52\n",
      "episode 916, reward 1076, memory_length 2000, epsilon 0.4385, total time 725, loss -, compute time 11.72\n",
      "episode 917, reward 547, memory_length 2000, epsilon 0.4381, total time 723, loss [333.0582580566406], compute time 11.93\n",
      "episode 918, reward 694, memory_length 2000, epsilon 0.43771, total time 721, loss -, compute time 12.44\n",
      "episode 919, reward 855, memory_length 2000, epsilon 0.43732, total time 725, loss [327.8742980957031], compute time 12.79\n",
      "episode 920, reward 774, memory_length 2000, epsilon 0.43692, total time 721, loss -, compute time 12.86\n",
      "Saving model for episode: 920\n",
      "episode 921, reward 837, memory_length 2000, epsilon 0.43653, total time 728, loss [2.2872118949890137], compute time 12.55\n",
      "episode 922, reward 774, memory_length 2000, epsilon 0.43614, total time 725, loss [312.9190673828125], compute time 13.87\n",
      "episode 923, reward 999, memory_length 2000, epsilon 0.43574, total time 721, loss -, compute time 11.77\n",
      "episode 924, reward 1005, memory_length 2000, epsilon 0.43535, total time 730, loss -, compute time 12.91\n",
      "episode 925, reward 692, memory_length 2000, epsilon 0.43496, total time 722, loss -, compute time 11.45\n",
      "episode 926, reward 856, memory_length 2000, epsilon 0.43457, total time 721, loss -, compute time 12.46\n",
      "episode 927, reward 632, memory_length 2000, epsilon 0.43418, total time 725, loss -, compute time 12.42\n",
      "episode 928, reward 613, memory_length 2000, epsilon 0.43379, total time 727, loss -, compute time 12.72\n",
      "episode 929, reward 927, memory_length 2000, epsilon 0.4334, total time 729, loss [265.07806396484375], compute time 12.77\n",
      "episode 930, reward 956, memory_length 2000, epsilon 0.43301, total time 722, loss [298.00286865234375], compute time 12.1\n",
      "Saving model for episode: 930\n",
      "episode 931, reward 853, memory_length 2000, epsilon 0.43262, total time 725, loss [96.745849609375], compute time 11.39\n",
      "episode 932, reward 946, memory_length 2000, epsilon 0.43223, total time 729, loss -, compute time 14.54\n",
      "episode 933, reward 961, memory_length 2000, epsilon 0.43184, total time 723, loss [206.12501525878906], compute time 11.17\n",
      "episode 934, reward 555, memory_length 2000, epsilon 0.43145, total time 722, loss -, compute time 12.81\n",
      "episode 935, reward 914, memory_length 2000, epsilon 0.43106, total time 721, loss -, compute time 12.17\n",
      "episode 936, reward 549, memory_length 2000, epsilon 0.43068, total time 727, loss [115.40165710449219], compute time 13.39\n",
      "episode 937, reward 1026, memory_length 2000, epsilon 0.43029, total time 724, loss -, compute time 12.62\n",
      "episode 938, reward 911, memory_length 2000, epsilon 0.4299, total time 723, loss [110.24653625488281], compute time 13.44\n",
      "episode 939, reward 1053, memory_length 2000, epsilon 0.42951, total time 724, loss -, compute time 11.17\n",
      "episode 940, reward 598, memory_length 2000, epsilon 0.42913, total time 721, loss -, compute time 12.02\n",
      "Saving model for episode: 940\n",
      "episode 941, reward 819, memory_length 2000, epsilon 0.42874, total time 726, loss -, compute time 12.46\n",
      "episode 942, reward 668, memory_length 2000, epsilon 0.42836, total time 723, loss -, compute time 12.02\n",
      "episode 943, reward 708, memory_length 2000, epsilon 0.42797, total time 721, loss [104.82905578613281], compute time 11.79\n",
      "episode 944, reward 630, memory_length 2000, epsilon 0.42759, total time 721, loss [215.07203674316406], compute time 12.34\n",
      "episode 945, reward 1195, memory_length 2000, epsilon 0.4272, total time 722, loss -, compute time 11.61\n",
      "episode 946, reward 832, memory_length 2000, epsilon 0.42682, total time 723, loss -, compute time 12.54\n",
      "episode 947, reward 635, memory_length 2000, epsilon 0.42643, total time 726, loss -, compute time 14.18\n",
      "episode 948, reward 948, memory_length 2000, epsilon 0.42605, total time 721, loss -, compute time 12.48\n",
      "episode 949, reward 608, memory_length 2000, epsilon 0.42567, total time 728, loss -, compute time 13.52\n",
      "episode 950, reward 751, memory_length 2000, epsilon 0.42528, total time 723, loss -, compute time 12.11\n",
      "Saving model for episode: 950\n",
      "episode 951, reward 1063, memory_length 2000, epsilon 0.4249, total time 721, loss [154.07518005371094], compute time 11.89\n",
      "episode 952, reward 768, memory_length 2000, epsilon 0.42452, total time 721, loss [104.4251708984375], compute time 11.99\n",
      "episode 953, reward 825, memory_length 2000, epsilon 0.42414, total time 725, loss -, compute time 12.61\n",
      "episode 954, reward 747, memory_length 2000, epsilon 0.42375, total time 731, loss -, compute time 12.79\n",
      "episode 955, reward 932, memory_length 2000, epsilon 0.42337, total time 732, loss [113.01036071777344], compute time 12.58\n",
      "episode 956, reward 756, memory_length 2000, epsilon 0.42299, total time 732, loss -, compute time 12.59\n",
      "episode 957, reward 901, memory_length 2000, epsilon 0.42261, total time 721, loss [2.09906268119812], compute time 12.32\n",
      "episode 958, reward 673, memory_length 2000, epsilon 0.42223, total time 727, loss -, compute time 14.51\n",
      "episode 959, reward 767, memory_length 2000, epsilon 0.42185, total time 725, loss [312.3083190917969], compute time 11.88\n",
      "episode 960, reward 840, memory_length 2000, epsilon 0.42147, total time 722, loss -, compute time 12.96\n",
      "Saving model for episode: 960\n",
      "episode 961, reward 851, memory_length 2000, epsilon 0.42109, total time 725, loss -, compute time 11.88\n",
      "episode 962, reward 1064, memory_length 2000, epsilon 0.42071, total time 725, loss -, compute time 12.5\n",
      "episode 963, reward 653, memory_length 2000, epsilon 0.42034, total time 725, loss [107.52184295654297], compute time 11.8\n",
      "episode 964, reward 1109, memory_length 2000, epsilon 0.41996, total time 724, loss -, compute time 12.88\n",
      "episode 965, reward 801, memory_length 2000, epsilon 0.41958, total time 721, loss [197.70635986328125], compute time 13.1\n",
      "episode 966, reward 460, memory_length 2000, epsilon 0.4192, total time 725, loss -, compute time 11.39\n",
      "episode 967, reward 989, memory_length 2000, epsilon 0.41883, total time 727, loss [101.6661605834961], compute time 12.84\n",
      "episode 968, reward 929, memory_length 2000, epsilon 0.41845, total time 721, loss -, compute time 11.41\n",
      "episode 969, reward 360, memory_length 2000, epsilon 0.41807, total time 730, loss [221.7473907470703], compute time 11.87\n",
      "episode 970, reward 696, memory_length 2000, epsilon 0.4177, total time 723, loss [124.27368927001953], compute time 11.57\n",
      "Saving model for episode: 970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 971, reward 698, memory_length 2000, epsilon 0.41732, total time 725, loss -, compute time 10.84\n",
      "episode 972, reward 629, memory_length 2000, epsilon 0.41695, total time 724, loss -, compute time 13.35\n",
      "episode 973, reward 720, memory_length 2000, epsilon 0.41657, total time 723, loss -, compute time 11.95\n",
      "episode 974, reward 849, memory_length 2000, epsilon 0.4162, total time 722, loss -, compute time 12.19\n",
      "episode 975, reward 461, memory_length 2000, epsilon 0.41582, total time 725, loss -, compute time 11.98\n",
      "episode 976, reward 699, memory_length 2000, epsilon 0.41545, total time 728, loss -, compute time 11.61\n",
      "episode 977, reward 428, memory_length 2000, epsilon 0.41507, total time 732, loss -, compute time 11.84\n",
      "episode 978, reward 957, memory_length 2000, epsilon 0.4147, total time 725, loss [163.82553100585938], compute time 10.82\n",
      "episode 979, reward 654, memory_length 2000, epsilon 0.41433, total time 721, loss [312.08685302734375], compute time 12.02\n",
      "episode 980, reward 1109, memory_length 2000, epsilon 0.41395, total time 731, loss [337.7283935546875], compute time 12.36\n",
      "Saving model for episode: 980\n",
      "episode 981, reward 631, memory_length 2000, epsilon 0.41358, total time 727, loss [188.71864318847656], compute time 12.78\n",
      "episode 982, reward 955, memory_length 2000, epsilon 0.41321, total time 723, loss -, compute time 14.11\n",
      "episode 983, reward 505, memory_length 2000, epsilon 0.41284, total time 726, loss -, compute time 14.68\n",
      "episode 984, reward 826, memory_length 2000, epsilon 0.41247, total time 721, loss -, compute time 11.06\n",
      "episode 985, reward 892, memory_length 2000, epsilon 0.4121, total time 728, loss -, compute time 12.01\n",
      "episode 986, reward 893, memory_length 2000, epsilon 0.41172, total time 727, loss [291.0554504394531], compute time 12.23\n",
      "episode 987, reward 1071, memory_length 2000, epsilon 0.41135, total time 721, loss [277.7725830078125], compute time 13.42\n",
      "episode 988, reward 1139, memory_length 2000, epsilon 0.41098, total time 729, loss -, compute time 14.93\n",
      "episode 989, reward 1195, memory_length 2000, epsilon 0.41061, total time 722, loss [105.7718276977539], compute time 12.96\n",
      "episode 990, reward 865, memory_length 2000, epsilon 0.41025, total time 724, loss -, compute time 13.31\n",
      "Saving model for episode: 990\n",
      "episode 991, reward 869, memory_length 2000, epsilon 0.40988, total time 721, loss [302.0287780761719], compute time 13.69\n",
      "episode 992, reward 823, memory_length 2000, epsilon 0.40951, total time 725, loss -, compute time 12.38\n",
      "episode 993, reward 541, memory_length 2000, epsilon 0.40914, total time 726, loss -, compute time 13.64\n",
      "episode 994, reward 1018, memory_length 2000, epsilon 0.40877, total time 728, loss -, compute time 12.77\n",
      "episode 995, reward 822, memory_length 2000, epsilon 0.4084, total time 723, loss -, compute time 13.11\n",
      "episode 996, reward 1004, memory_length 2000, epsilon 0.40804, total time 721, loss -, compute time 13.53\n",
      "episode 997, reward 1283, memory_length 2000, epsilon 0.40767, total time 722, loss [187.24951171875], compute time 11.71\n",
      "episode 998, reward 833, memory_length 2000, epsilon 0.4073, total time 726, loss -, compute time 12.64\n",
      "episode 999, reward 1063, memory_length 2000, epsilon 0.40694, total time 728, loss -, compute time 12.84\n",
      "episode 1000, reward 965, memory_length 2000, epsilon 0.40657, total time 730, loss [168.82235717773438], compute time 13.47\n",
      "Saving model for episode: 1000\n",
      "episode 1001, reward 784, memory_length 2000, epsilon 0.4062, total time 722, loss [238.84449768066406], compute time 12.34\n",
      "episode 1002, reward 1249, memory_length 2000, epsilon 0.40584, total time 723, loss -, compute time 13.22\n",
      "episode 1003, reward 1016, memory_length 2000, epsilon 0.40547, total time 723, loss -, compute time 11.5\n",
      "episode 1004, reward 896, memory_length 2000, epsilon 0.40511, total time 722, loss -, compute time 12.06\n",
      "episode 1005, reward 966, memory_length 2000, epsilon 0.40474, total time 725, loss -, compute time 12.78\n",
      "episode 1006, reward 705, memory_length 2000, epsilon 0.40438, total time 723, loss -, compute time 15.4\n",
      "episode 1007, reward 737, memory_length 2000, epsilon 0.40402, total time 722, loss -, compute time 10.97\n",
      "episode 1008, reward 775, memory_length 2000, epsilon 0.40365, total time 721, loss -, compute time 12.19\n",
      "episode 1009, reward 648, memory_length 2000, epsilon 0.40329, total time 721, loss [1.2526556253433228], compute time 14.45\n",
      "episode 1010, reward 960, memory_length 2000, epsilon 0.40293, total time 721, loss [4.019039630889893], compute time 11.48\n",
      "Saving model for episode: 1010\n",
      "episode 1011, reward 578, memory_length 2000, epsilon 0.40256, total time 722, loss -, compute time 13.95\n",
      "episode 1012, reward 870, memory_length 2000, epsilon 0.4022, total time 727, loss -, compute time 13.14\n",
      "episode 1013, reward 1006, memory_length 2000, epsilon 0.40184, total time 723, loss [5.736603260040283], compute time 13.53\n",
      "episode 1014, reward 770, memory_length 2000, epsilon 0.40148, total time 727, loss -, compute time 13.09\n",
      "episode 1015, reward 1149, memory_length 2000, epsilon 0.40112, total time 723, loss -, compute time 11.3\n",
      "episode 1016, reward 755, memory_length 2000, epsilon 0.40076, total time 721, loss -, compute time 13.88\n",
      "episode 1017, reward 600, memory_length 2000, epsilon 0.4004, total time 723, loss -, compute time 11.56\n",
      "episode 1018, reward 1148, memory_length 2000, epsilon 0.40004, total time 723, loss [211.70538330078125], compute time 11.58\n",
      "episode 1019, reward 713, memory_length 2000, epsilon 0.39968, total time 729, loss -, compute time 13.58\n",
      "episode 1020, reward 788, memory_length 2000, epsilon 0.39932, total time 730, loss [592.4299926757812], compute time 12.44\n",
      "Saving model for episode: 1020\n",
      "episode 1021, reward 891, memory_length 2000, epsilon 0.39896, total time 721, loss [126.41097259521484], compute time 12.98\n",
      "episode 1022, reward 833, memory_length 2000, epsilon 0.3986, total time 723, loss -, compute time 12.11\n",
      "episode 1023, reward 1069, memory_length 2000, epsilon 0.39824, total time 723, loss -, compute time 11.49\n",
      "episode 1024, reward 693, memory_length 2000, epsilon 0.39788, total time 724, loss [2.4906301498413086], compute time 11.93\n",
      "episode 1025, reward 1143, memory_length 2000, epsilon 0.39752, total time 730, loss -, compute time 11.33\n",
      "episode 1026, reward 844, memory_length 2000, epsilon 0.39717, total time 735, loss -, compute time 13.12\n",
      "episode 1027, reward 1167, memory_length 2000, epsilon 0.39681, total time 727, loss [229.89332580566406], compute time 12.71\n",
      "episode 1028, reward 981, memory_length 2000, epsilon 0.39645, total time 721, loss [125.29265594482422], compute time 11.96\n",
      "episode 1029, reward 1001, memory_length 2000, epsilon 0.3961, total time 721, loss -, compute time 12.39\n",
      "episode 1030, reward 965, memory_length 2000, epsilon 0.39574, total time 722, loss [114.06704711914062], compute time 12.73\n",
      "Saving model for episode: 1030\n",
      "episode 1031, reward 533, memory_length 2000, epsilon 0.39538, total time 725, loss -, compute time 12.75\n",
      "episode 1032, reward 1062, memory_length 2000, epsilon 0.39503, total time 726, loss -, compute time 14.13\n",
      "episode 1033, reward 667, memory_length 2000, epsilon 0.39467, total time 726, loss [102.65068054199219], compute time 13.11\n",
      "episode 1034, reward 925, memory_length 2000, epsilon 0.39432, total time 725, loss -, compute time 13.46\n",
      "episode 1035, reward 1035, memory_length 2000, epsilon 0.39396, total time 721, loss [475.05169677734375], compute time 12.48\n",
      "episode 1036, reward 900, memory_length 2000, epsilon 0.39361, total time 724, loss [2.127429723739624], compute time 12.44\n",
      "episode 1037, reward 1173, memory_length 2000, epsilon 0.39325, total time 726, loss -, compute time 13.48\n",
      "episode 1038, reward 847, memory_length 2000, epsilon 0.3929, total time 729, loss [436.3363037109375], compute time 14.01\n",
      "episode 1039, reward 914, memory_length 2000, epsilon 0.39255, total time 723, loss [1.6414283514022827], compute time 12.02\n",
      "episode 1040, reward 898, memory_length 2000, epsilon 0.39219, total time 722, loss -, compute time 11.85\n",
      "Saving model for episode: 1040\n",
      "episode 1041, reward 928, memory_length 2000, epsilon 0.39184, total time 729, loss -, compute time 12.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1042, reward 1018, memory_length 2000, epsilon 0.39149, total time 726, loss -, compute time 11.3\n",
      "episode 1043, reward 829, memory_length 2000, epsilon 0.39114, total time 733, loss -, compute time 12.42\n",
      "episode 1044, reward 1040, memory_length 2000, epsilon 0.39078, total time 725, loss -, compute time 13.09\n",
      "episode 1045, reward 1099, memory_length 2000, epsilon 0.39043, total time 730, loss -, compute time 12.27\n",
      "episode 1046, reward 1107, memory_length 2000, epsilon 0.39008, total time 725, loss -, compute time 12.81\n",
      "episode 1047, reward 851, memory_length 2000, epsilon 0.38973, total time 730, loss [1.9458832740783691], compute time 11.64\n",
      "episode 1048, reward 941, memory_length 2000, epsilon 0.38938, total time 724, loss [283.44305419921875], compute time 13.89\n",
      "episode 1049, reward 880, memory_length 2000, epsilon 0.38903, total time 721, loss [462.2127685546875], compute time 11.97\n",
      "episode 1050, reward 811, memory_length 2000, epsilon 0.38868, total time 729, loss -, compute time 12.5\n",
      "Saving model for episode: 1050\n",
      "episode 1051, reward 734, memory_length 2000, epsilon 0.38833, total time 722, loss [176.90628051757812], compute time 12.1\n",
      "episode 1052, reward 1067, memory_length 2000, epsilon 0.38798, total time 723, loss -, compute time 11.23\n",
      "episode 1053, reward 847, memory_length 2000, epsilon 0.38763, total time 728, loss [191.74124145507812], compute time 11.38\n",
      "episode 1054, reward 1141, memory_length 2000, epsilon 0.38728, total time 729, loss [305.8916931152344], compute time 12.04\n",
      "episode 1055, reward 954, memory_length 2000, epsilon 0.38693, total time 722, loss -, compute time 12.28\n",
      "episode 1056, reward 873, memory_length 2000, epsilon 0.38659, total time 724, loss [397.0201110839844], compute time 12.96\n",
      "episode 1057, reward 1144, memory_length 2000, epsilon 0.38624, total time 725, loss [343.7313232421875], compute time 11.87\n",
      "episode 1058, reward 819, memory_length 2000, epsilon 0.38589, total time 738, loss [113.43292999267578], compute time 12.05\n",
      "episode 1059, reward 696, memory_length 2000, epsilon 0.38554, total time 723, loss [222.98666381835938], compute time 11.39\n",
      "episode 1060, reward 839, memory_length 2000, epsilon 0.3852, total time 722, loss [197.13182067871094], compute time 12.24\n",
      "Saving model for episode: 1060\n",
      "episode 1061, reward 1068, memory_length 2000, epsilon 0.38485, total time 723, loss [482.8409118652344], compute time 11.3\n",
      "episode 1062, reward 1269, memory_length 2000, epsilon 0.3845, total time 728, loss [434.55322265625], compute time 12.77\n",
      "episode 1063, reward 802, memory_length 2000, epsilon 0.38416, total time 724, loss -, compute time 12.62\n",
      "episode 1064, reward 1116, memory_length 2000, epsilon 0.38381, total time 724, loss [2.8567233085632324], compute time 12.42\n",
      "episode 1065, reward 844, memory_length 2000, epsilon 0.38347, total time 722, loss -, compute time 13.49\n",
      "episode 1066, reward 896, memory_length 2000, epsilon 0.38312, total time 722, loss [273.5533142089844], compute time 12.39\n",
      "episode 1067, reward 849, memory_length 2000, epsilon 0.38278, total time 722, loss [227.18209838867188], compute time 12.16\n",
      "episode 1068, reward 959, memory_length 2000, epsilon 0.38243, total time 721, loss [125.91643524169922], compute time 11.71\n",
      "episode 1069, reward 1098, memory_length 2000, epsilon 0.38209, total time 726, loss -, compute time 12.87\n",
      "episode 1070, reward 963, memory_length 2000, epsilon 0.38175, total time 721, loss -, compute time 13.05\n",
      "Saving model for episode: 1070\n",
      "episode 1071, reward 689, memory_length 2000, epsilon 0.3814, total time 725, loss [1.6996537446975708], compute time 12.62\n",
      "episode 1072, reward 788, memory_length 2000, epsilon 0.38106, total time 724, loss -, compute time 12.86\n",
      "episode 1073, reward 936, memory_length 2000, epsilon 0.38072, total time 729, loss -, compute time 12.29\n",
      "episode 1074, reward 1076, memory_length 2000, epsilon 0.38037, total time 728, loss -, compute time 13.76\n",
      "episode 1075, reward 784, memory_length 2000, epsilon 0.38003, total time 722, loss -, compute time 13.23\n",
      "episode 1076, reward 823, memory_length 2000, epsilon 0.37969, total time 722, loss -, compute time 12.86\n",
      "episode 1077, reward 923, memory_length 2000, epsilon 0.37935, total time 726, loss [2.405118942260742], compute time 11.46\n",
      "episode 1078, reward 1193, memory_length 2000, epsilon 0.37901, total time 726, loss -, compute time 11.93\n",
      "episode 1079, reward 950, memory_length 2000, epsilon 0.37867, total time 731, loss [2.4044992923736572], compute time 12.9\n",
      "episode 1080, reward 667, memory_length 2000, epsilon 0.37833, total time 726, loss -, compute time 11.64\n",
      "Saving model for episode: 1080\n",
      "episode 1081, reward 1275, memory_length 2000, epsilon 0.37799, total time 731, loss -, compute time 11.47\n",
      "episode 1082, reward 644, memory_length 2000, epsilon 0.37765, total time 721, loss -, compute time 12.74\n",
      "episode 1083, reward 909, memory_length 2000, epsilon 0.37731, total time 723, loss [2.2178359031677246], compute time 12.4\n",
      "episode 1084, reward 977, memory_length 2000, epsilon 0.37697, total time 731, loss [184.97470092773438], compute time 12.22\n",
      "episode 1085, reward 1260, memory_length 2000, epsilon 0.37663, total time 722, loss -, compute time 12.04\n",
      "episode 1086, reward 1116, memory_length 2000, epsilon 0.37629, total time 721, loss -, compute time 11.43\n",
      "episode 1087, reward 881, memory_length 2000, epsilon 0.37595, total time 721, loss -, compute time 12.0\n",
      "episode 1088, reward 1001, memory_length 2000, epsilon 0.37561, total time 727, loss -, compute time 11.48\n",
      "episode 1089, reward 1050, memory_length 2000, epsilon 0.37527, total time 724, loss -, compute time 12.48\n",
      "episode 1090, reward 1023, memory_length 2000, epsilon 0.37494, total time 721, loss -, compute time 11.01\n",
      "Saving model for episode: 1090\n",
      "episode 1091, reward 774, memory_length 2000, epsilon 0.3746, total time 726, loss -, compute time 12.15\n",
      "episode 1092, reward 701, memory_length 2000, epsilon 0.37426, total time 721, loss [120.3046646118164], compute time 11.9\n",
      "episode 1093, reward 1007, memory_length 2000, epsilon 0.37393, total time 722, loss -, compute time 13.43\n",
      "episode 1094, reward 752, memory_length 2000, epsilon 0.37359, total time 728, loss -, compute time 12.06\n",
      "episode 1095, reward 1107, memory_length 2000, epsilon 0.37325, total time 725, loss [1.9389950037002563], compute time 12.77\n",
      "episode 1096, reward 924, memory_length 2000, epsilon 0.37292, total time 726, loss -, compute time 12.38\n",
      "episode 1097, reward 960, memory_length 2000, epsilon 0.37258, total time 722, loss -, compute time 12.78\n",
      "episode 1098, reward 902, memory_length 2000, epsilon 0.37225, total time 722, loss -, compute time 11.56\n",
      "episode 1099, reward 930, memory_length 2000, epsilon 0.37191, total time 722, loss -, compute time 12.01\n",
      "episode 1100, reward 532, memory_length 2000, epsilon 0.37158, total time 721, loss [399.27789306640625], compute time 12.17\n",
      "Saving model for episode: 1100\n",
      "episode 1101, reward 852, memory_length 2000, epsilon 0.37124, total time 728, loss -, compute time 12.02\n",
      "episode 1102, reward 1040, memory_length 2000, epsilon 0.37091, total time 723, loss -, compute time 13.53\n",
      "episode 1103, reward 1127, memory_length 2000, epsilon 0.37057, total time 724, loss [217.79249572753906], compute time 11.91\n",
      "episode 1104, reward 1139, memory_length 2000, epsilon 0.37024, total time 725, loss -, compute time 12.22\n",
      "episode 1105, reward 846, memory_length 2000, epsilon 0.36991, total time 721, loss -, compute time 13.06\n",
      "episode 1106, reward 753, memory_length 2000, epsilon 0.36958, total time 722, loss -, compute time 12.53\n",
      "episode 1107, reward 834, memory_length 2000, epsilon 0.36924, total time 723, loss [2.733673334121704], compute time 12.14\n",
      "episode 1108, reward 889, memory_length 2000, epsilon 0.36891, total time 726, loss [152.8165740966797], compute time 14.25\n",
      "episode 1109, reward 807, memory_length 2000, epsilon 0.36858, total time 722, loss -, compute time 13.17\n",
      "episode 1110, reward 924, memory_length 2000, epsilon 0.36825, total time 727, loss -, compute time 12.23\n",
      "Saving model for episode: 1110\n",
      "episode 1111, reward 870, memory_length 2000, epsilon 0.36792, total time 721, loss [237.041015625], compute time 14.0\n",
      "episode 1112, reward 1077, memory_length 2000, epsilon 0.36759, total time 725, loss -, compute time 13.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1113, reward 792, memory_length 2000, epsilon 0.36725, total time 730, loss -, compute time 12.69\n",
      "episode 1114, reward 1055, memory_length 2000, epsilon 0.36692, total time 723, loss [2.4220523834228516], compute time 12.05\n",
      "episode 1115, reward 1136, memory_length 2000, epsilon 0.36659, total time 727, loss -, compute time 12.34\n",
      "episode 1116, reward 981, memory_length 2000, epsilon 0.36626, total time 721, loss [185.4228515625], compute time 12.3\n",
      "episode 1117, reward 868, memory_length 2000, epsilon 0.36593, total time 725, loss -, compute time 11.93\n",
      "episode 1118, reward 856, memory_length 2000, epsilon 0.36561, total time 733, loss [189.32542419433594], compute time 11.26\n",
      "episode 1119, reward 1181, memory_length 2000, epsilon 0.36528, total time 721, loss -, compute time 13.29\n",
      "episode 1120, reward 1215, memory_length 2000, epsilon 0.36495, total time 725, loss -, compute time 12.55\n",
      "Saving model for episode: 1120\n",
      "episode 1121, reward 956, memory_length 2000, epsilon 0.36462, total time 723, loss -, compute time 12.48\n",
      "episode 1122, reward 736, memory_length 2000, epsilon 0.36429, total time 725, loss [129.8682861328125], compute time 13.27\n",
      "episode 1123, reward 1059, memory_length 2000, epsilon 0.36396, total time 723, loss [121.87051391601562], compute time 12.33\n",
      "episode 1124, reward 972, memory_length 2000, epsilon 0.36364, total time 726, loss [348.62408447265625], compute time 12.07\n",
      "episode 1125, reward 738, memory_length 2000, epsilon 0.36331, total time 721, loss -, compute time 13.65\n",
      "episode 1126, reward 972, memory_length 2000, epsilon 0.36298, total time 721, loss -, compute time 13.09\n",
      "episode 1127, reward 932, memory_length 2000, epsilon 0.36266, total time 726, loss -, compute time 12.52\n",
      "episode 1128, reward 621, memory_length 2000, epsilon 0.36233, total time 721, loss -, compute time 11.8\n",
      "episode 1129, reward 974, memory_length 2000, epsilon 0.362, total time 722, loss [2.0938215255737305], compute time 12.46\n",
      "episode 1130, reward 942, memory_length 2000, epsilon 0.36168, total time 722, loss [1.595282793045044], compute time 11.65\n",
      "Saving model for episode: 1130\n",
      "episode 1131, reward 942, memory_length 2000, epsilon 0.36135, total time 725, loss -, compute time 14.54\n",
      "episode 1132, reward 831, memory_length 2000, epsilon 0.36103, total time 723, loss -, compute time 11.52\n",
      "episode 1133, reward 1031, memory_length 2000, epsilon 0.3607, total time 730, loss -, compute time 12.82\n",
      "episode 1134, reward 806, memory_length 2000, epsilon 0.36038, total time 734, loss -, compute time 12.42\n",
      "episode 1135, reward 907, memory_length 2000, epsilon 0.36005, total time 721, loss -, compute time 13.19\n",
      "episode 1136, reward 681, memory_length 2000, epsilon 0.35973, total time 727, loss -, compute time 11.38\n",
      "episode 1137, reward 1182, memory_length 2000, epsilon 0.35941, total time 727, loss [323.85186767578125], compute time 13.03\n",
      "episode 1138, reward 748, memory_length 2000, epsilon 0.35908, total time 726, loss -, compute time 12.62\n",
      "episode 1139, reward 969, memory_length 2000, epsilon 0.35876, total time 726, loss [123.4989242553711], compute time 11.87\n",
      "episode 1140, reward 1082, memory_length 2000, epsilon 0.35844, total time 721, loss -, compute time 13.03\n",
      "Saving model for episode: 1140\n",
      "episode 1141, reward 1242, memory_length 2000, epsilon 0.35812, total time 725, loss -, compute time 13.32\n",
      "episode 1142, reward 1050, memory_length 2000, epsilon 0.35779, total time 726, loss -, compute time 11.54\n",
      "episode 1143, reward 1069, memory_length 2000, epsilon 0.35747, total time 723, loss -, compute time 12.36\n",
      "episode 1144, reward 825, memory_length 2000, epsilon 0.35715, total time 721, loss -, compute time 13.12\n",
      "episode 1145, reward 714, memory_length 2000, epsilon 0.35683, total time 723, loss -, compute time 12.93\n",
      "episode 1146, reward 504, memory_length 2000, epsilon 0.35651, total time 723, loss [117.7580795288086], compute time 14.36\n",
      "episode 1147, reward 983, memory_length 2000, epsilon 0.35619, total time 721, loss -, compute time 13.05\n",
      "episode 1148, reward 1107, memory_length 2000, epsilon 0.35587, total time 721, loss [217.89222717285156], compute time 12.98\n",
      "episode 1149, reward 936, memory_length 2000, epsilon 0.35555, total time 722, loss -, compute time 11.77\n",
      "episode 1150, reward 1252, memory_length 2000, epsilon 0.35523, total time 731, loss [2.0369436740875244], compute time 11.84\n",
      "Saving model for episode: 1150\n",
      "episode 1151, reward 820, memory_length 2000, epsilon 0.35491, total time 723, loss -, compute time 13.52\n",
      "episode 1152, reward 1477, memory_length 2000, epsilon 0.35459, total time 725, loss [104.513671875], compute time 11.59\n",
      "episode 1153, reward 1144, memory_length 2000, epsilon 0.35427, total time 724, loss -, compute time 12.06\n",
      "episode 1154, reward 790, memory_length 2000, epsilon 0.35395, total time 724, loss [3.7366623878479004], compute time 12.7\n",
      "episode 1155, reward 815, memory_length 2000, epsilon 0.35363, total time 721, loss [188.46389770507812], compute time 12.1\n",
      "episode 1156, reward 963, memory_length 2000, epsilon 0.35331, total time 723, loss [2.0252721309661865], compute time 11.29\n",
      "episode 1157, reward 1098, memory_length 2000, epsilon 0.353, total time 721, loss [111.4869613647461], compute time 14.1\n",
      "episode 1158, reward 833, memory_length 2000, epsilon 0.35268, total time 721, loss -, compute time 10.96\n",
      "episode 1159, reward 975, memory_length 2000, epsilon 0.35236, total time 726, loss -, compute time 13.11\n",
      "episode 1160, reward 1032, memory_length 2000, epsilon 0.35204, total time 727, loss -, compute time 12.45\n",
      "Saving model for episode: 1160\n",
      "episode 1161, reward 963, memory_length 2000, epsilon 0.35173, total time 727, loss -, compute time 10.94\n",
      "episode 1162, reward 955, memory_length 2000, epsilon 0.35141, total time 726, loss [114.12612915039062], compute time 12.18\n",
      "episode 1163, reward 921, memory_length 2000, epsilon 0.35109, total time 727, loss [358.3079833984375], compute time 12.06\n",
      "episode 1164, reward 1224, memory_length 2000, epsilon 0.35078, total time 721, loss [332.00445556640625], compute time 13.47\n",
      "episode 1165, reward 999, memory_length 2000, epsilon 0.35046, total time 728, loss [86.86279296875], compute time 12.4\n",
      "episode 1166, reward 783, memory_length 2000, epsilon 0.35015, total time 727, loss -, compute time 12.89\n",
      "episode 1167, reward 839, memory_length 2000, epsilon 0.34983, total time 722, loss -, compute time 11.74\n",
      "episode 1168, reward 1168, memory_length 2000, epsilon 0.34952, total time 728, loss [120.29778289794922], compute time 12.53\n",
      "episode 1169, reward 1257, memory_length 2000, epsilon 0.3492, total time 721, loss -, compute time 12.76\n",
      "episode 1170, reward 1062, memory_length 2000, epsilon 0.34889, total time 733, loss [3.7554821968078613], compute time 12.58\n",
      "Saving model for episode: 1170\n",
      "episode 1171, reward 1116, memory_length 2000, epsilon 0.34858, total time 721, loss [106.32872772216797], compute time 12.09\n",
      "episode 1172, reward 695, memory_length 2000, epsilon 0.34826, total time 721, loss -, compute time 12.32\n",
      "episode 1173, reward 682, memory_length 2000, epsilon 0.34795, total time 721, loss -, compute time 11.88\n",
      "episode 1174, reward 1203, memory_length 2000, epsilon 0.34764, total time 728, loss -, compute time 11.87\n",
      "episode 1175, reward 860, memory_length 2000, epsilon 0.34732, total time 729, loss [318.07464599609375], compute time 12.32\n",
      "episode 1176, reward 748, memory_length 2000, epsilon 0.34701, total time 726, loss [74.20072174072266], compute time 11.72\n",
      "episode 1177, reward 1074, memory_length 2000, epsilon 0.3467, total time 721, loss -, compute time 11.79\n",
      "episode 1178, reward 1209, memory_length 2000, epsilon 0.34639, total time 721, loss -, compute time 12.97\n",
      "episode 1179, reward 1071, memory_length 2000, epsilon 0.34607, total time 721, loss [132.64773559570312], compute time 12.02\n",
      "episode 1180, reward 1226, memory_length 2000, epsilon 0.34576, total time 727, loss -, compute time 11.58\n",
      "Saving model for episode: 1180\n",
      "episode 1181, reward 708, memory_length 2000, epsilon 0.34545, total time 721, loss -, compute time 12.69\n",
      "episode 1182, reward 716, memory_length 2000, epsilon 0.34514, total time 724, loss -, compute time 13.36\n",
      "episode 1183, reward 1153, memory_length 2000, epsilon 0.34483, total time 725, loss -, compute time 13.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1184, reward 897, memory_length 2000, epsilon 0.34452, total time 723, loss [3.024930477142334], compute time 11.65\n",
      "episode 1185, reward 1314, memory_length 2000, epsilon 0.34421, total time 724, loss -, compute time 14.14\n",
      "episode 1186, reward 950, memory_length 2000, epsilon 0.3439, total time 727, loss -, compute time 13.27\n",
      "episode 1187, reward 1188, memory_length 2000, epsilon 0.34359, total time 721, loss -, compute time 11.18\n",
      "episode 1188, reward 951, memory_length 2000, epsilon 0.34328, total time 724, loss -, compute time 13.49\n",
      "episode 1189, reward 987, memory_length 2000, epsilon 0.34297, total time 725, loss -, compute time 12.96\n",
      "episode 1190, reward 991, memory_length 2000, epsilon 0.34267, total time 725, loss -, compute time 13.54\n",
      "Saving model for episode: 1190\n",
      "episode 1191, reward 843, memory_length 2000, epsilon 0.34236, total time 724, loss -, compute time 12.24\n",
      "episode 1192, reward 837, memory_length 2000, epsilon 0.34205, total time 735, loss -, compute time 12.01\n",
      "episode 1193, reward 916, memory_length 2000, epsilon 0.34174, total time 724, loss -, compute time 12.64\n",
      "episode 1194, reward 902, memory_length 2000, epsilon 0.34143, total time 726, loss -, compute time 12.79\n",
      "episode 1195, reward 942, memory_length 2000, epsilon 0.34113, total time 726, loss -, compute time 12.11\n",
      "episode 1196, reward 916, memory_length 2000, epsilon 0.34082, total time 721, loss -, compute time 13.44\n",
      "episode 1197, reward 1126, memory_length 2000, epsilon 0.34051, total time 726, loss [235.32107543945312], compute time 14.0\n",
      "episode 1198, reward 1040, memory_length 2000, epsilon 0.34021, total time 723, loss [2.6571249961853027], compute time 12.45\n",
      "episode 1199, reward 1041, memory_length 2000, epsilon 0.3399, total time 725, loss -, compute time 12.46\n",
      "episode 1200, reward 1206, memory_length 2000, epsilon 0.3396, total time 725, loss -, compute time 13.35\n",
      "Saving model for episode: 1200\n",
      "episode 1201, reward 1081, memory_length 2000, epsilon 0.33929, total time 722, loss -, compute time 13.69\n",
      "episode 1202, reward 576, memory_length 2000, epsilon 0.33898, total time 727, loss -, compute time 13.09\n",
      "episode 1203, reward 854, memory_length 2000, epsilon 0.33868, total time 728, loss [1.540203332901001], compute time 13.12\n",
      "episode 1204, reward 1266, memory_length 2000, epsilon 0.33838, total time 725, loss -, compute time 13.28\n",
      "episode 1205, reward 1064, memory_length 2000, epsilon 0.33807, total time 722, loss [2.268324375152588], compute time 12.91\n",
      "episode 1206, reward 981, memory_length 2000, epsilon 0.33777, total time 725, loss [3.1523618698120117], compute time 13.98\n",
      "episode 1207, reward 942, memory_length 2000, epsilon 0.33746, total time 722, loss -, compute time 12.58\n",
      "episode 1208, reward 972, memory_length 2000, epsilon 0.33716, total time 726, loss -, compute time 14.25\n",
      "episode 1209, reward 946, memory_length 2000, epsilon 0.33686, total time 733, loss [1.691546082496643], compute time 12.79\n",
      "episode 1210, reward 793, memory_length 2000, epsilon 0.33655, total time 724, loss -, compute time 12.39\n",
      "Saving model for episode: 1210\n",
      "episode 1211, reward 820, memory_length 2000, epsilon 0.33625, total time 721, loss -, compute time 12.9\n",
      "episode 1212, reward 919, memory_length 2000, epsilon 0.33595, total time 728, loss -, compute time 13.08\n",
      "episode 1213, reward 824, memory_length 2000, epsilon 0.33565, total time 725, loss -, compute time 13.18\n",
      "episode 1214, reward 783, memory_length 2000, epsilon 0.33534, total time 724, loss -, compute time 13.95\n",
      "episode 1215, reward 1082, memory_length 2000, epsilon 0.33504, total time 721, loss [1.1982327699661255], compute time 12.91\n",
      "episode 1216, reward 1009, memory_length 2000, epsilon 0.33474, total time 730, loss -, compute time 13.76\n",
      "episode 1217, reward 874, memory_length 2000, epsilon 0.33444, total time 723, loss [197.68202209472656], compute time 11.07\n",
      "episode 1218, reward 720, memory_length 2000, epsilon 0.33414, total time 721, loss [320.2244873046875], compute time 13.16\n",
      "episode 1219, reward 1103, memory_length 2000, epsilon 0.33384, total time 728, loss -, compute time 12.2\n",
      "episode 1220, reward 838, memory_length 2000, epsilon 0.33354, total time 726, loss -, compute time 13.39\n",
      "Saving model for episode: 1220\n",
      "episode 1221, reward 1014, memory_length 2000, epsilon 0.33324, total time 727, loss -, compute time 11.94\n",
      "episode 1222, reward 1341, memory_length 2000, epsilon 0.33294, total time 726, loss -, compute time 13.12\n",
      "episode 1223, reward 945, memory_length 2000, epsilon 0.33264, total time 721, loss -, compute time 12.75\n",
      "episode 1224, reward 838, memory_length 2000, epsilon 0.33234, total time 728, loss -, compute time 13.62\n",
      "episode 1225, reward 893, memory_length 2000, epsilon 0.33204, total time 722, loss -, compute time 11.71\n",
      "episode 1226, reward 1010, memory_length 2000, epsilon 0.33174, total time 725, loss -, compute time 12.28\n",
      "episode 1227, reward 1005, memory_length 2000, epsilon 0.33144, total time 723, loss -, compute time 12.1\n",
      "episode 1228, reward 1216, memory_length 2000, epsilon 0.33114, total time 729, loss -, compute time 12.93\n",
      "episode 1229, reward 763, memory_length 2000, epsilon 0.33085, total time 729, loss -, compute time 12.72\n",
      "episode 1230, reward 968, memory_length 2000, epsilon 0.33055, total time 722, loss -, compute time 13.25\n",
      "Saving model for episode: 1230\n",
      "episode 1231, reward 963, memory_length 2000, epsilon 0.33025, total time 730, loss [3.657254934310913], compute time 13.72\n",
      "episode 1232, reward 995, memory_length 2000, epsilon 0.32995, total time 734, loss [1.983130693435669], compute time 12.93\n",
      "episode 1233, reward 910, memory_length 2000, epsilon 0.32966, total time 724, loss -, compute time 12.03\n",
      "episode 1234, reward 961, memory_length 2000, epsilon 0.32936, total time 721, loss -, compute time 12.45\n",
      "episode 1235, reward 1296, memory_length 2000, epsilon 0.32906, total time 721, loss [3.3737144470214844], compute time 13.0\n",
      "episode 1236, reward 1112, memory_length 2000, epsilon 0.32877, total time 729, loss -, compute time 11.89\n",
      "episode 1237, reward 1272, memory_length 2000, epsilon 0.32847, total time 732, loss -, compute time 11.2\n",
      "episode 1238, reward 734, memory_length 2000, epsilon 0.32818, total time 722, loss -, compute time 13.55\n",
      "episode 1239, reward 1200, memory_length 2000, epsilon 0.32788, total time 724, loss [2.0432705879211426], compute time 11.18\n",
      "episode 1240, reward 1242, memory_length 2000, epsilon 0.32759, total time 731, loss [108.68095397949219], compute time 11.97\n",
      "Saving model for episode: 1240\n",
      "episode 1241, reward 997, memory_length 2000, epsilon 0.32729, total time 721, loss -, compute time 13.22\n",
      "episode 1242, reward 1004, memory_length 2000, epsilon 0.327, total time 725, loss -, compute time 12.9\n",
      "episode 1243, reward 869, memory_length 2000, epsilon 0.3267, total time 725, loss -, compute time 12.46\n",
      "episode 1244, reward 1258, memory_length 2000, epsilon 0.32641, total time 722, loss -, compute time 13.03\n",
      "episode 1245, reward 1103, memory_length 2000, epsilon 0.32612, total time 728, loss -, compute time 11.58\n",
      "episode 1246, reward 905, memory_length 2000, epsilon 0.32582, total time 730, loss [2.3425116539001465], compute time 12.48\n",
      "episode 1247, reward 1140, memory_length 2000, epsilon 0.32553, total time 725, loss [1.679535150527954], compute time 14.05\n",
      "episode 1248, reward 1150, memory_length 2000, epsilon 0.32524, total time 728, loss -, compute time 13.0\n",
      "episode 1249, reward 937, memory_length 2000, epsilon 0.32494, total time 725, loss -, compute time 12.61\n",
      "episode 1250, reward 1023, memory_length 2000, epsilon 0.32465, total time 727, loss -, compute time 13.37\n",
      "Saving model for episode: 1250\n",
      "episode 1251, reward 1144, memory_length 2000, epsilon 0.32436, total time 722, loss -, compute time 12.01\n",
      "episode 1252, reward 1135, memory_length 2000, epsilon 0.32407, total time 730, loss -, compute time 10.86\n",
      "episode 1253, reward 1071, memory_length 2000, epsilon 0.32378, total time 726, loss -, compute time 11.2\n",
      "episode 1254, reward 1095, memory_length 2000, epsilon 0.32349, total time 722, loss -, compute time 12.59\n",
      "episode 1255, reward 986, memory_length 2000, epsilon 0.32319, total time 723, loss [116.93356323242188], compute time 12.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1256, reward 863, memory_length 2000, epsilon 0.3229, total time 726, loss -, compute time 12.22\n",
      "episode 1257, reward 1249, memory_length 2000, epsilon 0.32261, total time 724, loss [129.43922424316406], compute time 12.94\n",
      "episode 1258, reward 898, memory_length 2000, epsilon 0.32232, total time 721, loss -, compute time 12.36\n",
      "episode 1259, reward 1008, memory_length 2000, epsilon 0.32203, total time 722, loss [250.89297485351562], compute time 11.28\n",
      "episode 1260, reward 941, memory_length 2000, epsilon 0.32174, total time 722, loss [2.296233654022217], compute time 11.42\n",
      "Saving model for episode: 1260\n",
      "episode 1261, reward 801, memory_length 2000, epsilon 0.32145, total time 731, loss -, compute time 11.63\n",
      "episode 1262, reward 810, memory_length 2000, epsilon 0.32117, total time 727, loss -, compute time 12.17\n",
      "episode 1263, reward 471, memory_length 2000, epsilon 0.32088, total time 722, loss -, compute time 12.92\n",
      "episode 1264, reward 734, memory_length 2000, epsilon 0.32059, total time 723, loss -, compute time 13.96\n",
      "episode 1265, reward 869, memory_length 2000, epsilon 0.3203, total time 725, loss -, compute time 12.69\n",
      "episode 1266, reward 914, memory_length 2000, epsilon 0.32001, total time 723, loss -, compute time 12.87\n",
      "episode 1267, reward 894, memory_length 2000, epsilon 0.31972, total time 721, loss [116.6915512084961], compute time 12.57\n",
      "episode 1268, reward 572, memory_length 2000, epsilon 0.31944, total time 722, loss -, compute time 12.84\n",
      "episode 1269, reward 822, memory_length 2000, epsilon 0.31915, total time 721, loss -, compute time 12.47\n",
      "episode 1270, reward 543, memory_length 2000, epsilon 0.31886, total time 721, loss [1.061324119567871], compute time 14.5\n",
      "Saving model for episode: 1270\n",
      "episode 1271, reward 945, memory_length 2000, epsilon 0.31857, total time 724, loss [233.48153686523438], compute time 13.45\n",
      "episode 1272, reward 1194, memory_length 2000, epsilon 0.31829, total time 723, loss -, compute time 12.46\n",
      "episode 1273, reward 927, memory_length 2000, epsilon 0.318, total time 724, loss -, compute time 12.99\n",
      "episode 1274, reward 1100, memory_length 2000, epsilon 0.31772, total time 721, loss -, compute time 12.94\n",
      "episode 1275, reward 1280, memory_length 2000, epsilon 0.31743, total time 721, loss -, compute time 14.01\n",
      "episode 1276, reward 1080, memory_length 2000, epsilon 0.31714, total time 721, loss -, compute time 10.18\n",
      "episode 1277, reward 1078, memory_length 2000, epsilon 0.31686, total time 722, loss [85.84957122802734], compute time 12.72\n",
      "episode 1278, reward 973, memory_length 2000, epsilon 0.31657, total time 722, loss [114.3755874633789], compute time 12.87\n",
      "episode 1279, reward 1352, memory_length 2000, epsilon 0.31629, total time 723, loss -, compute time 13.26\n",
      "episode 1280, reward 828, memory_length 2000, epsilon 0.316, total time 731, loss [107.39814758300781], compute time 11.04\n",
      "Saving model for episode: 1280\n",
      "episode 1281, reward 992, memory_length 2000, epsilon 0.31572, total time 722, loss [91.9400634765625], compute time 13.04\n",
      "episode 1282, reward 1033, memory_length 2000, epsilon 0.31544, total time 725, loss [2.2559773921966553], compute time 12.49\n",
      "episode 1283, reward 1215, memory_length 2000, epsilon 0.31515, total time 731, loss -, compute time 13.83\n",
      "episode 1284, reward 1222, memory_length 2000, epsilon 0.31487, total time 728, loss -, compute time 13.57\n",
      "episode 1285, reward 1248, memory_length 2000, epsilon 0.31459, total time 725, loss [97.02195739746094], compute time 13.92\n",
      "episode 1286, reward 1202, memory_length 2000, epsilon 0.3143, total time 737, loss [323.365966796875], compute time 11.97\n",
      "episode 1287, reward 1421, memory_length 2000, epsilon 0.31402, total time 721, loss -, compute time 12.91\n",
      "episode 1288, reward 1088, memory_length 2000, epsilon 0.31374, total time 725, loss -, compute time 12.79\n",
      "episode 1289, reward 948, memory_length 2000, epsilon 0.31345, total time 722, loss -, compute time 12.96\n",
      "episode 1290, reward 1089, memory_length 2000, epsilon 0.31317, total time 726, loss -, compute time 12.7\n",
      "Saving model for episode: 1290\n",
      "episode 1291, reward 1072, memory_length 2000, epsilon 0.31289, total time 725, loss [1.986741065979004], compute time 12.58\n",
      "episode 1292, reward 1167, memory_length 2000, epsilon 0.31261, total time 723, loss [185.9565887451172], compute time 13.81\n",
      "episode 1293, reward 1074, memory_length 2000, epsilon 0.31233, total time 721, loss -, compute time 13.1\n",
      "episode 1294, reward 1278, memory_length 2000, epsilon 0.31205, total time 721, loss [120.15149688720703], compute time 12.31\n",
      "episode 1295, reward 1192, memory_length 2000, epsilon 0.31177, total time 727, loss [3.0949790477752686], compute time 12.0\n",
      "episode 1296, reward 1009, memory_length 2000, epsilon 0.31149, total time 724, loss -, compute time 12.02\n",
      "episode 1297, reward 1000, memory_length 2000, epsilon 0.31121, total time 722, loss -, compute time 13.29\n",
      "episode 1298, reward 969, memory_length 2000, epsilon 0.31093, total time 722, loss [1.9728751182556152], compute time 10.73\n",
      "episode 1299, reward 1241, memory_length 2000, epsilon 0.31065, total time 722, loss -, compute time 12.77\n",
      "episode 1300, reward 1129, memory_length 2000, epsilon 0.31037, total time 724, loss [1.5259066820144653], compute time 11.5\n",
      "Saving model for episode: 1300\n",
      "episode 1301, reward 1175, memory_length 2000, epsilon 0.31009, total time 725, loss [96.68212890625], compute time 11.9\n",
      "episode 1302, reward 992, memory_length 2000, epsilon 0.30981, total time 722, loss -, compute time 13.99\n",
      "episode 1303, reward 1134, memory_length 2000, epsilon 0.30953, total time 722, loss -, compute time 12.25\n",
      "episode 1304, reward 771, memory_length 2000, epsilon 0.30925, total time 723, loss [1.6843609809875488], compute time 13.49\n",
      "episode 1305, reward 1221, memory_length 2000, epsilon 0.30897, total time 721, loss -, compute time 12.62\n",
      "episode 1306, reward 1339, memory_length 2000, epsilon 0.3087, total time 721, loss [102.93231964111328], compute time 11.86\n",
      "episode 1307, reward 1263, memory_length 2000, epsilon 0.30842, total time 726, loss [243.61570739746094], compute time 14.3\n",
      "episode 1308, reward 995, memory_length 2000, epsilon 0.30814, total time 727, loss [173.04286193847656], compute time 13.38\n",
      "episode 1309, reward 1298, memory_length 2000, epsilon 0.30786, total time 722, loss -, compute time 13.17\n",
      "episode 1310, reward 1115, memory_length 2000, epsilon 0.30759, total time 721, loss [2.067700147628784], compute time 12.34\n",
      "Saving model for episode: 1310\n",
      "episode 1311, reward 1226, memory_length 2000, epsilon 0.30731, total time 722, loss -, compute time 13.44\n",
      "episode 1312, reward 1274, memory_length 2000, epsilon 0.30703, total time 725, loss -, compute time 13.58\n",
      "episode 1313, reward 1103, memory_length 2000, epsilon 0.30676, total time 725, loss [3.6489570140838623], compute time 13.23\n",
      "episode 1314, reward 946, memory_length 2000, epsilon 0.30648, total time 729, loss -, compute time 13.31\n",
      "episode 1315, reward 1139, memory_length 2000, epsilon 0.30621, total time 725, loss -, compute time 14.96\n",
      "episode 1316, reward 1060, memory_length 2000, epsilon 0.30593, total time 723, loss -, compute time 12.26\n",
      "episode 1317, reward 1318, memory_length 2000, epsilon 0.30565, total time 721, loss -, compute time 14.4\n",
      "episode 1318, reward 838, memory_length 2000, epsilon 0.30538, total time 727, loss [2.119724750518799], compute time 13.09\n",
      "episode 1319, reward 1027, memory_length 2000, epsilon 0.3051, total time 728, loss -, compute time 13.12\n",
      "episode 1320, reward 972, memory_length 2000, epsilon 0.30483, total time 721, loss -, compute time 13.23\n",
      "Saving model for episode: 1320\n",
      "episode 1321, reward 1160, memory_length 2000, epsilon 0.30456, total time 723, loss -, compute time 12.99\n",
      "episode 1322, reward 1157, memory_length 2000, epsilon 0.30428, total time 735, loss -, compute time 13.78\n",
      "episode 1323, reward 1084, memory_length 2000, epsilon 0.30401, total time 721, loss -, compute time 13.37\n",
      "episode 1324, reward 1065, memory_length 2000, epsilon 0.30373, total time 729, loss [121.60271453857422], compute time 14.68\n",
      "episode 1325, reward 1069, memory_length 2000, epsilon 0.30346, total time 723, loss -, compute time 12.47\n",
      "episode 1326, reward 1150, memory_length 2000, epsilon 0.30319, total time 722, loss [214.95220947265625], compute time 11.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1327, reward 1244, memory_length 2000, epsilon 0.30292, total time 721, loss -, compute time 12.39\n",
      "episode 1328, reward 955, memory_length 2000, epsilon 0.30264, total time 727, loss -, compute time 12.39\n",
      "episode 1329, reward 810, memory_length 2000, epsilon 0.30237, total time 728, loss -, compute time 12.13\n",
      "episode 1330, reward 1203, memory_length 2000, epsilon 0.3021, total time 728, loss -, compute time 11.32\n",
      "Saving model for episode: 1330\n",
      "episode 1331, reward 843, memory_length 2000, epsilon 0.30183, total time 723, loss [2.1553854942321777], compute time 12.3\n",
      "episode 1332, reward 1304, memory_length 2000, epsilon 0.30156, total time 726, loss [116.72694396972656], compute time 12.53\n",
      "episode 1333, reward 1044, memory_length 2000, epsilon 0.30128, total time 728, loss -, compute time 13.73\n",
      "episode 1334, reward 945, memory_length 2000, epsilon 0.30101, total time 722, loss -, compute time 13.32\n",
      "episode 1335, reward 1035, memory_length 2000, epsilon 0.30074, total time 728, loss [1.8111884593963623], compute time 13.49\n",
      "episode 1336, reward 1062, memory_length 2000, epsilon 0.30047, total time 722, loss -, compute time 13.0\n",
      "episode 1337, reward 995, memory_length 2000, epsilon 0.3002, total time 731, loss [2.576350212097168], compute time 11.66\n",
      "episode 1338, reward 1071, memory_length 2000, epsilon 0.29993, total time 721, loss [366.41412353515625], compute time 13.19\n",
      "episode 1339, reward 1306, memory_length 2000, epsilon 0.29966, total time 729, loss [106.94317626953125], compute time 13.06\n",
      "episode 1340, reward 1298, memory_length 2000, epsilon 0.29939, total time 722, loss -, compute time 13.19\n",
      "Saving model for episode: 1340\n",
      "episode 1341, reward 1139, memory_length 2000, epsilon 0.29912, total time 722, loss -, compute time 11.42\n",
      "episode 1342, reward 1247, memory_length 2000, epsilon 0.29885, total time 725, loss [105.95343017578125], compute time 13.29\n",
      "episode 1343, reward 767, memory_length 2000, epsilon 0.29859, total time 723, loss -, compute time 13.25\n",
      "episode 1344, reward 677, memory_length 2000, epsilon 0.29832, total time 723, loss -, compute time 12.39\n",
      "episode 1345, reward 1196, memory_length 2000, epsilon 0.29805, total time 721, loss -, compute time 11.44\n",
      "episode 1346, reward 1392, memory_length 2000, epsilon 0.29778, total time 726, loss -, compute time 13.22\n",
      "episode 1347, reward 1165, memory_length 2000, epsilon 0.29751, total time 722, loss [177.68772888183594], compute time 12.01\n",
      "episode 1348, reward 1085, memory_length 2000, epsilon 0.29724, total time 723, loss [210.90771484375], compute time 13.11\n",
      "episode 1349, reward 1144, memory_length 2000, epsilon 0.29698, total time 725, loss -, compute time 13.3\n",
      "episode 1350, reward 1019, memory_length 2000, epsilon 0.29671, total time 728, loss [1.394169569015503], compute time 13.12\n",
      "Saving model for episode: 1350\n",
      "episode 1351, reward 1139, memory_length 2000, epsilon 0.29644, total time 728, loss [123.56756591796875], compute time 11.62\n",
      "episode 1352, reward 1264, memory_length 2000, epsilon 0.29618, total time 721, loss [3.4987852573394775], compute time 11.93\n",
      "episode 1353, reward 1105, memory_length 2000, epsilon 0.29591, total time 724, loss -, compute time 12.02\n",
      "episode 1354, reward 861, memory_length 2000, epsilon 0.29564, total time 724, loss [3.1541829109191895], compute time 13.0\n",
      "episode 1355, reward 918, memory_length 2000, epsilon 0.29538, total time 729, loss -, compute time 11.83\n",
      "episode 1356, reward 1053, memory_length 2000, epsilon 0.29511, total time 725, loss [3.66805100440979], compute time 11.5\n",
      "episode 1357, reward 1585, memory_length 2000, epsilon 0.29485, total time 722, loss -, compute time 13.72\n",
      "episode 1358, reward 770, memory_length 2000, epsilon 0.29458, total time 724, loss -, compute time 12.31\n",
      "episode 1359, reward 977, memory_length 2000, epsilon 0.29432, total time 724, loss [1.6749861240386963], compute time 12.37\n",
      "episode 1360, reward 1005, memory_length 2000, epsilon 0.29405, total time 722, loss -, compute time 12.84\n",
      "Saving model for episode: 1360\n",
      "episode 1361, reward 1200, memory_length 2000, epsilon 0.29379, total time 721, loss -, compute time 13.07\n",
      "episode 1362, reward 1097, memory_length 2000, epsilon 0.29352, total time 721, loss -, compute time 12.64\n",
      "episode 1363, reward 1206, memory_length 2000, epsilon 0.29326, total time 726, loss -, compute time 12.52\n",
      "episode 1364, reward 1082, memory_length 2000, epsilon 0.29299, total time 721, loss -, compute time 12.02\n",
      "episode 1365, reward 1410, memory_length 2000, epsilon 0.29273, total time 725, loss -, compute time 14.91\n",
      "episode 1366, reward 914, memory_length 2000, epsilon 0.29247, total time 721, loss -, compute time 13.07\n",
      "episode 1367, reward 1179, memory_length 2000, epsilon 0.2922, total time 725, loss -, compute time 12.13\n",
      "episode 1368, reward 1194, memory_length 2000, epsilon 0.29194, total time 730, loss [82.82108306884766], compute time 11.95\n",
      "episode 1369, reward 1146, memory_length 2000, epsilon 0.29168, total time 725, loss [3.374833345413208], compute time 13.3\n",
      "episode 1370, reward 1038, memory_length 2000, epsilon 0.29142, total time 722, loss -, compute time 12.36\n",
      "Saving model for episode: 1370\n",
      "episode 1371, reward 1316, memory_length 2000, epsilon 0.29115, total time 732, loss [129.0370635986328], compute time 11.94\n",
      "episode 1372, reward 948, memory_length 2000, epsilon 0.29089, total time 723, loss [3.201066255569458], compute time 13.45\n",
      "episode 1373, reward 1194, memory_length 2000, epsilon 0.29063, total time 723, loss -, compute time 13.87\n",
      "episode 1374, reward 1374, memory_length 2000, epsilon 0.29037, total time 723, loss [356.1291198730469], compute time 12.4\n",
      "episode 1375, reward 1099, memory_length 2000, epsilon 0.29011, total time 728, loss [95.62104797363281], compute time 12.75\n",
      "episode 1376, reward 1361, memory_length 2000, epsilon 0.28985, total time 727, loss -, compute time 13.18\n",
      "episode 1377, reward 1213, memory_length 2000, epsilon 0.28959, total time 722, loss -, compute time 12.69\n",
      "episode 1378, reward 1404, memory_length 2000, epsilon 0.28933, total time 721, loss [3.056004524230957], compute time 13.21\n",
      "episode 1379, reward 774, memory_length 2000, epsilon 0.28907, total time 721, loss -, compute time 13.02\n",
      "episode 1380, reward 1224, memory_length 2000, epsilon 0.28881, total time 726, loss -, compute time 12.73\n",
      "Saving model for episode: 1380\n",
      "episode 1381, reward 1158, memory_length 2000, epsilon 0.28855, total time 728, loss -, compute time 12.6\n",
      "episode 1382, reward 1188, memory_length 2000, epsilon 0.28829, total time 722, loss -, compute time 11.64\n",
      "episode 1383, reward 963, memory_length 2000, epsilon 0.28803, total time 726, loss -, compute time 13.16\n",
      "episode 1384, reward 1056, memory_length 2000, epsilon 0.28777, total time 721, loss -, compute time 11.91\n",
      "episode 1385, reward 630, memory_length 2000, epsilon 0.28751, total time 723, loss -, compute time 11.41\n",
      "episode 1386, reward 1143, memory_length 2000, epsilon 0.28725, total time 721, loss -, compute time 12.02\n",
      "episode 1387, reward 864, memory_length 2000, epsilon 0.28699, total time 728, loss -, compute time 13.55\n",
      "episode 1388, reward 606, memory_length 2000, epsilon 0.28673, total time 721, loss -, compute time 11.46\n",
      "episode 1389, reward 985, memory_length 2000, epsilon 0.28648, total time 725, loss -, compute time 12.49\n",
      "episode 1390, reward 1107, memory_length 2000, epsilon 0.28622, total time 722, loss [3.476398468017578], compute time 12.43\n",
      "Saving model for episode: 1390\n",
      "episode 1391, reward 1073, memory_length 2000, epsilon 0.28596, total time 726, loss -, compute time 11.33\n",
      "episode 1392, reward 1242, memory_length 2000, epsilon 0.2857, total time 722, loss [207.52426147460938], compute time 11.64\n",
      "episode 1393, reward 1134, memory_length 2000, epsilon 0.28545, total time 721, loss [214.96607971191406], compute time 12.43\n",
      "episode 1394, reward 1243, memory_length 2000, epsilon 0.28519, total time 724, loss -, compute time 12.45\n",
      "episode 1395, reward 1053, memory_length 2000, epsilon 0.28493, total time 731, loss -, compute time 11.01\n",
      "episode 1396, reward 1103, memory_length 2000, epsilon 0.28468, total time 725, loss -, compute time 13.57\n",
      "episode 1397, reward 996, memory_length 2000, epsilon 0.28442, total time 724, loss -, compute time 11.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1398, reward 921, memory_length 2000, epsilon 0.28417, total time 724, loss -, compute time 12.61\n",
      "episode 1399, reward 1042, memory_length 2000, epsilon 0.28391, total time 723, loss -, compute time 12.05\n",
      "episode 1400, reward 1235, memory_length 2000, epsilon 0.28365, total time 723, loss -, compute time 11.68\n",
      "Saving model for episode: 1400\n",
      "episode 1401, reward 1474, memory_length 2000, epsilon 0.2834, total time 723, loss -, compute time 11.1\n",
      "episode 1402, reward 956, memory_length 2000, epsilon 0.28314, total time 725, loss [90.55516815185547], compute time 11.19\n",
      "episode 1403, reward 1090, memory_length 2000, epsilon 0.28289, total time 732, loss [2.6826114654541016], compute time 12.11\n",
      "episode 1404, reward 1436, memory_length 2000, epsilon 0.28263, total time 723, loss -, compute time 12.05\n",
      "episode 1405, reward 1319, memory_length 2000, epsilon 0.28238, total time 727, loss -, compute time 12.11\n",
      "episode 1406, reward 942, memory_length 2000, epsilon 0.28213, total time 730, loss -, compute time 12.65\n",
      "episode 1407, reward 961, memory_length 2000, epsilon 0.28187, total time 721, loss [1.839705467224121], compute time 12.61\n",
      "episode 1408, reward 1334, memory_length 2000, epsilon 0.28162, total time 722, loss -, compute time 12.95\n",
      "episode 1409, reward 1077, memory_length 2000, epsilon 0.28137, total time 721, loss -, compute time 13.02\n",
      "episode 1410, reward 1109, memory_length 2000, epsilon 0.28111, total time 722, loss -, compute time 12.0\n",
      "Saving model for episode: 1410\n",
      "episode 1411, reward 1148, memory_length 2000, epsilon 0.28086, total time 722, loss -, compute time 10.35\n",
      "episode 1412, reward 1090, memory_length 2000, epsilon 0.28061, total time 729, loss -, compute time 13.59\n",
      "episode 1413, reward 1036, memory_length 2000, epsilon 0.28035, total time 725, loss [107.66217041015625], compute time 11.51\n",
      "episode 1414, reward 836, memory_length 2000, epsilon 0.2801, total time 721, loss -, compute time 11.39\n",
      "episode 1415, reward 1331, memory_length 2000, epsilon 0.27985, total time 721, loss -, compute time 12.17\n",
      "episode 1416, reward 1059, memory_length 2000, epsilon 0.2796, total time 728, loss [104.58417510986328], compute time 12.84\n",
      "episode 1417, reward 1179, memory_length 2000, epsilon 0.27935, total time 729, loss [108.68627166748047], compute time 14.26\n",
      "episode 1418, reward 1031, memory_length 2000, epsilon 0.2791, total time 727, loss -, compute time 12.75\n",
      "episode 1419, reward 907, memory_length 2000, epsilon 0.27884, total time 723, loss -, compute time 13.72\n",
      "episode 1420, reward 1321, memory_length 2000, epsilon 0.27859, total time 722, loss -, compute time 13.07\n",
      "Saving model for episode: 1420\n",
      "episode 1421, reward 1207, memory_length 2000, epsilon 0.27834, total time 723, loss [179.32260131835938], compute time 14.18\n",
      "episode 1422, reward 1310, memory_length 2000, epsilon 0.27809, total time 722, loss [2.5871644020080566], compute time 12.39\n",
      "episode 1423, reward 1139, memory_length 2000, epsilon 0.27784, total time 721, loss -, compute time 13.87\n",
      "episode 1424, reward 1486, memory_length 2000, epsilon 0.27759, total time 728, loss -, compute time 13.16\n",
      "episode 1425, reward 1243, memory_length 2000, epsilon 0.27734, total time 728, loss [2.2343223094940186], compute time 11.35\n",
      "episode 1426, reward 1202, memory_length 2000, epsilon 0.27709, total time 728, loss [398.37432861328125], compute time 13.35\n",
      "episode 1427, reward 876, memory_length 2000, epsilon 0.27684, total time 722, loss -, compute time 10.92\n",
      "episode 1428, reward 1280, memory_length 2000, epsilon 0.2766, total time 731, loss -, compute time 12.08\n",
      "episode 1429, reward 827, memory_length 2000, epsilon 0.27635, total time 722, loss -, compute time 10.27\n",
      "episode 1430, reward 1286, memory_length 2000, epsilon 0.2761, total time 721, loss [2.0609219074249268], compute time 12.25\n",
      "Saving model for episode: 1430\n",
      "episode 1431, reward 1584, memory_length 2000, epsilon 0.27585, total time 723, loss -, compute time 11.99\n",
      "episode 1432, reward 828, memory_length 2000, epsilon 0.2756, total time 725, loss -, compute time 12.24\n",
      "episode 1433, reward 1114, memory_length 2000, epsilon 0.27535, total time 721, loss [2.671476364135742], compute time 13.32\n",
      "episode 1434, reward 1233, memory_length 2000, epsilon 0.27511, total time 721, loss [141.95867919921875], compute time 12.0\n",
      "episode 1435, reward 1208, memory_length 2000, epsilon 0.27486, total time 723, loss -, compute time 12.16\n",
      "episode 1436, reward 1158, memory_length 2000, epsilon 0.27461, total time 725, loss -, compute time 12.29\n",
      "episode 1437, reward 1182, memory_length 2000, epsilon 0.27436, total time 722, loss -, compute time 13.05\n",
      "episode 1438, reward 1024, memory_length 2000, epsilon 0.27412, total time 721, loss -, compute time 11.18\n",
      "episode 1439, reward 1253, memory_length 2000, epsilon 0.27387, total time 723, loss -, compute time 11.41\n",
      "episode 1440, reward 1035, memory_length 2000, epsilon 0.27362, total time 727, loss [278.5201721191406], compute time 12.52\n",
      "Saving model for episode: 1440\n",
      "episode 1441, reward 1364, memory_length 2000, epsilon 0.27338, total time 725, loss [3.030597686767578], compute time 13.16\n",
      "episode 1442, reward 959, memory_length 2000, epsilon 0.27313, total time 728, loss [1.4222347736358643], compute time 11.79\n",
      "episode 1443, reward 1176, memory_length 2000, epsilon 0.27289, total time 727, loss [1.3194825649261475], compute time 11.94\n",
      "episode 1444, reward 1141, memory_length 2000, epsilon 0.27264, total time 725, loss [2.231736183166504], compute time 11.74\n",
      "episode 1445, reward 1150, memory_length 2000, epsilon 0.2724, total time 724, loss -, compute time 15.06\n",
      "episode 1446, reward 1216, memory_length 2000, epsilon 0.27215, total time 721, loss -, compute time 12.13\n",
      "episode 1447, reward 1191, memory_length 2000, epsilon 0.27191, total time 721, loss -, compute time 12.88\n",
      "episode 1448, reward 1090, memory_length 2000, epsilon 0.27166, total time 725, loss -, compute time 12.75\n",
      "episode 1449, reward 1367, memory_length 2000, epsilon 0.27142, total time 722, loss [2.045473337173462], compute time 11.1\n",
      "episode 1450, reward 1281, memory_length 2000, epsilon 0.27117, total time 725, loss -, compute time 12.06\n",
      "Saving model for episode: 1450\n",
      "episode 1451, reward 968, memory_length 2000, epsilon 0.27093, total time 729, loss [234.45298767089844], compute time 11.46\n",
      "episode 1452, reward 1530, memory_length 2000, epsilon 0.27068, total time 732, loss -, compute time 12.13\n",
      "episode 1453, reward 1042, memory_length 2000, epsilon 0.27044, total time 725, loss [2.2627615928649902], compute time 13.99\n",
      "episode 1454, reward 1467, memory_length 2000, epsilon 0.2702, total time 731, loss -, compute time 11.33\n",
      "episode 1455, reward 1278, memory_length 2000, epsilon 0.26996, total time 727, loss -, compute time 11.18\n",
      "episode 1456, reward 1056, memory_length 2000, epsilon 0.26971, total time 721, loss -, compute time 12.62\n",
      "episode 1457, reward 1121, memory_length 2000, epsilon 0.26947, total time 729, loss -, compute time 12.11\n",
      "episode 1458, reward 1258, memory_length 2000, epsilon 0.26923, total time 725, loss -, compute time 12.02\n",
      "episode 1459, reward 1241, memory_length 2000, epsilon 0.26898, total time 722, loss [1.8266338109970093], compute time 12.12\n",
      "episode 1460, reward 1127, memory_length 2000, epsilon 0.26874, total time 722, loss -, compute time 10.66\n",
      "Saving model for episode: 1460\n",
      "episode 1461, reward 1191, memory_length 2000, epsilon 0.2685, total time 721, loss -, compute time 12.31\n",
      "episode 1462, reward 1088, memory_length 2000, epsilon 0.26826, total time 721, loss -, compute time 12.03\n",
      "episode 1463, reward 914, memory_length 2000, epsilon 0.26802, total time 724, loss -, compute time 12.87\n",
      "episode 1464, reward 1143, memory_length 2000, epsilon 0.26778, total time 721, loss -, compute time 12.01\n",
      "episode 1465, reward 896, memory_length 2000, epsilon 0.26754, total time 721, loss -, compute time 11.62\n",
      "episode 1466, reward 1026, memory_length 2000, epsilon 0.2673, total time 722, loss -, compute time 11.13\n",
      "episode 1467, reward 1029, memory_length 2000, epsilon 0.26706, total time 726, loss [2.146770715713501], compute time 13.29\n",
      "episode 1468, reward 1388, memory_length 2000, epsilon 0.26681, total time 725, loss [2.040128231048584], compute time 12.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1469, reward 1134, memory_length 2000, epsilon 0.26657, total time 734, loss [1.1576584577560425], compute time 11.72\n",
      "episode 1470, reward 1540, memory_length 2000, epsilon 0.26634, total time 721, loss -, compute time 13.02\n",
      "Saving model for episode: 1470\n",
      "episode 1471, reward 1346, memory_length 2000, epsilon 0.2661, total time 725, loss -, compute time 11.81\n",
      "episode 1472, reward 1071, memory_length 2000, epsilon 0.26586, total time 724, loss [2.371187686920166], compute time 11.8\n",
      "episode 1473, reward 1269, memory_length 2000, epsilon 0.26562, total time 725, loss -, compute time 11.95\n",
      "episode 1474, reward 1330, memory_length 2000, epsilon 0.26538, total time 724, loss -, compute time 11.06\n",
      "episode 1475, reward 713, memory_length 2000, epsilon 0.26514, total time 726, loss [132.13307189941406], compute time 11.73\n",
      "episode 1476, reward 1527, memory_length 2000, epsilon 0.2649, total time 729, loss -, compute time 12.68\n",
      "episode 1477, reward 999, memory_length 2000, epsilon 0.26466, total time 721, loss -, compute time 11.44\n",
      "episode 1478, reward 1072, memory_length 2000, epsilon 0.26442, total time 726, loss -, compute time 12.2\n",
      "episode 1479, reward 1323, memory_length 2000, epsilon 0.26419, total time 737, loss [1.5182627439498901], compute time 11.12\n",
      "episode 1480, reward 1225, memory_length 2000, epsilon 0.26395, total time 722, loss -, compute time 11.5\n",
      "Saving model for episode: 1480\n",
      "episode 1481, reward 1496, memory_length 2000, epsilon 0.26371, total time 721, loss -, compute time 11.87\n",
      "episode 1482, reward 1305, memory_length 2000, epsilon 0.26347, total time 727, loss -, compute time 12.99\n",
      "episode 1483, reward 1304, memory_length 2000, epsilon 0.26324, total time 724, loss [2.3427951335906982], compute time 10.98\n",
      "episode 1484, reward 909, memory_length 2000, epsilon 0.263, total time 728, loss [1.4375578165054321], compute time 13.66\n",
      "episode 1485, reward 1064, memory_length 2000, epsilon 0.26276, total time 723, loss -, compute time 12.23\n",
      "episode 1486, reward 1373, memory_length 2000, epsilon 0.26253, total time 725, loss -, compute time 11.66\n",
      "episode 1487, reward 1071, memory_length 2000, epsilon 0.26229, total time 726, loss -, compute time 11.01\n",
      "episode 1488, reward 895, memory_length 2000, epsilon 0.26206, total time 721, loss -, compute time 11.42\n",
      "episode 1489, reward 981, memory_length 2000, epsilon 0.26182, total time 721, loss -, compute time 10.98\n",
      "episode 1490, reward 1140, memory_length 2000, epsilon 0.26158, total time 724, loss -, compute time 11.81\n",
      "Saving model for episode: 1490\n",
      "episode 1491, reward 1265, memory_length 2000, epsilon 0.26135, total time 725, loss -, compute time 11.76\n",
      "episode 1492, reward 1236, memory_length 2000, epsilon 0.26111, total time 724, loss -, compute time 11.58\n",
      "episode 1493, reward 1188, memory_length 2000, epsilon 0.26088, total time 728, loss [2.8143467903137207], compute time 13.32\n",
      "episode 1494, reward 767, memory_length 2000, epsilon 0.26064, total time 724, loss -, compute time 12.54\n",
      "episode 1495, reward 1063, memory_length 2000, epsilon 0.26041, total time 729, loss -, compute time 10.43\n",
      "episode 1496, reward 1283, memory_length 2000, epsilon 0.26018, total time 724, loss -, compute time 12.35\n",
      "episode 1497, reward 1170, memory_length 2000, epsilon 0.25994, total time 734, loss [2.182929515838623], compute time 10.57\n",
      "episode 1498, reward 803, memory_length 2000, epsilon 0.25971, total time 722, loss -, compute time 11.92\n",
      "episode 1499, reward 928, memory_length 2000, epsilon 0.25947, total time 724, loss -, compute time 12.1\n",
      "episode 1500, reward 1305, memory_length 2000, epsilon 0.25924, total time 725, loss -, compute time 11.06\n",
      "Saving model for episode: 1500\n",
      "episode 1501, reward 1155, memory_length 2000, epsilon 0.25901, total time 721, loss -, compute time 11.57\n",
      "episode 1502, reward 1140, memory_length 2000, epsilon 0.25877, total time 729, loss [118.28815460205078], compute time 12.15\n",
      "episode 1503, reward 1395, memory_length 2000, epsilon 0.25854, total time 731, loss -, compute time 12.29\n",
      "episode 1504, reward 1581, memory_length 2000, epsilon 0.25831, total time 723, loss -, compute time 12.3\n",
      "episode 1505, reward 1258, memory_length 2000, epsilon 0.25808, total time 725, loss -, compute time 12.21\n",
      "episode 1506, reward 1436, memory_length 2000, epsilon 0.25784, total time 724, loss -, compute time 11.37\n",
      "episode 1507, reward 1406, memory_length 2000, epsilon 0.25761, total time 729, loss -, compute time 11.28\n",
      "episode 1508, reward 1186, memory_length 2000, epsilon 0.25738, total time 725, loss [1.7421808242797852], compute time 10.48\n",
      "episode 1509, reward 891, memory_length 2000, epsilon 0.25715, total time 722, loss -, compute time 10.01\n",
      "episode 1510, reward 961, memory_length 2000, epsilon 0.25692, total time 722, loss -, compute time 10.66\n",
      "Saving model for episode: 1510\n",
      "episode 1511, reward 1608, memory_length 2000, epsilon 0.25669, total time 722, loss -, compute time 11.87\n",
      "episode 1512, reward 1166, memory_length 2000, epsilon 0.25646, total time 726, loss -, compute time 12.93\n",
      "episode 1513, reward 1007, memory_length 2000, epsilon 0.25622, total time 722, loss -, compute time 12.31\n",
      "episode 1514, reward 830, memory_length 2000, epsilon 0.25599, total time 725, loss -, compute time 11.86\n",
      "episode 1515, reward 1413, memory_length 2000, epsilon 0.25576, total time 721, loss -, compute time 12.63\n",
      "episode 1516, reward 1026, memory_length 2000, epsilon 0.25553, total time 726, loss [88.57815551757812], compute time 10.5\n",
      "episode 1517, reward 1275, memory_length 2000, epsilon 0.2553, total time 723, loss -, compute time 12.38\n",
      "episode 1518, reward 1442, memory_length 2000, epsilon 0.25507, total time 724, loss -, compute time 11.43\n",
      "episode 1519, reward 1252, memory_length 2000, epsilon 0.25484, total time 721, loss [171.98988342285156], compute time 10.86\n",
      "episode 1520, reward 1189, memory_length 2000, epsilon 0.25462, total time 725, loss -, compute time 10.31\n",
      "Saving model for episode: 1520\n",
      "episode 1521, reward 1154, memory_length 2000, epsilon 0.25439, total time 727, loss [1.4571900367736816], compute time 11.72\n",
      "episode 1522, reward 1254, memory_length 2000, epsilon 0.25416, total time 721, loss [2.3682408332824707], compute time 12.77\n",
      "episode 1523, reward 947, memory_length 2000, epsilon 0.25393, total time 724, loss -, compute time 11.18\n",
      "episode 1524, reward 1193, memory_length 2000, epsilon 0.2537, total time 725, loss -, compute time 13.0\n",
      "episode 1525, reward 1348, memory_length 2000, epsilon 0.25347, total time 721, loss -, compute time 12.97\n",
      "episode 1526, reward 1323, memory_length 2000, epsilon 0.25324, total time 728, loss -, compute time 12.27\n",
      "episode 1527, reward 1428, memory_length 2000, epsilon 0.25302, total time 728, loss -, compute time 12.48\n",
      "episode 1528, reward 1331, memory_length 2000, epsilon 0.25279, total time 722, loss -, compute time 11.76\n",
      "episode 1529, reward 1279, memory_length 2000, epsilon 0.25256, total time 727, loss -, compute time 10.91\n",
      "episode 1530, reward 1257, memory_length 2000, epsilon 0.25233, total time 721, loss -, compute time 11.04\n",
      "Saving model for episode: 1530\n",
      "episode 1531, reward 1031, memory_length 2000, epsilon 0.25211, total time 724, loss -, compute time 10.5\n",
      "episode 1532, reward 1171, memory_length 2000, epsilon 0.25188, total time 725, loss -, compute time 13.11\n",
      "episode 1533, reward 1203, memory_length 2000, epsilon 0.25165, total time 730, loss -, compute time 10.41\n",
      "episode 1534, reward 1271, memory_length 2000, epsilon 0.25143, total time 728, loss [2.5510263442993164], compute time 11.66\n",
      "episode 1535, reward 973, memory_length 2000, epsilon 0.2512, total time 726, loss -, compute time 11.3\n",
      "episode 1536, reward 1301, memory_length 2000, epsilon 0.25098, total time 725, loss -, compute time 13.32\n",
      "episode 1537, reward 1267, memory_length 2000, epsilon 0.25075, total time 725, loss -, compute time 12.89\n",
      "episode 1538, reward 1348, memory_length 2000, epsilon 0.25052, total time 726, loss [119.23894500732422], compute time 12.16\n",
      "episode 1539, reward 1257, memory_length 2000, epsilon 0.2503, total time 721, loss -, compute time 12.72\n",
      "episode 1540, reward 1329, memory_length 2000, epsilon 0.25007, total time 724, loss -, compute time 12.12\n",
      "Saving model for episode: 1540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1541, reward 1400, memory_length 2000, epsilon 0.24985, total time 731, loss -, compute time 12.56\n",
      "episode 1542, reward 1045, memory_length 2000, epsilon 0.24962, total time 723, loss -, compute time 11.86\n",
      "episode 1543, reward 1089, memory_length 2000, epsilon 0.2494, total time 724, loss -, compute time 12.32\n",
      "episode 1544, reward 924, memory_length 2000, epsilon 0.24917, total time 721, loss [1.5552436113357544], compute time 11.78\n",
      "episode 1545, reward 1518, memory_length 2000, epsilon 0.24895, total time 731, loss -, compute time 12.91\n",
      "episode 1546, reward 1244, memory_length 2000, epsilon 0.24873, total time 726, loss [113.26255798339844], compute time 11.05\n",
      "episode 1547, reward 1134, memory_length 2000, epsilon 0.2485, total time 729, loss -, compute time 11.8\n",
      "episode 1548, reward 992, memory_length 2000, epsilon 0.24828, total time 727, loss -, compute time 12.44\n",
      "episode 1549, reward 1334, memory_length 2000, epsilon 0.24806, total time 729, loss -, compute time 11.53\n",
      "episode 1550, reward 752, memory_length 2000, epsilon 0.24783, total time 731, loss -, compute time 11.61\n",
      "Saving model for episode: 1550\n",
      "episode 1551, reward 1220, memory_length 2000, epsilon 0.24761, total time 721, loss -, compute time 11.42\n",
      "episode 1552, reward 1038, memory_length 2000, epsilon 0.24739, total time 722, loss -, compute time 13.84\n",
      "episode 1553, reward 1409, memory_length 2000, epsilon 0.24716, total time 727, loss -, compute time 13.05\n",
      "episode 1554, reward 1130, memory_length 2000, epsilon 0.24694, total time 723, loss -, compute time 10.77\n",
      "episode 1555, reward 1446, memory_length 2000, epsilon 0.24672, total time 725, loss -, compute time 11.24\n",
      "episode 1556, reward 1291, memory_length 2000, epsilon 0.2465, total time 721, loss [217.14559936523438], compute time 13.19\n",
      "episode 1557, reward 1108, memory_length 2000, epsilon 0.24628, total time 731, loss -, compute time 10.68\n",
      "episode 1558, reward 1314, memory_length 2000, epsilon 0.24606, total time 722, loss -, compute time 11.63\n",
      "episode 1559, reward 970, memory_length 2000, epsilon 0.24583, total time 723, loss [120.78492736816406], compute time 11.37\n",
      "episode 1560, reward 1171, memory_length 2000, epsilon 0.24561, total time 725, loss -, compute time 12.04\n",
      "Saving model for episode: 1560\n",
      "episode 1561, reward 1107, memory_length 2000, epsilon 0.24539, total time 721, loss -, compute time 11.69\n",
      "episode 1562, reward 968, memory_length 2000, epsilon 0.24517, total time 723, loss [115.34764099121094], compute time 11.56\n",
      "episode 1563, reward 1080, memory_length 2000, epsilon 0.24495, total time 721, loss -, compute time 12.15\n",
      "episode 1564, reward 1288, memory_length 2000, epsilon 0.24473, total time 724, loss -, compute time 12.66\n",
      "episode 1565, reward 1187, memory_length 2000, epsilon 0.24451, total time 723, loss [111.80939483642578], compute time 11.86\n",
      "episode 1566, reward 1133, memory_length 2000, epsilon 0.24429, total time 721, loss -, compute time 12.26\n",
      "episode 1567, reward 1161, memory_length 2000, epsilon 0.24407, total time 735, loss -, compute time 11.84\n",
      "episode 1568, reward 1274, memory_length 2000, epsilon 0.24385, total time 727, loss -, compute time 12.19\n",
      "episode 1569, reward 1116, memory_length 2000, epsilon 0.24363, total time 726, loss -, compute time 11.75\n",
      "episode 1570, reward 1236, memory_length 2000, epsilon 0.24341, total time 726, loss [109.19221496582031], compute time 11.3\n",
      "Saving model for episode: 1570\n",
      "episode 1571, reward 1091, memory_length 2000, epsilon 0.24319, total time 724, loss -, compute time 10.82\n",
      "episode 1572, reward 1306, memory_length 2000, epsilon 0.24297, total time 726, loss -, compute time 12.12\n",
      "episode 1573, reward 1683, memory_length 2000, epsilon 0.24276, total time 721, loss [113.90306091308594], compute time 12.79\n",
      "episode 1574, reward 1018, memory_length 2000, epsilon 0.24254, total time 725, loss -, compute time 14.22\n",
      "episode 1575, reward 1108, memory_length 2000, epsilon 0.24232, total time 727, loss -, compute time 11.3\n",
      "episode 1576, reward 1288, memory_length 2000, epsilon 0.2421, total time 730, loss -, compute time 12.96\n",
      "episode 1577, reward 1223, memory_length 2000, epsilon 0.24188, total time 724, loss -, compute time 11.41\n",
      "episode 1578, reward 1028, memory_length 2000, epsilon 0.24167, total time 722, loss [1.478522539138794], compute time 12.2\n",
      "episode 1579, reward 1305, memory_length 2000, epsilon 0.24145, total time 724, loss -, compute time 13.13\n",
      "episode 1580, reward 1074, memory_length 2000, epsilon 0.24123, total time 722, loss -, compute time 12.99\n",
      "Saving model for episode: 1580\n",
      "episode 1581, reward 1031, memory_length 2000, epsilon 0.24101, total time 722, loss -, compute time 12.99\n",
      "episode 1582, reward 889, memory_length 2000, epsilon 0.2408, total time 723, loss -, compute time 13.32\n",
      "episode 1583, reward 976, memory_length 2000, epsilon 0.24058, total time 721, loss [1.450282335281372], compute time 13.4\n",
      "episode 1584, reward 1083, memory_length 2000, epsilon 0.24036, total time 723, loss -, compute time 11.13\n",
      "episode 1585, reward 1088, memory_length 2000, epsilon 0.24015, total time 721, loss -, compute time 11.63\n",
      "episode 1586, reward 1226, memory_length 2000, epsilon 0.23993, total time 724, loss -, compute time 10.89\n",
      "episode 1587, reward 1279, memory_length 2000, epsilon 0.23972, total time 724, loss -, compute time 13.66\n",
      "episode 1588, reward 1252, memory_length 2000, epsilon 0.2395, total time 723, loss -, compute time 12.11\n",
      "episode 1589, reward 1095, memory_length 2000, epsilon 0.23928, total time 727, loss -, compute time 12.88\n",
      "episode 1590, reward 1046, memory_length 2000, epsilon 0.23907, total time 729, loss [111.43608856201172], compute time 13.56\n",
      "Saving model for episode: 1590\n",
      "episode 1591, reward 903, memory_length 2000, epsilon 0.23885, total time 721, loss -, compute time 12.84\n",
      "episode 1592, reward 1265, memory_length 2000, epsilon 0.23864, total time 724, loss -, compute time 11.59\n",
      "episode 1593, reward 1270, memory_length 2000, epsilon 0.23843, total time 733, loss [117.66341400146484], compute time 12.8\n",
      "episode 1594, reward 1379, memory_length 2000, epsilon 0.23821, total time 725, loss [110.94427490234375], compute time 11.69\n",
      "episode 1595, reward 1044, memory_length 2000, epsilon 0.238, total time 721, loss -, compute time 12.56\n",
      "episode 1596, reward 1181, memory_length 2000, epsilon 0.23778, total time 723, loss -, compute time 11.62\n",
      "episode 1597, reward 1196, memory_length 2000, epsilon 0.23757, total time 722, loss -, compute time 10.75\n",
      "episode 1598, reward 1100, memory_length 2000, epsilon 0.23735, total time 722, loss [109.11493682861328], compute time 12.15\n",
      "episode 1599, reward 1203, memory_length 2000, epsilon 0.23714, total time 722, loss -, compute time 12.45\n",
      "episode 1600, reward 1445, memory_length 2000, epsilon 0.23693, total time 726, loss [95.06153106689453], compute time 13.45\n",
      "Saving model for episode: 1600\n",
      "episode 1601, reward 1305, memory_length 2000, epsilon 0.23671, total time 721, loss [109.56216430664062], compute time 11.34\n",
      "episode 1602, reward 1306, memory_length 2000, epsilon 0.2365, total time 729, loss [2.315720558166504], compute time 11.59\n",
      "episode 1603, reward 980, memory_length 2000, epsilon 0.23629, total time 725, loss [79.8983383178711], compute time 11.61\n",
      "episode 1604, reward 1143, memory_length 2000, epsilon 0.23608, total time 726, loss -, compute time 12.16\n",
      "episode 1605, reward 1306, memory_length 2000, epsilon 0.23586, total time 725, loss -, compute time 12.54\n",
      "episode 1606, reward 1221, memory_length 2000, epsilon 0.23565, total time 729, loss [106.0351333618164], compute time 11.61\n",
      "episode 1607, reward 1204, memory_length 2000, epsilon 0.23544, total time 726, loss -, compute time 11.16\n",
      "episode 1608, reward 1284, memory_length 2000, epsilon 0.23523, total time 724, loss -, compute time 11.32\n",
      "episode 1609, reward 1359, memory_length 2000, epsilon 0.23502, total time 721, loss [2.216392993927002], compute time 12.55\n",
      "episode 1610, reward 1100, memory_length 2000, epsilon 0.2348, total time 728, loss -, compute time 12.15\n",
      "Saving model for episode: 1610\n",
      "episode 1611, reward 1425, memory_length 2000, epsilon 0.23459, total time 721, loss -, compute time 11.81\n",
      "episode 1612, reward 1023, memory_length 2000, epsilon 0.23438, total time 723, loss -, compute time 11.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1613, reward 1445, memory_length 2000, epsilon 0.23417, total time 724, loss -, compute time 13.14\n",
      "episode 1614, reward 1143, memory_length 2000, epsilon 0.23396, total time 728, loss -, compute time 11.97\n",
      "episode 1615, reward 1154, memory_length 2000, epsilon 0.23375, total time 723, loss -, compute time 11.44\n",
      "episode 1616, reward 998, memory_length 2000, epsilon 0.23354, total time 722, loss -, compute time 11.13\n",
      "episode 1617, reward 1370, memory_length 2000, epsilon 0.23333, total time 722, loss -, compute time 11.85\n",
      "episode 1618, reward 1665, memory_length 2000, epsilon 0.23312, total time 728, loss -, compute time 11.26\n",
      "episode 1619, reward 1188, memory_length 2000, epsilon 0.23291, total time 724, loss -, compute time 13.23\n",
      "episode 1620, reward 1076, memory_length 2000, epsilon 0.2327, total time 721, loss -, compute time 12.18\n",
      "Saving model for episode: 1620\n",
      "episode 1621, reward 1494, memory_length 2000, epsilon 0.23249, total time 729, loss -, compute time 11.85\n",
      "episode 1622, reward 1440, memory_length 2000, epsilon 0.23228, total time 727, loss [1.9483609199523926], compute time 12.36\n",
      "episode 1623, reward 1280, memory_length 2000, epsilon 0.23207, total time 721, loss [104.98467254638672], compute time 10.91\n",
      "episode 1624, reward 1027, memory_length 2000, epsilon 0.23186, total time 724, loss -, compute time 11.99\n",
      "episode 1625, reward 1449, memory_length 2000, epsilon 0.23166, total time 724, loss -, compute time 12.3\n",
      "episode 1626, reward 1065, memory_length 2000, epsilon 0.23145, total time 721, loss [2.2537858486175537], compute time 11.8\n",
      "episode 1627, reward 1644, memory_length 2000, epsilon 0.23124, total time 725, loss -, compute time 12.48\n",
      "episode 1628, reward 1359, memory_length 2000, epsilon 0.23103, total time 727, loss [0.9789438843727112], compute time 12.24\n",
      "episode 1629, reward 979, memory_length 2000, epsilon 0.23082, total time 726, loss -, compute time 12.28\n",
      "episode 1630, reward 1189, memory_length 2000, epsilon 0.23062, total time 723, loss -, compute time 11.09\n",
      "Saving model for episode: 1630\n",
      "episode 1631, reward 1496, memory_length 2000, epsilon 0.23041, total time 724, loss -, compute time 11.63\n",
      "episode 1632, reward 1329, memory_length 2000, epsilon 0.2302, total time 721, loss -, compute time 12.72\n",
      "episode 1633, reward 1108, memory_length 2000, epsilon 0.22999, total time 729, loss -, compute time 12.43\n",
      "episode 1634, reward 1368, memory_length 2000, epsilon 0.22979, total time 724, loss -, compute time 12.79\n",
      "episode 1635, reward 1532, memory_length 2000, epsilon 0.22958, total time 727, loss -, compute time 13.47\n",
      "episode 1636, reward 1290, memory_length 2000, epsilon 0.22937, total time 724, loss [202.1805419921875], compute time 11.32\n",
      "episode 1637, reward 1701, memory_length 2000, epsilon 0.22917, total time 721, loss -, compute time 13.05\n",
      "episode 1638, reward 1221, memory_length 2000, epsilon 0.22896, total time 721, loss -, compute time 11.03\n",
      "episode 1639, reward 1099, memory_length 2000, epsilon 0.22876, total time 724, loss -, compute time 11.94\n",
      "episode 1640, reward 1218, memory_length 2000, epsilon 0.22855, total time 721, loss [2.4297704696655273], compute time 12.45\n",
      "Saving model for episode: 1640\n",
      "episode 1641, reward 1206, memory_length 2000, epsilon 0.22834, total time 729, loss -, compute time 11.88\n",
      "episode 1642, reward 1404, memory_length 2000, epsilon 0.22814, total time 723, loss -, compute time 12.05\n",
      "episode 1643, reward 1188, memory_length 2000, epsilon 0.22793, total time 724, loss -, compute time 11.84\n",
      "episode 1644, reward 1278, memory_length 2000, epsilon 0.22773, total time 724, loss -, compute time 10.92\n",
      "episode 1645, reward 1410, memory_length 2000, epsilon 0.22752, total time 728, loss [2.044992446899414], compute time 12.56\n",
      "episode 1646, reward 1449, memory_length 2000, epsilon 0.22732, total time 728, loss -, compute time 12.27\n",
      "episode 1647, reward 1356, memory_length 2000, epsilon 0.22711, total time 727, loss -, compute time 12.66\n",
      "episode 1648, reward 1258, memory_length 2000, epsilon 0.22691, total time 721, loss -, compute time 12.1\n",
      "episode 1649, reward 1321, memory_length 2000, epsilon 0.22671, total time 724, loss -, compute time 12.0\n",
      "episode 1650, reward 1347, memory_length 2000, epsilon 0.2265, total time 723, loss -, compute time 12.83\n",
      "Saving model for episode: 1650\n",
      "episode 1651, reward 1323, memory_length 2000, epsilon 0.2263, total time 721, loss [2.15087890625], compute time 11.09\n",
      "episode 1652, reward 1172, memory_length 2000, epsilon 0.2261, total time 722, loss -, compute time 11.97\n",
      "episode 1653, reward 1230, memory_length 2000, epsilon 0.22589, total time 726, loss -, compute time 11.83\n",
      "episode 1654, reward 1450, memory_length 2000, epsilon 0.22569, total time 729, loss -, compute time 11.66\n",
      "episode 1655, reward 1394, memory_length 2000, epsilon 0.22549, total time 722, loss -, compute time 10.88\n",
      "episode 1656, reward 1172, memory_length 2000, epsilon 0.22528, total time 730, loss -, compute time 11.42\n",
      "episode 1657, reward 1241, memory_length 2000, epsilon 0.22508, total time 727, loss -, compute time 10.98\n",
      "episode 1658, reward 1093, memory_length 2000, epsilon 0.22488, total time 723, loss [0.8328940868377686], compute time 11.45\n",
      "episode 1659, reward 1442, memory_length 2000, epsilon 0.22468, total time 724, loss -, compute time 11.78\n",
      "episode 1660, reward 1328, memory_length 2000, epsilon 0.22447, total time 723, loss -, compute time 11.4\n",
      "Saving model for episode: 1660\n",
      "episode 1661, reward 1108, memory_length 2000, epsilon 0.22427, total time 721, loss -, compute time 11.12\n",
      "episode 1662, reward 1310, memory_length 2000, epsilon 0.22407, total time 729, loss -, compute time 13.9\n",
      "episode 1663, reward 1200, memory_length 2000, epsilon 0.22387, total time 723, loss [92.7421875], compute time 12.57\n",
      "episode 1664, reward 1289, memory_length 2000, epsilon 0.22367, total time 721, loss -, compute time 11.1\n",
      "episode 1665, reward 1221, memory_length 2000, epsilon 0.22347, total time 725, loss -, compute time 13.01\n",
      "episode 1666, reward 1155, memory_length 2000, epsilon 0.22326, total time 724, loss -, compute time 11.57\n",
      "episode 1667, reward 1242, memory_length 2000, epsilon 0.22306, total time 730, loss -, compute time 11.15\n",
      "episode 1668, reward 1037, memory_length 2000, epsilon 0.22286, total time 729, loss -, compute time 11.54\n",
      "episode 1669, reward 1485, memory_length 2000, epsilon 0.22266, total time 722, loss -, compute time 13.26\n",
      "episode 1670, reward 1173, memory_length 2000, epsilon 0.22246, total time 724, loss -, compute time 12.03\n",
      "Saving model for episode: 1670\n",
      "episode 1671, reward 972, memory_length 2000, epsilon 0.22226, total time 725, loss -, compute time 12.97\n",
      "episode 1672, reward 1137, memory_length 2000, epsilon 0.22206, total time 721, loss [443.080322265625], compute time 10.38\n",
      "episode 1673, reward 1118, memory_length 2000, epsilon 0.22186, total time 724, loss [246.76637268066406], compute time 13.08\n",
      "episode 1674, reward 1442, memory_length 2000, epsilon 0.22166, total time 724, loss -, compute time 11.73\n",
      "episode 1675, reward 1595, memory_length 2000, epsilon 0.22146, total time 722, loss -, compute time 11.9\n",
      "episode 1676, reward 1362, memory_length 2000, epsilon 0.22126, total time 721, loss -, compute time 12.56\n",
      "episode 1677, reward 1154, memory_length 2000, epsilon 0.22106, total time 727, loss -, compute time 11.72\n",
      "episode 1678, reward 1297, memory_length 2000, epsilon 0.22087, total time 722, loss -, compute time 12.7\n",
      "episode 1679, reward 1467, memory_length 2000, epsilon 0.22067, total time 726, loss -, compute time 12.6\n",
      "episode 1680, reward 1259, memory_length 2000, epsilon 0.22047, total time 725, loss -, compute time 10.75\n",
      "Saving model for episode: 1680\n",
      "episode 1681, reward 1581, memory_length 2000, epsilon 0.22027, total time 721, loss [114.65673828125], compute time 12.41\n",
      "episode 1682, reward 1137, memory_length 2000, epsilon 0.22007, total time 722, loss -, compute time 11.78\n",
      "episode 1683, reward 1001, memory_length 2000, epsilon 0.21987, total time 722, loss -, compute time 13.07\n",
      "episode 1684, reward 1277, memory_length 2000, epsilon 0.21968, total time 728, loss -, compute time 12.32\n",
      "episode 1685, reward 1487, memory_length 2000, epsilon 0.21948, total time 729, loss -, compute time 11.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1686, reward 1122, memory_length 2000, epsilon 0.21928, total time 728, loss -, compute time 11.22\n",
      "episode 1687, reward 1130, memory_length 2000, epsilon 0.21908, total time 721, loss -, compute time 12.9\n",
      "episode 1688, reward 1370, memory_length 2000, epsilon 0.21889, total time 722, loss -, compute time 12.06\n",
      "episode 1689, reward 1188, memory_length 2000, epsilon 0.21869, total time 721, loss -, compute time 11.92\n",
      "episode 1690, reward 1333, memory_length 2000, epsilon 0.21849, total time 725, loss -, compute time 11.52\n",
      "Saving model for episode: 1690\n",
      "episode 1691, reward 1323, memory_length 2000, epsilon 0.2183, total time 726, loss -, compute time 12.95\n",
      "episode 1692, reward 1404, memory_length 2000, epsilon 0.2181, total time 726, loss -, compute time 11.45\n",
      "episode 1693, reward 1104, memory_length 2000, epsilon 0.2179, total time 724, loss -, compute time 11.93\n",
      "episode 1694, reward 1298, memory_length 2000, epsilon 0.21771, total time 726, loss -, compute time 12.54\n",
      "episode 1695, reward 1380, memory_length 2000, epsilon 0.21751, total time 723, loss -, compute time 11.39\n",
      "episode 1696, reward 1127, memory_length 2000, epsilon 0.21732, total time 723, loss -, compute time 11.62\n",
      "episode 1697, reward 1321, memory_length 2000, epsilon 0.21712, total time 724, loss -, compute time 11.77\n",
      "episode 1698, reward 1574, memory_length 2000, epsilon 0.21693, total time 721, loss -, compute time 12.32\n",
      "episode 1699, reward 1144, memory_length 2000, epsilon 0.21673, total time 721, loss -, compute time 14.17\n",
      "episode 1700, reward 1321, memory_length 2000, epsilon 0.21654, total time 723, loss -, compute time 13.68\n",
      "Saving model for episode: 1700\n",
      "episode 1701, reward 1283, memory_length 2000, epsilon 0.21634, total time 731, loss -, compute time 12.63\n",
      "episode 1702, reward 1296, memory_length 2000, epsilon 0.21615, total time 725, loss -, compute time 12.86\n",
      "episode 1703, reward 1368, memory_length 2000, epsilon 0.21595, total time 730, loss -, compute time 11.8\n",
      "episode 1704, reward 834, memory_length 2000, epsilon 0.21576, total time 728, loss -, compute time 11.97\n",
      "episode 1705, reward 1626, memory_length 2000, epsilon 0.21556, total time 730, loss [129.3035888671875], compute time 12.96\n",
      "episode 1706, reward 1018, memory_length 2000, epsilon 0.21537, total time 723, loss [112.38475036621094], compute time 10.11\n",
      "episode 1707, reward 1350, memory_length 2000, epsilon 0.21518, total time 721, loss -, compute time 12.37\n",
      "episode 1708, reward 1491, memory_length 2000, epsilon 0.21498, total time 729, loss -, compute time 13.32\n",
      "episode 1709, reward 1151, memory_length 2000, epsilon 0.21479, total time 723, loss -, compute time 12.03\n",
      "episode 1710, reward 1050, memory_length 2000, epsilon 0.2146, total time 722, loss [1.5932586193084717], compute time 13.75\n",
      "Saving model for episode: 1710\n",
      "episode 1711, reward 1257, memory_length 2000, epsilon 0.2144, total time 727, loss -, compute time 12.16\n",
      "episode 1712, reward 1055, memory_length 2000, epsilon 0.21421, total time 727, loss -, compute time 13.09\n",
      "episode 1713, reward 1404, memory_length 2000, epsilon 0.21402, total time 721, loss -, compute time 13.16\n",
      "episode 1714, reward 1330, memory_length 2000, epsilon 0.21382, total time 722, loss -, compute time 12.66\n",
      "episode 1715, reward 1346, memory_length 2000, epsilon 0.21363, total time 732, loss [3.672170877456665], compute time 13.45\n",
      "episode 1716, reward 1450, memory_length 2000, epsilon 0.21344, total time 723, loss -, compute time 11.85\n",
      "episode 1717, reward 1350, memory_length 2000, epsilon 0.21325, total time 725, loss -, compute time 12.82\n",
      "episode 1718, reward 1157, memory_length 2000, epsilon 0.21306, total time 724, loss -, compute time 12.05\n",
      "episode 1719, reward 1256, memory_length 2000, epsilon 0.21286, total time 730, loss -, compute time 12.83\n",
      "episode 1720, reward 1522, memory_length 2000, epsilon 0.21267, total time 724, loss -, compute time 12.24\n",
      "Saving model for episode: 1720\n",
      "episode 1721, reward 1317, memory_length 2000, epsilon 0.21248, total time 721, loss -, compute time 13.21\n",
      "episode 1722, reward 1279, memory_length 2000, epsilon 0.21229, total time 728, loss -, compute time 12.26\n",
      "episode 1723, reward 1141, memory_length 2000, epsilon 0.2121, total time 721, loss -, compute time 12.61\n",
      "episode 1724, reward 1503, memory_length 2000, epsilon 0.21191, total time 725, loss -, compute time 12.51\n",
      "episode 1725, reward 1306, memory_length 2000, epsilon 0.21172, total time 728, loss -, compute time 12.9\n",
      "episode 1726, reward 1526, memory_length 2000, epsilon 0.21153, total time 722, loss -, compute time 13.66\n",
      "episode 1727, reward 1398, memory_length 2000, epsilon 0.21134, total time 727, loss -, compute time 13.23\n",
      "episode 1728, reward 1103, memory_length 2000, epsilon 0.21115, total time 731, loss [3.2161316871643066], compute time 12.58\n",
      "episode 1729, reward 1613, memory_length 2000, epsilon 0.21096, total time 721, loss -, compute time 11.24\n",
      "episode 1730, reward 1002, memory_length 2000, epsilon 0.21077, total time 721, loss -, compute time 10.64\n",
      "Saving model for episode: 1730\n",
      "episode 1731, reward 1351, memory_length 2000, epsilon 0.21058, total time 726, loss -, compute time 10.75\n",
      "episode 1732, reward 1540, memory_length 2000, epsilon 0.21039, total time 728, loss [2.5137557983398438], compute time 12.15\n",
      "episode 1733, reward 1058, memory_length 2000, epsilon 0.2102, total time 728, loss [2.00537109375], compute time 10.91\n",
      "episode 1734, reward 1199, memory_length 2000, epsilon 0.21001, total time 722, loss -, compute time 13.17\n",
      "episode 1735, reward 950, memory_length 2000, epsilon 0.20982, total time 721, loss -, compute time 12.15\n",
      "episode 1736, reward 1290, memory_length 2000, epsilon 0.20963, total time 722, loss -, compute time 12.88\n",
      "episode 1737, reward 1683, memory_length 2000, epsilon 0.20944, total time 728, loss -, compute time 13.0\n",
      "episode 1738, reward 1330, memory_length 2000, epsilon 0.20926, total time 725, loss [5.414578914642334], compute time 12.13\n",
      "episode 1739, reward 1232, memory_length 2000, epsilon 0.20907, total time 726, loss [119.40692901611328], compute time 12.24\n",
      "episode 1740, reward 1299, memory_length 2000, epsilon 0.20888, total time 722, loss -, compute time 12.87\n",
      "Saving model for episode: 1740\n",
      "episode 1741, reward 1135, memory_length 2000, epsilon 0.20869, total time 729, loss [106.93081665039062], compute time 13.39\n",
      "episode 1742, reward 1184, memory_length 2000, epsilon 0.2085, total time 726, loss [108.9397964477539], compute time 13.0\n",
      "episode 1743, reward 1413, memory_length 2000, epsilon 0.20832, total time 721, loss -, compute time 13.14\n",
      "episode 1744, reward 1134, memory_length 2000, epsilon 0.20813, total time 729, loss [1.3298687934875488], compute time 12.71\n",
      "episode 1745, reward 1375, memory_length 2000, epsilon 0.20794, total time 724, loss -, compute time 13.62\n",
      "episode 1746, reward 1117, memory_length 2000, epsilon 0.20775, total time 724, loss -, compute time 10.88\n",
      "episode 1747, reward 1150, memory_length 2000, epsilon 0.20757, total time 724, loss -, compute time 11.55\n",
      "episode 1748, reward 1211, memory_length 2000, epsilon 0.20738, total time 726, loss -, compute time 14.17\n",
      "episode 1749, reward 1150, memory_length 2000, epsilon 0.20719, total time 724, loss -, compute time 13.26\n",
      "episode 1750, reward 1221, memory_length 2000, epsilon 0.20701, total time 724, loss -, compute time 12.08\n",
      "Saving model for episode: 1750\n",
      "episode 1751, reward 1665, memory_length 2000, epsilon 0.20682, total time 727, loss -, compute time 11.65\n",
      "episode 1752, reward 1217, memory_length 2000, epsilon 0.20664, total time 723, loss -, compute time 11.93\n",
      "episode 1753, reward 1398, memory_length 2000, epsilon 0.20645, total time 721, loss -, compute time 13.0\n",
      "episode 1754, reward 1301, memory_length 2000, epsilon 0.20626, total time 724, loss -, compute time 12.1\n",
      "episode 1755, reward 1494, memory_length 2000, epsilon 0.20608, total time 721, loss -, compute time 12.32\n",
      "episode 1756, reward 1262, memory_length 2000, epsilon 0.20589, total time 726, loss -, compute time 12.8\n",
      "episode 1757, reward 1077, memory_length 2000, epsilon 0.20571, total time 722, loss -, compute time 11.39\n",
      "episode 1758, reward 1206, memory_length 2000, epsilon 0.20552, total time 733, loss -, compute time 11.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1759, reward 1269, memory_length 2000, epsilon 0.20534, total time 721, loss [96.55317687988281], compute time 12.7\n",
      "episode 1760, reward 1175, memory_length 2000, epsilon 0.20515, total time 723, loss -, compute time 13.6\n",
      "Saving model for episode: 1760\n",
      "episode 1761, reward 1422, memory_length 2000, epsilon 0.20497, total time 725, loss -, compute time 13.08\n",
      "episode 1762, reward 1431, memory_length 2000, epsilon 0.20478, total time 721, loss -, compute time 11.02\n",
      "episode 1763, reward 1451, memory_length 2000, epsilon 0.2046, total time 721, loss [1.691097617149353], compute time 11.4\n",
      "episode 1764, reward 1311, memory_length 2000, epsilon 0.20442, total time 727, loss -, compute time 12.08\n",
      "episode 1765, reward 1374, memory_length 2000, epsilon 0.20423, total time 724, loss [101.60363006591797], compute time 12.43\n",
      "episode 1766, reward 1344, memory_length 2000, epsilon 0.20405, total time 724, loss [0.9984180927276611], compute time 11.77\n",
      "episode 1767, reward 1054, memory_length 2000, epsilon 0.20386, total time 727, loss [2.9272069931030273], compute time 11.61\n",
      "episode 1768, reward 1268, memory_length 2000, epsilon 0.20368, total time 723, loss -, compute time 12.42\n",
      "episode 1769, reward 1119, memory_length 2000, epsilon 0.2035, total time 721, loss -, compute time 12.67\n",
      "episode 1770, reward 1176, memory_length 2000, epsilon 0.20331, total time 724, loss -, compute time 12.04\n",
      "Saving model for episode: 1770\n",
      "episode 1771, reward 1290, memory_length 2000, epsilon 0.20313, total time 723, loss -, compute time 12.73\n",
      "episode 1772, reward 1359, memory_length 2000, epsilon 0.20295, total time 721, loss -, compute time 11.86\n",
      "episode 1773, reward 1046, memory_length 2000, epsilon 0.20277, total time 721, loss -, compute time 11.98\n",
      "episode 1774, reward 1389, memory_length 2000, epsilon 0.20258, total time 721, loss [85.36869049072266], compute time 11.8\n",
      "episode 1775, reward 1324, memory_length 2000, epsilon 0.2024, total time 724, loss -, compute time 13.26\n",
      "episode 1776, reward 905, memory_length 2000, epsilon 0.20222, total time 724, loss -, compute time 11.06\n",
      "episode 1777, reward 1407, memory_length 2000, epsilon 0.20204, total time 722, loss -, compute time 10.52\n",
      "episode 1778, reward 1294, memory_length 2000, epsilon 0.20186, total time 723, loss -, compute time 12.12\n",
      "episode 1779, reward 1234, memory_length 2000, epsilon 0.20167, total time 725, loss -, compute time 12.87\n",
      "episode 1780, reward 908, memory_length 2000, epsilon 0.20149, total time 725, loss -, compute time 12.34\n",
      "Saving model for episode: 1780\n",
      "episode 1781, reward 1204, memory_length 2000, epsilon 0.20131, total time 722, loss -, compute time 11.66\n",
      "episode 1782, reward 1576, memory_length 2000, epsilon 0.20113, total time 725, loss -, compute time 12.54\n",
      "episode 1783, reward 1017, memory_length 2000, epsilon 0.20095, total time 726, loss -, compute time 12.7\n",
      "episode 1784, reward 1653, memory_length 2000, epsilon 0.20077, total time 721, loss -, compute time 11.53\n",
      "episode 1785, reward 1558, memory_length 2000, epsilon 0.20059, total time 729, loss -, compute time 11.07\n",
      "episode 1786, reward 1238, memory_length 2000, epsilon 0.20041, total time 724, loss -, compute time 13.52\n",
      "episode 1787, reward 1384, memory_length 2000, epsilon 0.20023, total time 721, loss -, compute time 12.41\n",
      "episode 1788, reward 1042, memory_length 2000, epsilon 0.20005, total time 721, loss -, compute time 11.2\n",
      "episode 1789, reward 963, memory_length 2000, epsilon 0.19987, total time 725, loss -, compute time 12.32\n",
      "episode 1790, reward 941, memory_length 2000, epsilon 0.19969, total time 727, loss -, compute time 11.28\n",
      "Saving model for episode: 1790\n",
      "episode 1791, reward 792, memory_length 2000, epsilon 0.19951, total time 729, loss -, compute time 12.65\n",
      "episode 1792, reward 928, memory_length 2000, epsilon 0.19933, total time 726, loss -, compute time 13.07\n",
      "episode 1793, reward 1405, memory_length 2000, epsilon 0.19915, total time 724, loss [1.7787773609161377], compute time 12.76\n",
      "episode 1794, reward 1463, memory_length 2000, epsilon 0.19897, total time 722, loss -, compute time 12.16\n",
      "episode 1795, reward 1220, memory_length 2000, epsilon 0.19879, total time 727, loss [1.2830193042755127], compute time 12.41\n",
      "episode 1796, reward 1177, memory_length 2000, epsilon 0.19861, total time 723, loss -, compute time 12.2\n",
      "episode 1797, reward 1430, memory_length 2000, epsilon 0.19843, total time 723, loss -, compute time 12.46\n",
      "episode 1798, reward 1324, memory_length 2000, epsilon 0.19826, total time 733, loss -, compute time 13.44\n",
      "episode 1799, reward 1362, memory_length 2000, epsilon 0.19808, total time 727, loss -, compute time 10.78\n",
      "episode 1800, reward 1382, memory_length 2000, epsilon 0.1979, total time 724, loss -, compute time 14.14\n",
      "Saving model for episode: 1800\n",
      "episode 1801, reward 1573, memory_length 2000, epsilon 0.19772, total time 726, loss [100.37464904785156], compute time 12.16\n",
      "episode 1802, reward 1100, memory_length 2000, epsilon 0.19754, total time 726, loss [1.683748483657837], compute time 13.29\n",
      "episode 1803, reward 1585, memory_length 2000, epsilon 0.19737, total time 727, loss -, compute time 12.26\n",
      "episode 1804, reward 1241, memory_length 2000, epsilon 0.19719, total time 730, loss [2.093966484069824], compute time 10.62\n",
      "episode 1805, reward 1400, memory_length 2000, epsilon 0.19701, total time 729, loss [1.2405316829681396], compute time 13.46\n",
      "episode 1806, reward 1660, memory_length 2000, epsilon 0.19683, total time 727, loss -, compute time 12.1\n",
      "episode 1807, reward 1458, memory_length 2000, epsilon 0.19666, total time 724, loss -, compute time 11.73\n",
      "episode 1808, reward 1452, memory_length 2000, epsilon 0.19648, total time 721, loss -, compute time 12.3\n",
      "episode 1809, reward 1384, memory_length 2000, epsilon 0.1963, total time 724, loss -, compute time 13.2\n",
      "episode 1810, reward 1288, memory_length 2000, epsilon 0.19613, total time 722, loss -, compute time 13.15\n",
      "Saving model for episode: 1810\n",
      "episode 1811, reward 1437, memory_length 2000, epsilon 0.19595, total time 721, loss [2.060089588165283], compute time 13.35\n",
      "episode 1812, reward 1238, memory_length 2000, epsilon 0.19577, total time 726, loss -, compute time 13.15\n",
      "episode 1813, reward 1078, memory_length 2000, epsilon 0.1956, total time 723, loss [1.2883455753326416], compute time 11.38\n",
      "episode 1814, reward 1503, memory_length 2000, epsilon 0.19542, total time 726, loss -, compute time 11.85\n",
      "episode 1815, reward 1274, memory_length 2000, epsilon 0.19525, total time 721, loss -, compute time 11.69\n",
      "episode 1816, reward 1442, memory_length 2000, epsilon 0.19507, total time 729, loss -, compute time 12.55\n",
      "episode 1817, reward 1611, memory_length 2000, epsilon 0.19489, total time 721, loss [104.42692565917969], compute time 11.31\n",
      "episode 1818, reward 1051, memory_length 2000, epsilon 0.19472, total time 722, loss -, compute time 11.17\n",
      "episode 1819, reward 1296, memory_length 2000, epsilon 0.19454, total time 721, loss -, compute time 12.43\n",
      "episode 1820, reward 1352, memory_length 2000, epsilon 0.19437, total time 725, loss [114.96534729003906], compute time 13.28\n",
      "Saving model for episode: 1820\n",
      "episode 1821, reward 1202, memory_length 2000, epsilon 0.19419, total time 728, loss -, compute time 12.31\n",
      "episode 1822, reward 1222, memory_length 2000, epsilon 0.19402, total time 721, loss -, compute time 11.42\n",
      "episode 1823, reward 1531, memory_length 2000, epsilon 0.19384, total time 722, loss -, compute time 12.8\n",
      "episode 1824, reward 1304, memory_length 2000, epsilon 0.19367, total time 724, loss [3.0414774417877197], compute time 12.7\n",
      "episode 1825, reward 1182, memory_length 2000, epsilon 0.1935, total time 723, loss [114.2071533203125], compute time 11.55\n",
      "episode 1826, reward 1378, memory_length 2000, epsilon 0.19332, total time 724, loss -, compute time 13.02\n",
      "episode 1827, reward 1063, memory_length 2000, epsilon 0.19315, total time 724, loss -, compute time 13.29\n",
      "episode 1828, reward 1071, memory_length 2000, epsilon 0.19297, total time 732, loss -, compute time 11.35\n",
      "episode 1829, reward 1550, memory_length 2000, epsilon 0.1928, total time 722, loss -, compute time 11.46\n",
      "episode 1830, reward 1413, memory_length 2000, epsilon 0.19263, total time 731, loss [0.7996891736984253], compute time 10.86\n",
      "Saving model for episode: 1830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1831, reward 1514, memory_length 2000, epsilon 0.19245, total time 724, loss -, compute time 11.59\n",
      "episode 1832, reward 1059, memory_length 2000, epsilon 0.19228, total time 721, loss -, compute time 12.56\n",
      "episode 1833, reward 1152, memory_length 2000, epsilon 0.19211, total time 729, loss -, compute time 11.72\n",
      "episode 1834, reward 1379, memory_length 2000, epsilon 0.19193, total time 727, loss -, compute time 12.93\n",
      "episode 1835, reward 1094, memory_length 2000, epsilon 0.19176, total time 723, loss -, compute time 12.24\n",
      "episode 1836, reward 1364, memory_length 2000, epsilon 0.19159, total time 731, loss -, compute time 12.81\n",
      "episode 1837, reward 1332, memory_length 2000, epsilon 0.19142, total time 734, loss [1.792574167251587], compute time 10.08\n",
      "episode 1838, reward 1224, memory_length 2000, epsilon 0.19124, total time 721, loss -, compute time 13.03\n",
      "episode 1839, reward 1468, memory_length 2000, epsilon 0.19107, total time 726, loss -, compute time 12.93\n",
      "episode 1840, reward 1439, memory_length 2000, epsilon 0.1909, total time 721, loss -, compute time 11.72\n",
      "Saving model for episode: 1840\n",
      "episode 1841, reward 1292, memory_length 2000, epsilon 0.19073, total time 721, loss -, compute time 11.71\n",
      "episode 1842, reward 1396, memory_length 2000, epsilon 0.19056, total time 721, loss -, compute time 10.86\n",
      "episode 1843, reward 1242, memory_length 2000, epsilon 0.19039, total time 727, loss -, compute time 11.17\n",
      "episode 1844, reward 1311, memory_length 2000, epsilon 0.19022, total time 725, loss -, compute time 11.94\n",
      "episode 1845, reward 1410, memory_length 2000, epsilon 0.19004, total time 724, loss -, compute time 10.39\n",
      "episode 1846, reward 1359, memory_length 2000, epsilon 0.18987, total time 731, loss -, compute time 12.67\n",
      "episode 1847, reward 1458, memory_length 2000, epsilon 0.1897, total time 725, loss -, compute time 11.65\n",
      "episode 1848, reward 1312, memory_length 2000, epsilon 0.18953, total time 721, loss -, compute time 11.62\n",
      "episode 1849, reward 1266, memory_length 2000, epsilon 0.18936, total time 723, loss -, compute time 11.43\n",
      "episode 1850, reward 1563, memory_length 2000, epsilon 0.18919, total time 724, loss -, compute time 11.09\n",
      "Saving model for episode: 1850\n",
      "episode 1851, reward 1627, memory_length 2000, epsilon 0.18902, total time 726, loss [114.07820892333984], compute time 12.92\n",
      "episode 1852, reward 1026, memory_length 2000, epsilon 0.18885, total time 729, loss [101.17225646972656], compute time 11.67\n",
      "episode 1853, reward 1162, memory_length 2000, epsilon 0.18868, total time 725, loss -, compute time 10.76\n",
      "episode 1854, reward 1176, memory_length 2000, epsilon 0.18851, total time 722, loss -, compute time 11.89\n",
      "episode 1855, reward 1452, memory_length 2000, epsilon 0.18834, total time 723, loss -, compute time 12.43\n",
      "episode 1856, reward 1068, memory_length 2000, epsilon 0.18817, total time 721, loss [2.6289725303649902], compute time 12.82\n",
      "episode 1857, reward 1233, memory_length 2000, epsilon 0.188, total time 721, loss [1.1919430494308472], compute time 12.76\n",
      "episode 1858, reward 1580, memory_length 2000, epsilon 0.18783, total time 725, loss -, compute time 12.03\n",
      "episode 1859, reward 1319, memory_length 2000, epsilon 0.18766, total time 725, loss [248.92544555664062], compute time 11.02\n",
      "episode 1860, reward 1425, memory_length 2000, epsilon 0.1875, total time 724, loss -, compute time 11.99\n",
      "Saving model for episode: 1860\n",
      "episode 1861, reward 1187, memory_length 2000, epsilon 0.18733, total time 726, loss -, compute time 11.13\n",
      "episode 1862, reward 1144, memory_length 2000, epsilon 0.18716, total time 724, loss -, compute time 12.38\n",
      "episode 1863, reward 1116, memory_length 2000, epsilon 0.18699, total time 730, loss -, compute time 10.9\n",
      "episode 1864, reward 1623, memory_length 2000, epsilon 0.18682, total time 721, loss -, compute time 12.24\n",
      "episode 1865, reward 1199, memory_length 2000, epsilon 0.18665, total time 721, loss -, compute time 12.75\n",
      "episode 1866, reward 1636, memory_length 2000, epsilon 0.18649, total time 722, loss -, compute time 12.01\n",
      "episode 1867, reward 1281, memory_length 2000, epsilon 0.18632, total time 723, loss -, compute time 11.07\n",
      "episode 1868, reward 1319, memory_length 2000, epsilon 0.18615, total time 721, loss -, compute time 12.01\n",
      "episode 1869, reward 1275, memory_length 2000, epsilon 0.18598, total time 725, loss -, compute time 11.69\n",
      "episode 1870, reward 1186, memory_length 2000, epsilon 0.18582, total time 723, loss [1.6843740940093994], compute time 12.44\n",
      "Saving model for episode: 1870\n",
      "episode 1871, reward 1134, memory_length 2000, epsilon 0.18565, total time 729, loss -, compute time 11.13\n",
      "episode 1872, reward 1136, memory_length 2000, epsilon 0.18548, total time 723, loss -, compute time 12.24\n",
      "episode 1873, reward 1404, memory_length 2000, epsilon 0.18531, total time 730, loss [1.2523688077926636], compute time 12.53\n",
      "episode 1874, reward 1231, memory_length 2000, epsilon 0.18515, total time 721, loss -, compute time 11.35\n",
      "episode 1875, reward 955, memory_length 2000, epsilon 0.18498, total time 722, loss -, compute time 12.82\n",
      "episode 1876, reward 1067, memory_length 2000, epsilon 0.18481, total time 725, loss -, compute time 13.27\n",
      "episode 1877, reward 1256, memory_length 2000, epsilon 0.18465, total time 729, loss -, compute time 13.99\n",
      "episode 1878, reward 1198, memory_length 2000, epsilon 0.18448, total time 724, loss -, compute time 12.44\n",
      "episode 1879, reward 1554, memory_length 2000, epsilon 0.18432, total time 722, loss -, compute time 10.98\n",
      "episode 1880, reward 1522, memory_length 2000, epsilon 0.18415, total time 722, loss -, compute time 11.53\n",
      "Saving model for episode: 1880\n",
      "episode 1881, reward 1162, memory_length 2000, epsilon 0.18399, total time 726, loss -, compute time 11.59\n",
      "episode 1882, reward 1187, memory_length 2000, epsilon 0.18382, total time 724, loss -, compute time 10.87\n",
      "episode 1883, reward 1347, memory_length 2000, epsilon 0.18365, total time 721, loss [83.13339233398438], compute time 12.74\n",
      "episode 1884, reward 1324, memory_length 2000, epsilon 0.18349, total time 724, loss -, compute time 12.52\n",
      "episode 1885, reward 1036, memory_length 2000, epsilon 0.18332, total time 728, loss -, compute time 12.55\n",
      "episode 1886, reward 1247, memory_length 2000, epsilon 0.18316, total time 724, loss -, compute time 12.69\n",
      "episode 1887, reward 1197, memory_length 2000, epsilon 0.18299, total time 728, loss -, compute time 13.39\n",
      "episode 1888, reward 1401, memory_length 2000, epsilon 0.18283, total time 728, loss -, compute time 12.89\n",
      "episode 1889, reward 1358, memory_length 2000, epsilon 0.18267, total time 723, loss -, compute time 9.93\n",
      "episode 1890, reward 1308, memory_length 2000, epsilon 0.1825, total time 721, loss -, compute time 13.11\n",
      "Saving model for episode: 1890\n",
      "episode 1891, reward 1380, memory_length 2000, epsilon 0.18234, total time 721, loss -, compute time 10.15\n",
      "episode 1892, reward 1019, memory_length 2000, epsilon 0.18217, total time 723, loss -, compute time 12.11\n",
      "episode 1893, reward 1303, memory_length 2000, epsilon 0.18201, total time 723, loss [114.8471450805664], compute time 13.9\n",
      "episode 1894, reward 1346, memory_length 2000, epsilon 0.18185, total time 728, loss -, compute time 12.75\n",
      "episode 1895, reward 1403, memory_length 2000, epsilon 0.18168, total time 721, loss -, compute time 11.87\n",
      "episode 1896, reward 1151, memory_length 2000, epsilon 0.18152, total time 721, loss -, compute time 11.48\n",
      "episode 1897, reward 1371, memory_length 2000, epsilon 0.18135, total time 724, loss -, compute time 12.33\n",
      "episode 1898, reward 1494, memory_length 2000, epsilon 0.18119, total time 724, loss [1.4056228399276733], compute time 13.38\n",
      "episode 1899, reward 1290, memory_length 2000, epsilon 0.18103, total time 724, loss -, compute time 13.13\n",
      "episode 1900, reward 1557, memory_length 2000, epsilon 0.18087, total time 724, loss [2.429704189300537], compute time 11.55\n",
      "Saving model for episode: 1900\n",
      "episode 1901, reward 1229, memory_length 2000, epsilon 0.1807, total time 729, loss -, compute time 10.93\n",
      "episode 1902, reward 1323, memory_length 2000, epsilon 0.18054, total time 721, loss -, compute time 12.94\n",
      "episode 1903, reward 1128, memory_length 2000, epsilon 0.18038, total time 724, loss -, compute time 12.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1904, reward 1323, memory_length 2000, epsilon 0.18022, total time 722, loss -, compute time 11.66\n",
      "episode 1905, reward 1445, memory_length 2000, epsilon 0.18005, total time 722, loss -, compute time 11.76\n",
      "episode 1906, reward 896, memory_length 2000, epsilon 0.17989, total time 722, loss -, compute time 11.12\n",
      "episode 1907, reward 1090, memory_length 2000, epsilon 0.17973, total time 724, loss -, compute time 12.87\n",
      "episode 1908, reward 1301, memory_length 2000, epsilon 0.17957, total time 737, loss -, compute time 11.6\n",
      "episode 1909, reward 1546, memory_length 2000, epsilon 0.17941, total time 724, loss -, compute time 12.5\n",
      "episode 1910, reward 1774, memory_length 2000, epsilon 0.17925, total time 727, loss -, compute time 12.48\n",
      "Saving model for episode: 1910\n",
      "episode 1911, reward 1517, memory_length 2000, epsilon 0.17908, total time 729, loss -, compute time 11.77\n",
      "episode 1912, reward 1272, memory_length 2000, epsilon 0.17892, total time 724, loss -, compute time 13.0\n",
      "episode 1913, reward 1163, memory_length 2000, epsilon 0.17876, total time 725, loss -, compute time 11.99\n",
      "episode 1914, reward 973, memory_length 2000, epsilon 0.1786, total time 729, loss [1.7005327939987183], compute time 11.75\n",
      "episode 1915, reward 1623, memory_length 2000, epsilon 0.17844, total time 723, loss -, compute time 11.46\n",
      "episode 1916, reward 1242, memory_length 2000, epsilon 0.17828, total time 727, loss -, compute time 11.8\n",
      "episode 1917, reward 1539, memory_length 2000, epsilon 0.17812, total time 731, loss -, compute time 12.2\n",
      "episode 1918, reward 1391, memory_length 2000, epsilon 0.17796, total time 724, loss [1.8143348693847656], compute time 11.79\n",
      "episode 1919, reward 1217, memory_length 2000, epsilon 0.1778, total time 727, loss -, compute time 11.09\n",
      "episode 1920, reward 1501, memory_length 2000, epsilon 0.17764, total time 721, loss -, compute time 11.1\n",
      "Saving model for episode: 1920\n",
      "episode 1921, reward 1296, memory_length 2000, epsilon 0.17748, total time 727, loss -, compute time 12.84\n",
      "episode 1922, reward 1375, memory_length 2000, epsilon 0.17732, total time 721, loss -, compute time 12.5\n",
      "episode 1923, reward 1404, memory_length 2000, epsilon 0.17716, total time 722, loss -, compute time 13.55\n",
      "episode 1924, reward 1441, memory_length 2000, epsilon 0.177, total time 726, loss -, compute time 10.37\n",
      "episode 1925, reward 1131, memory_length 2000, epsilon 0.17684, total time 725, loss -, compute time 11.23\n",
      "episode 1926, reward 1034, memory_length 2000, epsilon 0.17668, total time 725, loss -, compute time 12.98\n",
      "episode 1927, reward 1256, memory_length 2000, epsilon 0.17652, total time 725, loss -, compute time 11.28\n",
      "episode 1928, reward 1342, memory_length 2000, epsilon 0.17636, total time 726, loss -, compute time 13.11\n",
      "episode 1929, reward 1290, memory_length 2000, epsilon 0.17621, total time 724, loss -, compute time 11.53\n",
      "episode 1930, reward 1512, memory_length 2000, epsilon 0.17605, total time 728, loss -, compute time 11.18\n",
      "Saving model for episode: 1930\n",
      "episode 1931, reward 1474, memory_length 2000, epsilon 0.17589, total time 723, loss -, compute time 12.22\n",
      "episode 1932, reward 1582, memory_length 2000, epsilon 0.17573, total time 725, loss -, compute time 12.52\n",
      "episode 1933, reward 1509, memory_length 2000, epsilon 0.17557, total time 728, loss -, compute time 13.03\n",
      "episode 1934, reward 1323, memory_length 2000, epsilon 0.17542, total time 726, loss [2.1637141704559326], compute time 13.03\n",
      "episode 1935, reward 1347, memory_length 2000, epsilon 0.17526, total time 723, loss -, compute time 12.26\n",
      "episode 1936, reward 1393, memory_length 2000, epsilon 0.1751, total time 721, loss -, compute time 12.74\n",
      "episode 1937, reward 1467, memory_length 2000, epsilon 0.17494, total time 723, loss -, compute time 12.57\n",
      "episode 1938, reward 1402, memory_length 2000, epsilon 0.17478, total time 726, loss -, compute time 12.23\n",
      "episode 1939, reward 1554, memory_length 2000, epsilon 0.17463, total time 730, loss -, compute time 12.07\n",
      "episode 1940, reward 1511, memory_length 2000, epsilon 0.17447, total time 726, loss -, compute time 12.18\n",
      "Saving model for episode: 1940\n",
      "episode 1941, reward 1428, memory_length 2000, epsilon 0.17431, total time 729, loss [1.2809323072433472], compute time 11.43\n",
      "episode 1942, reward 1458, memory_length 2000, epsilon 0.17416, total time 738, loss -, compute time 13.45\n",
      "episode 1943, reward 1482, memory_length 2000, epsilon 0.174, total time 724, loss -, compute time 11.95\n",
      "episode 1944, reward 1270, memory_length 2000, epsilon 0.17384, total time 721, loss -, compute time 11.76\n",
      "episode 1945, reward 1220, memory_length 2000, epsilon 0.17369, total time 730, loss -, compute time 12.56\n",
      "episode 1946, reward 1103, memory_length 2000, epsilon 0.17353, total time 723, loss -, compute time 12.72\n",
      "episode 1947, reward 1605, memory_length 2000, epsilon 0.17337, total time 723, loss [1.7583930492401123], compute time 12.06\n",
      "episode 1948, reward 1279, memory_length 2000, epsilon 0.17322, total time 724, loss -, compute time 12.62\n",
      "episode 1949, reward 1379, memory_length 2000, epsilon 0.17306, total time 725, loss -, compute time 11.83\n",
      "episode 1950, reward 1440, memory_length 2000, epsilon 0.17291, total time 722, loss [86.33840942382812], compute time 11.53\n",
      "Saving model for episode: 1950\n",
      "episode 1951, reward 1711, memory_length 2000, epsilon 0.17275, total time 723, loss -, compute time 11.74\n",
      "episode 1952, reward 1412, memory_length 2000, epsilon 0.1726, total time 724, loss -, compute time 11.63\n",
      "episode 1953, reward 1181, memory_length 2000, epsilon 0.17244, total time 725, loss -, compute time 11.84\n",
      "episode 1954, reward 1436, memory_length 2000, epsilon 0.17229, total time 729, loss -, compute time 10.53\n",
      "episode 1955, reward 1359, memory_length 2000, epsilon 0.17213, total time 722, loss -, compute time 13.09\n",
      "episode 1956, reward 1446, memory_length 2000, epsilon 0.17198, total time 723, loss [145.5731964111328], compute time 13.05\n",
      "episode 1957, reward 1438, memory_length 2000, epsilon 0.17182, total time 722, loss -, compute time 11.39\n",
      "episode 1958, reward 1143, memory_length 2000, epsilon 0.17167, total time 722, loss -, compute time 10.47\n",
      "episode 1959, reward 1589, memory_length 2000, epsilon 0.17151, total time 726, loss -, compute time 12.54\n",
      "episode 1960, reward 1206, memory_length 2000, epsilon 0.17136, total time 725, loss -, compute time 12.37\n",
      "Saving model for episode: 1960\n",
      "episode 1961, reward 1620, memory_length 2000, epsilon 0.1712, total time 725, loss -, compute time 12.37\n",
      "episode 1962, reward 1355, memory_length 2000, epsilon 0.17105, total time 726, loss -, compute time 12.69\n",
      "episode 1963, reward 1314, memory_length 2000, epsilon 0.1709, total time 723, loss -, compute time 12.95\n",
      "episode 1964, reward 1465, memory_length 2000, epsilon 0.17074, total time 723, loss -, compute time 12.15\n",
      "episode 1965, reward 1367, memory_length 2000, epsilon 0.17059, total time 721, loss -, compute time 13.58\n",
      "episode 1966, reward 1274, memory_length 2000, epsilon 0.17044, total time 721, loss -, compute time 10.4\n",
      "episode 1967, reward 1693, memory_length 2000, epsilon 0.17028, total time 726, loss [110.07483673095703], compute time 12.92\n",
      "episode 1968, reward 1329, memory_length 2000, epsilon 0.17013, total time 721, loss [2.1760475635528564], compute time 12.05\n",
      "episode 1969, reward 1176, memory_length 2000, epsilon 0.16998, total time 723, loss -, compute time 14.0\n",
      "episode 1970, reward 1308, memory_length 2000, epsilon 0.16982, total time 722, loss -, compute time 12.59\n",
      "Saving model for episode: 1970\n",
      "episode 1971, reward 1193, memory_length 2000, epsilon 0.16967, total time 723, loss -, compute time 12.3\n",
      "episode 1972, reward 1197, memory_length 2000, epsilon 0.16952, total time 729, loss -, compute time 12.26\n",
      "episode 1973, reward 1270, memory_length 2000, epsilon 0.16936, total time 724, loss [325.86004638671875], compute time 12.28\n",
      "episode 1974, reward 1182, memory_length 2000, epsilon 0.16921, total time 721, loss -, compute time 11.65\n",
      "episode 1975, reward 1417, memory_length 2000, epsilon 0.16906, total time 721, loss -, compute time 14.19\n",
      "episode 1976, reward 1625, memory_length 2000, epsilon 0.16891, total time 726, loss -, compute time 13.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1977, reward 1478, memory_length 2000, epsilon 0.16876, total time 721, loss -, compute time 11.9\n",
      "episode 1978, reward 1365, memory_length 2000, epsilon 0.1686, total time 721, loss [242.08702087402344], compute time 11.86\n",
      "episode 1979, reward 1368, memory_length 2000, epsilon 0.16845, total time 724, loss -, compute time 11.72\n",
      "episode 1980, reward 1328, memory_length 2000, epsilon 0.1683, total time 727, loss -, compute time 12.11\n",
      "Saving model for episode: 1980\n",
      "episode 1981, reward 1584, memory_length 2000, epsilon 0.16815, total time 726, loss -, compute time 11.55\n",
      "episode 1982, reward 1343, memory_length 2000, epsilon 0.168, total time 722, loss -, compute time 11.11\n",
      "episode 1983, reward 1361, memory_length 2000, epsilon 0.16785, total time 729, loss -, compute time 11.54\n",
      "episode 1984, reward 1481, memory_length 2000, epsilon 0.1677, total time 731, loss -, compute time 12.76\n",
      "episode 1985, reward 1559, memory_length 2000, epsilon 0.16755, total time 723, loss -, compute time 11.47\n",
      "episode 1986, reward 1261, memory_length 2000, epsilon 0.16739, total time 721, loss [3.363889455795288], compute time 11.62\n",
      "episode 1987, reward 1130, memory_length 2000, epsilon 0.16724, total time 726, loss -, compute time 10.87\n",
      "episode 1988, reward 1292, memory_length 2000, epsilon 0.16709, total time 722, loss -, compute time 12.52\n",
      "episode 1989, reward 1011, memory_length 2000, epsilon 0.16694, total time 723, loss -, compute time 13.18\n",
      "episode 1990, reward 1411, memory_length 2000, epsilon 0.16679, total time 721, loss -, compute time 10.73\n",
      "Saving model for episode: 1990\n",
      "episode 1991, reward 1209, memory_length 2000, epsilon 0.16664, total time 723, loss -, compute time 12.34\n",
      "episode 1992, reward 1370, memory_length 2000, epsilon 0.16649, total time 727, loss -, compute time 12.42\n",
      "episode 1993, reward 1780, memory_length 2000, epsilon 0.16634, total time 725, loss -, compute time 12.2\n",
      "episode 1994, reward 1558, memory_length 2000, epsilon 0.16619, total time 722, loss -, compute time 11.31\n",
      "episode 1995, reward 1526, memory_length 2000, epsilon 0.16604, total time 721, loss -, compute time 11.39\n",
      "episode 1996, reward 1630, memory_length 2000, epsilon 0.1659, total time 724, loss -, compute time 12.92\n",
      "episode 1997, reward 1383, memory_length 2000, epsilon 0.16575, total time 728, loss -, compute time 11.57\n",
      "episode 1998, reward 1283, memory_length 2000, epsilon 0.1656, total time 723, loss -, compute time 12.27\n",
      "episode 1999, reward 1607, memory_length 2000, epsilon 0.16545, total time 732, loss -, compute time 11.85\n",
      "episode 2000, reward 976, memory_length 2000, epsilon 0.1653, total time 721, loss -, compute time 12.08\n",
      "Saving model for episode: 2000\n",
      "episode 2001, reward 1213, memory_length 2000, epsilon 0.16515, total time 723, loss -, compute time 13.01\n",
      "episode 2002, reward 1418, memory_length 2000, epsilon 0.165, total time 732, loss -, compute time 12.54\n",
      "episode 2003, reward 1237, memory_length 2000, epsilon 0.16485, total time 722, loss -, compute time 12.35\n",
      "episode 2004, reward 1355, memory_length 2000, epsilon 0.1647, total time 727, loss [120.53455352783203], compute time 12.77\n",
      "episode 2005, reward 1337, memory_length 2000, epsilon 0.16456, total time 722, loss -, compute time 10.98\n",
      "episode 2006, reward 1685, memory_length 2000, epsilon 0.16441, total time 723, loss -, compute time 13.38\n",
      "episode 2007, reward 1215, memory_length 2000, epsilon 0.16426, total time 730, loss -, compute time 13.82\n",
      "episode 2008, reward 1319, memory_length 2000, epsilon 0.16411, total time 725, loss -, compute time 12.38\n",
      "episode 2009, reward 1440, memory_length 2000, epsilon 0.16397, total time 724, loss -, compute time 11.51\n",
      "episode 2010, reward 1400, memory_length 2000, epsilon 0.16382, total time 726, loss -, compute time 12.16\n",
      "Saving model for episode: 2010\n",
      "episode 2011, reward 1487, memory_length 2000, epsilon 0.16367, total time 727, loss -, compute time 12.4\n",
      "episode 2012, reward 1521, memory_length 2000, epsilon 0.16352, total time 722, loss [1.8872880935668945], compute time 11.26\n",
      "episode 2013, reward 1212, memory_length 2000, epsilon 0.16338, total time 721, loss -, compute time 11.87\n",
      "episode 2014, reward 1151, memory_length 2000, epsilon 0.16323, total time 721, loss -, compute time 13.74\n",
      "episode 2015, reward 1436, memory_length 2000, epsilon 0.16308, total time 724, loss [1.5267549753189087], compute time 12.04\n",
      "episode 2016, reward 1283, memory_length 2000, epsilon 0.16294, total time 727, loss -, compute time 12.75\n",
      "episode 2017, reward 1305, memory_length 2000, epsilon 0.16279, total time 721, loss -, compute time 12.1\n",
      "episode 2018, reward 1355, memory_length 2000, epsilon 0.16264, total time 722, loss -, compute time 11.96\n",
      "episode 2019, reward 1233, memory_length 2000, epsilon 0.1625, total time 723, loss -, compute time 13.13\n",
      "episode 2020, reward 1369, memory_length 2000, epsilon 0.16235, total time 724, loss -, compute time 11.16\n",
      "Saving model for episode: 2020\n",
      "episode 2021, reward 1303, memory_length 2000, epsilon 0.1622, total time 721, loss -, compute time 11.8\n",
      "episode 2022, reward 1311, memory_length 2000, epsilon 0.16206, total time 722, loss -, compute time 11.28\n",
      "episode 2023, reward 1297, memory_length 2000, epsilon 0.16191, total time 728, loss -, compute time 10.6\n",
      "episode 2024, reward 1193, memory_length 2000, epsilon 0.16177, total time 722, loss -, compute time 13.74\n",
      "episode 2025, reward 1278, memory_length 2000, epsilon 0.16162, total time 721, loss -, compute time 12.28\n",
      "episode 2026, reward 1796, memory_length 2000, epsilon 0.16148, total time 726, loss -, compute time 12.38\n",
      "episode 2027, reward 1451, memory_length 2000, epsilon 0.16133, total time 725, loss -, compute time 11.43\n",
      "episode 2028, reward 1730, memory_length 2000, epsilon 0.16119, total time 728, loss -, compute time 12.44\n",
      "episode 2029, reward 1449, memory_length 2000, epsilon 0.16104, total time 724, loss -, compute time 11.4\n",
      "episode 2030, reward 1358, memory_length 2000, epsilon 0.1609, total time 722, loss -, compute time 11.56\n",
      "Saving model for episode: 2030\n",
      "episode 2031, reward 1464, memory_length 2000, epsilon 0.16075, total time 724, loss -, compute time 11.23\n",
      "episode 2032, reward 1359, memory_length 2000, epsilon 0.16061, total time 730, loss -, compute time 10.13\n",
      "episode 2033, reward 1441, memory_length 2000, epsilon 0.16046, total time 726, loss -, compute time 10.64\n",
      "episode 2034, reward 1492, memory_length 2000, epsilon 0.16032, total time 724, loss [2.340894937515259], compute time 12.38\n",
      "episode 2035, reward 1261, memory_length 2000, epsilon 0.16017, total time 724, loss -, compute time 12.67\n",
      "episode 2036, reward 1351, memory_length 2000, epsilon 0.16003, total time 729, loss -, compute time 12.39\n",
      "episode 2037, reward 1582, memory_length 2000, epsilon 0.15989, total time 724, loss -, compute time 13.36\n",
      "episode 2038, reward 1481, memory_length 2000, epsilon 0.15974, total time 729, loss -, compute time 13.02\n",
      "episode 2039, reward 1500, memory_length 2000, epsilon 0.1596, total time 726, loss -, compute time 12.8\n",
      "episode 2040, reward 1486, memory_length 2000, epsilon 0.15945, total time 730, loss -, compute time 11.28\n",
      "Saving model for episode: 2040\n",
      "episode 2041, reward 1486, memory_length 2000, epsilon 0.15931, total time 723, loss -, compute time 11.33\n",
      "episode 2042, reward 1428, memory_length 2000, epsilon 0.15917, total time 729, loss -, compute time 10.6\n",
      "episode 2043, reward 1414, memory_length 2000, epsilon 0.15902, total time 721, loss [95.86963653564453], compute time 11.8\n",
      "episode 2044, reward 1324, memory_length 2000, epsilon 0.15888, total time 724, loss -, compute time 12.65\n",
      "episode 2045, reward 1401, memory_length 2000, epsilon 0.15874, total time 726, loss -, compute time 12.3\n",
      "episode 2046, reward 1197, memory_length 2000, epsilon 0.1586, total time 721, loss -, compute time 11.42\n",
      "episode 2047, reward 1153, memory_length 2000, epsilon 0.15845, total time 726, loss -, compute time 11.34\n",
      "episode 2048, reward 1523, memory_length 2000, epsilon 0.15831, total time 726, loss -, compute time 11.08\n",
      "episode 2049, reward 1366, memory_length 2000, epsilon 0.15817, total time 726, loss -, compute time 11.84\n",
      "episode 2050, reward 1532, memory_length 2000, epsilon 0.15803, total time 721, loss [92.12843322753906], compute time 11.86\n",
      "Saving model for episode: 2050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2051, reward 1558, memory_length 2000, epsilon 0.15788, total time 728, loss -, compute time 12.26\n",
      "episode 2052, reward 1327, memory_length 2000, epsilon 0.15774, total time 723, loss -, compute time 11.3\n",
      "episode 2053, reward 1269, memory_length 2000, epsilon 0.1576, total time 725, loss -, compute time 11.58\n",
      "episode 2054, reward 1355, memory_length 2000, epsilon 0.15746, total time 727, loss -, compute time 11.53\n",
      "episode 2055, reward 1512, memory_length 2000, epsilon 0.15732, total time 722, loss -, compute time 12.11\n",
      "episode 2056, reward 1026, memory_length 2000, epsilon 0.15717, total time 725, loss -, compute time 12.7\n",
      "episode 2057, reward 1270, memory_length 2000, epsilon 0.15703, total time 728, loss -, compute time 11.34\n",
      "episode 2058, reward 1047, memory_length 2000, epsilon 0.15689, total time 724, loss -, compute time 12.63\n",
      "episode 2059, reward 1449, memory_length 2000, epsilon 0.15675, total time 724, loss [2.2367734909057617], compute time 11.89\n",
      "episode 2060, reward 1383, memory_length 2000, epsilon 0.15661, total time 724, loss -, compute time 12.82\n",
      "Saving model for episode: 2060\n",
      "episode 2061, reward 873, memory_length 2000, epsilon 0.15647, total time 723, loss -, compute time 11.0\n",
      "episode 2062, reward 1377, memory_length 2000, epsilon 0.15633, total time 721, loss -, compute time 14.69\n",
      "episode 2063, reward 1288, memory_length 2000, epsilon 0.15619, total time 722, loss [88.24913024902344], compute time 12.33\n",
      "episode 2064, reward 1314, memory_length 2000, epsilon 0.15605, total time 721, loss -, compute time 11.08\n",
      "episode 2065, reward 1341, memory_length 2000, epsilon 0.15591, total time 727, loss -, compute time 12.82\n",
      "episode 2066, reward 1090, memory_length 2000, epsilon 0.15577, total time 724, loss -, compute time 11.99\n",
      "episode 2067, reward 1155, memory_length 2000, epsilon 0.15563, total time 723, loss -, compute time 12.99\n",
      "episode 2068, reward 1721, memory_length 2000, epsilon 0.15549, total time 721, loss -, compute time 12.07\n",
      "episode 2069, reward 1244, memory_length 2000, epsilon 0.15535, total time 723, loss -, compute time 12.55\n",
      "episode 2070, reward 1163, memory_length 2000, epsilon 0.15521, total time 723, loss -, compute time 12.55\n",
      "Saving model for episode: 2070\n",
      "episode 2071, reward 1198, memory_length 2000, epsilon 0.15507, total time 727, loss -, compute time 12.56\n",
      "episode 2072, reward 1242, memory_length 2000, epsilon 0.15493, total time 729, loss -, compute time 12.59\n",
      "episode 2073, reward 1581, memory_length 2000, epsilon 0.15479, total time 726, loss -, compute time 12.32\n",
      "episode 2074, reward 968, memory_length 2000, epsilon 0.15465, total time 728, loss -, compute time 13.21\n",
      "episode 2075, reward 1531, memory_length 2000, epsilon 0.15451, total time 725, loss -, compute time 14.25\n",
      "episode 2076, reward 1333, memory_length 2000, epsilon 0.15437, total time 728, loss [1.0843733549118042], compute time 12.72\n",
      "episode 2077, reward 1306, memory_length 2000, epsilon 0.15423, total time 728, loss -, compute time 11.13\n",
      "episode 2078, reward 1665, memory_length 2000, epsilon 0.15409, total time 722, loss [94.65479278564453], compute time 12.39\n",
      "episode 2079, reward 1080, memory_length 2000, epsilon 0.15395, total time 727, loss -, compute time 12.34\n",
      "episode 2080, reward 1291, memory_length 2000, epsilon 0.15382, total time 725, loss -, compute time 12.55\n",
      "Saving model for episode: 2080\n",
      "episode 2081, reward 1363, memory_length 2000, epsilon 0.15368, total time 721, loss -, compute time 11.99\n",
      "episode 2082, reward 1062, memory_length 2000, epsilon 0.15354, total time 726, loss -, compute time 12.84\n",
      "episode 2083, reward 1499, memory_length 2000, epsilon 0.1534, total time 721, loss -, compute time 10.92\n",
      "episode 2084, reward 1221, memory_length 2000, epsilon 0.15326, total time 725, loss -, compute time 11.73\n",
      "episode 2085, reward 1557, memory_length 2000, epsilon 0.15313, total time 726, loss -, compute time 12.21\n",
      "episode 2086, reward 998, memory_length 2000, epsilon 0.15299, total time 723, loss -, compute time 11.2\n",
      "episode 2087, reward 1729, memory_length 2000, epsilon 0.15285, total time 726, loss -, compute time 12.37\n",
      "episode 2088, reward 1486, memory_length 2000, epsilon 0.15271, total time 724, loss -, compute time 11.32\n",
      "episode 2089, reward 1327, memory_length 2000, epsilon 0.15257, total time 721, loss -, compute time 12.67\n",
      "episode 2090, reward 1242, memory_length 2000, epsilon 0.15244, total time 722, loss -, compute time 13.05\n",
      "Saving model for episode: 2090\n",
      "episode 2091, reward 1478, memory_length 2000, epsilon 0.1523, total time 728, loss -, compute time 12.4\n",
      "episode 2092, reward 1422, memory_length 2000, epsilon 0.15216, total time 723, loss -, compute time 13.14\n",
      "episode 2093, reward 1448, memory_length 2000, epsilon 0.15203, total time 722, loss -, compute time 10.9\n",
      "episode 2094, reward 1139, memory_length 2000, epsilon 0.15189, total time 723, loss [3.6693062782287598], compute time 12.94\n",
      "episode 2095, reward 1307, memory_length 2000, epsilon 0.15175, total time 725, loss -, compute time 12.15\n",
      "episode 2096, reward 1376, memory_length 2000, epsilon 0.15162, total time 722, loss -, compute time 12.51\n",
      "episode 2097, reward 1233, memory_length 2000, epsilon 0.15148, total time 722, loss -, compute time 10.9\n",
      "episode 2098, reward 1592, memory_length 2000, epsilon 0.15134, total time 722, loss -, compute time 13.77\n",
      "episode 2099, reward 1537, memory_length 2000, epsilon 0.15121, total time 722, loss -, compute time 11.93\n",
      "episode 2100, reward 1522, memory_length 2000, epsilon 0.15107, total time 728, loss -, compute time 12.38\n",
      "Saving model for episode: 2100\n",
      "episode 2101, reward 1485, memory_length 2000, epsilon 0.15094, total time 735, loss -, compute time 10.73\n",
      "episode 2102, reward 1478, memory_length 2000, epsilon 0.1508, total time 726, loss -, compute time 13.17\n",
      "episode 2103, reward 1485, memory_length 2000, epsilon 0.15066, total time 727, loss -, compute time 11.84\n",
      "episode 2104, reward 962, memory_length 2000, epsilon 0.15053, total time 724, loss -, compute time 10.42\n",
      "episode 2105, reward 1058, memory_length 2000, epsilon 0.15039, total time 726, loss -, compute time 13.82\n",
      "episode 2106, reward 1288, memory_length 2000, epsilon 0.15026, total time 724, loss [127.41836547851562], compute time 13.14\n",
      "episode 2107, reward 1680, memory_length 2000, epsilon 0.15012, total time 721, loss -, compute time 12.84\n",
      "episode 2108, reward 1540, memory_length 2000, epsilon 0.14999, total time 727, loss -, compute time 12.65\n",
      "episode 2109, reward 1473, memory_length 2000, epsilon 0.14985, total time 721, loss -, compute time 12.5\n",
      "episode 2110, reward 1292, memory_length 2000, epsilon 0.14972, total time 725, loss -, compute time 11.87\n",
      "Saving model for episode: 2110\n",
      "episode 2111, reward 1044, memory_length 2000, epsilon 0.14958, total time 727, loss -, compute time 11.38\n",
      "episode 2112, reward 1396, memory_length 2000, epsilon 0.14945, total time 725, loss -, compute time 12.64\n",
      "episode 2113, reward 1494, memory_length 2000, epsilon 0.14931, total time 727, loss -, compute time 12.3\n",
      "episode 2114, reward 1472, memory_length 2000, epsilon 0.14918, total time 723, loss -, compute time 13.04\n",
      "episode 2115, reward 1224, memory_length 2000, epsilon 0.14905, total time 728, loss -, compute time 13.04\n",
      "episode 2116, reward 1211, memory_length 2000, epsilon 0.14891, total time 723, loss -, compute time 12.59\n",
      "episode 2117, reward 1397, memory_length 2000, epsilon 0.14878, total time 725, loss [1.7915400266647339], compute time 13.2\n",
      "episode 2118, reward 1167, memory_length 2000, epsilon 0.14864, total time 722, loss -, compute time 12.47\n",
      "episode 2119, reward 1136, memory_length 2000, epsilon 0.14851, total time 725, loss -, compute time 12.43\n",
      "episode 2120, reward 1582, memory_length 2000, epsilon 0.14838, total time 724, loss -, compute time 13.29\n",
      "Saving model for episode: 2120\n",
      "episode 2121, reward 1434, memory_length 2000, epsilon 0.14824, total time 722, loss -, compute time 12.71\n",
      "episode 2122, reward 1445, memory_length 2000, epsilon 0.14811, total time 721, loss [1.9131797552108765], compute time 12.52\n",
      "episode 2123, reward 1458, memory_length 2000, epsilon 0.14798, total time 723, loss -, compute time 11.43\n",
      "episode 2124, reward 1487, memory_length 2000, epsilon 0.14784, total time 724, loss -, compute time 10.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2125, reward 1285, memory_length 2000, epsilon 0.14771, total time 721, loss -, compute time 10.78\n",
      "episode 2126, reward 1762, memory_length 2000, epsilon 0.14758, total time 721, loss -, compute time 12.73\n",
      "episode 2127, reward 1482, memory_length 2000, epsilon 0.14745, total time 728, loss -, compute time 13.07\n",
      "episode 2128, reward 1425, memory_length 2000, epsilon 0.14731, total time 723, loss -, compute time 11.93\n",
      "episode 2129, reward 1773, memory_length 2000, epsilon 0.14718, total time 727, loss -, compute time 11.75\n",
      "episode 2130, reward 1104, memory_length 2000, epsilon 0.14705, total time 726, loss -, compute time 12.46\n",
      "Saving model for episode: 2130\n",
      "episode 2131, reward 1432, memory_length 2000, epsilon 0.14692, total time 722, loss -, compute time 12.05\n",
      "episode 2132, reward 1404, memory_length 2000, epsilon 0.14678, total time 721, loss -, compute time 13.41\n",
      "episode 2133, reward 1377, memory_length 2000, epsilon 0.14665, total time 723, loss [2.0069503784179688], compute time 12.81\n",
      "episode 2134, reward 1605, memory_length 2000, epsilon 0.14652, total time 724, loss -, compute time 10.76\n",
      "episode 2135, reward 1462, memory_length 2000, epsilon 0.14639, total time 722, loss -, compute time 12.31\n",
      "episode 2136, reward 1272, memory_length 2000, epsilon 0.14626, total time 721, loss -, compute time 11.77\n",
      "episode 2137, reward 1531, memory_length 2000, epsilon 0.14612, total time 725, loss -, compute time 13.01\n",
      "episode 2138, reward 1140, memory_length 2000, epsilon 0.14599, total time 722, loss -, compute time 13.2\n",
      "episode 2139, reward 1352, memory_length 2000, epsilon 0.14586, total time 722, loss -, compute time 11.08\n",
      "episode 2140, reward 1422, memory_length 2000, epsilon 0.14573, total time 727, loss [233.46185302734375], compute time 12.38\n",
      "Saving model for episode: 2140\n",
      "episode 2141, reward 1214, memory_length 2000, epsilon 0.1456, total time 723, loss -, compute time 13.43\n",
      "episode 2142, reward 1040, memory_length 2000, epsilon 0.14547, total time 727, loss -, compute time 12.27\n",
      "episode 2143, reward 1414, memory_length 2000, epsilon 0.14534, total time 723, loss -, compute time 12.47\n",
      "episode 2144, reward 1539, memory_length 2000, epsilon 0.14521, total time 722, loss -, compute time 11.72\n",
      "episode 2145, reward 1035, memory_length 2000, epsilon 0.14508, total time 731, loss -, compute time 12.62\n",
      "episode 2146, reward 1523, memory_length 2000, epsilon 0.14495, total time 722, loss -, compute time 11.65\n",
      "episode 2147, reward 1125, memory_length 2000, epsilon 0.14481, total time 721, loss -, compute time 13.59\n",
      "episode 2148, reward 1575, memory_length 2000, epsilon 0.14468, total time 729, loss -, compute time 13.43\n",
      "episode 2149, reward 1342, memory_length 2000, epsilon 0.14455, total time 725, loss -, compute time 12.89\n",
      "episode 2150, reward 1331, memory_length 2000, epsilon 0.14442, total time 723, loss [80.27708435058594], compute time 11.77\n",
      "Saving model for episode: 2150\n",
      "episode 2151, reward 1411, memory_length 2000, epsilon 0.14429, total time 725, loss -, compute time 12.15\n",
      "episode 2152, reward 1707, memory_length 2000, epsilon 0.14416, total time 726, loss -, compute time 11.22\n",
      "episode 2153, reward 1449, memory_length 2000, epsilon 0.14403, total time 724, loss -, compute time 12.54\n",
      "episode 2154, reward 1455, memory_length 2000, epsilon 0.14391, total time 721, loss -, compute time 12.52\n",
      "episode 2155, reward 1185, memory_length 2000, epsilon 0.14378, total time 724, loss [2.8911526203155518], compute time 12.39\n",
      "episode 2156, reward 1226, memory_length 2000, epsilon 0.14365, total time 723, loss -, compute time 12.62\n",
      "episode 2157, reward 1431, memory_length 2000, epsilon 0.14352, total time 730, loss -, compute time 13.22\n",
      "episode 2158, reward 1419, memory_length 2000, epsilon 0.14339, total time 725, loss -, compute time 12.12\n",
      "episode 2159, reward 1278, memory_length 2000, epsilon 0.14326, total time 731, loss -, compute time 12.32\n",
      "episode 2160, reward 1671, memory_length 2000, epsilon 0.14313, total time 721, loss -, compute time 12.91\n",
      "Saving model for episode: 2160\n",
      "episode 2161, reward 1422, memory_length 2000, epsilon 0.143, total time 727, loss -, compute time 11.82\n",
      "episode 2162, reward 1269, memory_length 2000, epsilon 0.14287, total time 727, loss [1.5241739749908447], compute time 11.79\n",
      "episode 2163, reward 1212, memory_length 2000, epsilon 0.14274, total time 723, loss -, compute time 12.29\n",
      "episode 2164, reward 1374, memory_length 2000, epsilon 0.14262, total time 722, loss -, compute time 11.54\n",
      "episode 2165, reward 1496, memory_length 2000, epsilon 0.14249, total time 725, loss -, compute time 13.21\n",
      "episode 2166, reward 1104, memory_length 2000, epsilon 0.14236, total time 732, loss -, compute time 12.66\n",
      "episode 2167, reward 1393, memory_length 2000, epsilon 0.14223, total time 726, loss -, compute time 11.97\n",
      "episode 2168, reward 1346, memory_length 2000, epsilon 0.1421, total time 721, loss [2.5598325729370117], compute time 11.44\n",
      "episode 2169, reward 1591, memory_length 2000, epsilon 0.14198, total time 726, loss -, compute time 13.49\n",
      "episode 2170, reward 1553, memory_length 2000, epsilon 0.14185, total time 721, loss -, compute time 12.25\n",
      "Saving model for episode: 2170\n",
      "episode 2171, reward 1116, memory_length 2000, epsilon 0.14172, total time 730, loss -, compute time 12.44\n",
      "episode 2172, reward 1464, memory_length 2000, epsilon 0.14159, total time 721, loss -, compute time 11.11\n",
      "episode 2173, reward 1309, memory_length 2000, epsilon 0.14147, total time 722, loss -, compute time 11.85\n",
      "episode 2174, reward 1450, memory_length 2000, epsilon 0.14134, total time 726, loss -, compute time 12.02\n",
      "episode 2175, reward 1598, memory_length 2000, epsilon 0.14121, total time 723, loss -, compute time 12.36\n",
      "episode 2176, reward 1427, memory_length 2000, epsilon 0.14108, total time 724, loss -, compute time 13.3\n",
      "episode 2177, reward 1395, memory_length 2000, epsilon 0.14096, total time 722, loss [4.145925998687744], compute time 11.57\n",
      "episode 2178, reward 1317, memory_length 2000, epsilon 0.14083, total time 725, loss -, compute time 11.45\n",
      "episode 2179, reward 1183, memory_length 2000, epsilon 0.1407, total time 724, loss -, compute time 11.98\n",
      "episode 2180, reward 1476, memory_length 2000, epsilon 0.14058, total time 727, loss -, compute time 12.35\n",
      "Saving model for episode: 2180\n",
      "episode 2181, reward 1704, memory_length 2000, epsilon 0.14045, total time 723, loss -, compute time 12.63\n",
      "episode 2182, reward 1223, memory_length 2000, epsilon 0.14032, total time 722, loss -, compute time 13.51\n",
      "episode 2183, reward 1407, memory_length 2000, epsilon 0.1402, total time 726, loss -, compute time 12.46\n",
      "episode 2184, reward 1596, memory_length 2000, epsilon 0.14007, total time 722, loss -, compute time 13.49\n",
      "episode 2185, reward 1378, memory_length 2000, epsilon 0.13995, total time 725, loss -, compute time 11.87\n",
      "episode 2186, reward 1013, memory_length 2000, epsilon 0.13982, total time 725, loss -, compute time 12.54\n",
      "episode 2187, reward 1487, memory_length 2000, epsilon 0.13969, total time 721, loss -, compute time 12.76\n",
      "episode 2188, reward 1293, memory_length 2000, epsilon 0.13957, total time 723, loss [3.091500759124756], compute time 10.6\n",
      "episode 2189, reward 1365, memory_length 2000, epsilon 0.13944, total time 724, loss -, compute time 12.14\n",
      "episode 2190, reward 1641, memory_length 2000, epsilon 0.13932, total time 722, loss -, compute time 11.7\n",
      "Saving model for episode: 2190\n",
      "episode 2191, reward 1364, memory_length 2000, epsilon 0.13919, total time 728, loss -, compute time 11.39\n",
      "episode 2192, reward 1409, memory_length 2000, epsilon 0.13907, total time 722, loss -, compute time 11.36\n",
      "episode 2193, reward 1135, memory_length 2000, epsilon 0.13894, total time 724, loss -, compute time 12.0\n",
      "episode 2194, reward 1613, memory_length 2000, epsilon 0.13882, total time 724, loss -, compute time 11.94\n",
      "episode 2195, reward 1589, memory_length 2000, epsilon 0.13869, total time 721, loss -, compute time 12.44\n",
      "episode 2196, reward 1533, memory_length 2000, epsilon 0.13857, total time 721, loss -, compute time 12.78\n",
      "episode 2197, reward 1464, memory_length 2000, epsilon 0.13844, total time 723, loss -, compute time 12.53\n",
      "episode 2198, reward 1423, memory_length 2000, epsilon 0.13832, total time 721, loss [3.478214740753174], compute time 12.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2199, reward 1360, memory_length 2000, epsilon 0.13819, total time 733, loss -, compute time 11.64\n",
      "episode 2200, reward 1653, memory_length 2000, epsilon 0.13807, total time 722, loss -, compute time 11.88\n",
      "Saving model for episode: 2200\n",
      "episode 2201, reward 1450, memory_length 2000, epsilon 0.13795, total time 723, loss -, compute time 13.71\n",
      "episode 2202, reward 1184, memory_length 2000, epsilon 0.13782, total time 721, loss -, compute time 11.91\n",
      "episode 2203, reward 1419, memory_length 2000, epsilon 0.1377, total time 723, loss -, compute time 13.63\n",
      "episode 2204, reward 1469, memory_length 2000, epsilon 0.13757, total time 730, loss -, compute time 12.41\n",
      "episode 2205, reward 1716, memory_length 2000, epsilon 0.13745, total time 721, loss -, compute time 12.41\n",
      "episode 2206, reward 1400, memory_length 2000, epsilon 0.13733, total time 725, loss -, compute time 12.18\n",
      "episode 2207, reward 1419, memory_length 2000, epsilon 0.1372, total time 727, loss -, compute time 12.71\n",
      "episode 2208, reward 1548, memory_length 2000, epsilon 0.13708, total time 728, loss -, compute time 13.45\n",
      "episode 2209, reward 1256, memory_length 2000, epsilon 0.13696, total time 729, loss -, compute time 13.17\n",
      "episode 2210, reward 1567, memory_length 2000, epsilon 0.13683, total time 721, loss [2.531751871109009], compute time 12.63\n",
      "Saving model for episode: 2210\n",
      "episode 2211, reward 1257, memory_length 2000, epsilon 0.13671, total time 726, loss -, compute time 11.92\n",
      "episode 2212, reward 1580, memory_length 2000, epsilon 0.13659, total time 727, loss -, compute time 13.01\n",
      "episode 2213, reward 1543, memory_length 2000, epsilon 0.13646, total time 721, loss -, compute time 12.19\n",
      "episode 2214, reward 1437, memory_length 2000, epsilon 0.13634, total time 731, loss [0.9967729449272156], compute time 11.24\n",
      "episode 2215, reward 1580, memory_length 2000, epsilon 0.13622, total time 734, loss [2.072946071624756], compute time 11.43\n",
      "episode 2216, reward 1366, memory_length 2000, epsilon 0.1361, total time 727, loss [2.3128206729888916], compute time 12.18\n",
      "episode 2217, reward 1471, memory_length 2000, epsilon 0.13597, total time 723, loss -, compute time 12.34\n",
      "episode 2218, reward 1432, memory_length 2000, epsilon 0.13585, total time 725, loss -, compute time 11.84\n",
      "episode 2219, reward 1337, memory_length 2000, epsilon 0.13573, total time 727, loss -, compute time 12.13\n",
      "episode 2220, reward 1564, memory_length 2000, epsilon 0.13561, total time 725, loss -, compute time 12.14\n",
      "Saving model for episode: 2220\n",
      "episode 2221, reward 981, memory_length 2000, epsilon 0.13548, total time 728, loss -, compute time 10.94\n",
      "episode 2222, reward 1381, memory_length 2000, epsilon 0.13536, total time 725, loss -, compute time 13.42\n",
      "episode 2223, reward 897, memory_length 2000, epsilon 0.13524, total time 725, loss -, compute time 11.78\n",
      "episode 2224, reward 1338, memory_length 2000, epsilon 0.13512, total time 729, loss -, compute time 12.51\n",
      "episode 2225, reward 1234, memory_length 2000, epsilon 0.135, total time 724, loss -, compute time 11.07\n",
      "episode 2226, reward 1510, memory_length 2000, epsilon 0.13488, total time 722, loss [219.546875], compute time 12.74\n",
      "episode 2227, reward 1409, memory_length 2000, epsilon 0.13475, total time 721, loss -, compute time 12.71\n",
      "episode 2228, reward 1396, memory_length 2000, epsilon 0.13463, total time 725, loss -, compute time 12.16\n",
      "episode 2229, reward 1261, memory_length 2000, epsilon 0.13451, total time 723, loss -, compute time 11.74\n",
      "episode 2230, reward 1404, memory_length 2000, epsilon 0.13439, total time 733, loss [1.3756071329116821], compute time 12.92\n",
      "Saving model for episode: 2230\n",
      "episode 2231, reward 1509, memory_length 2000, epsilon 0.13427, total time 722, loss [77.97950744628906], compute time 12.61\n",
      "episode 2232, reward 1565, memory_length 2000, epsilon 0.13415, total time 722, loss [99.22776794433594], compute time 11.31\n",
      "episode 2233, reward 1144, memory_length 2000, epsilon 0.13403, total time 730, loss -, compute time 13.71\n",
      "episode 2234, reward 1571, memory_length 2000, epsilon 0.13391, total time 725, loss -, compute time 13.44\n",
      "episode 2235, reward 1205, memory_length 2000, epsilon 0.13379, total time 721, loss -, compute time 11.79\n",
      "episode 2236, reward 1153, memory_length 2000, epsilon 0.13367, total time 726, loss -, compute time 12.49\n",
      "episode 2237, reward 1707, memory_length 2000, epsilon 0.13355, total time 723, loss -, compute time 11.94\n",
      "episode 2238, reward 1320, memory_length 2000, epsilon 0.13343, total time 728, loss -, compute time 10.89\n",
      "episode 2239, reward 1355, memory_length 2000, epsilon 0.13331, total time 726, loss -, compute time 13.3\n",
      "episode 2240, reward 1400, memory_length 2000, epsilon 0.13319, total time 725, loss [4.310859680175781], compute time 12.54\n",
      "Saving model for episode: 2240\n",
      "episode 2241, reward 1409, memory_length 2000, epsilon 0.13307, total time 725, loss [1.2992051839828491], compute time 12.32\n",
      "episode 2242, reward 1369, memory_length 2000, epsilon 0.13295, total time 724, loss -, compute time 12.05\n",
      "episode 2243, reward 1492, memory_length 2000, epsilon 0.13283, total time 735, loss -, compute time 11.65\n",
      "episode 2244, reward 1350, memory_length 2000, epsilon 0.13271, total time 731, loss -, compute time 11.76\n",
      "episode 2245, reward 1770, memory_length 2000, epsilon 0.13259, total time 727, loss -, compute time 12.65\n",
      "episode 2246, reward 1311, memory_length 2000, epsilon 0.13247, total time 721, loss -, compute time 13.26\n",
      "episode 2247, reward 1647, memory_length 2000, epsilon 0.13235, total time 721, loss [1.2852811813354492], compute time 12.48\n",
      "episode 2248, reward 1449, memory_length 2000, epsilon 0.13223, total time 721, loss -, compute time 13.98\n",
      "episode 2249, reward 1113, memory_length 2000, epsilon 0.13211, total time 724, loss -, compute time 10.56\n",
      "episode 2250, reward 1525, memory_length 2000, epsilon 0.13199, total time 726, loss [120.25696563720703], compute time 11.49\n",
      "Saving model for episode: 2250\n",
      "episode 2251, reward 1426, memory_length 2000, epsilon 0.13188, total time 726, loss -, compute time 13.36\n",
      "episode 2252, reward 1386, memory_length 2000, epsilon 0.13176, total time 728, loss -, compute time 11.37\n",
      "episode 2253, reward 1405, memory_length 2000, epsilon 0.13164, total time 726, loss -, compute time 12.82\n",
      "episode 2254, reward 1404, memory_length 2000, epsilon 0.13152, total time 722, loss -, compute time 12.26\n",
      "episode 2255, reward 1348, memory_length 2000, epsilon 0.1314, total time 722, loss -, compute time 11.48\n",
      "episode 2256, reward 1424, memory_length 2000, epsilon 0.13128, total time 729, loss -, compute time 13.24\n",
      "episode 2257, reward 1366, memory_length 2000, epsilon 0.13116, total time 721, loss -, compute time 11.8\n",
      "episode 2258, reward 1292, memory_length 2000, epsilon 0.13105, total time 727, loss -, compute time 12.39\n",
      "episode 2259, reward 1195, memory_length 2000, epsilon 0.13093, total time 723, loss [3.3696205615997314], compute time 13.43\n",
      "episode 2260, reward 1350, memory_length 2000, epsilon 0.13081, total time 727, loss -, compute time 12.61\n",
      "Saving model for episode: 2260\n",
      "episode 2261, reward 1391, memory_length 2000, epsilon 0.13069, total time 721, loss -, compute time 11.53\n",
      "episode 2262, reward 1438, memory_length 2000, epsilon 0.13058, total time 724, loss -, compute time 12.43\n",
      "episode 2263, reward 1362, memory_length 2000, epsilon 0.13046, total time 722, loss -, compute time 13.02\n",
      "episode 2264, reward 1415, memory_length 2000, epsilon 0.13034, total time 722, loss -, compute time 11.43\n",
      "episode 2265, reward 1445, memory_length 2000, epsilon 0.13022, total time 724, loss -, compute time 11.98\n",
      "episode 2266, reward 1452, memory_length 2000, epsilon 0.13011, total time 723, loss -, compute time 12.19\n",
      "episode 2267, reward 1234, memory_length 2000, epsilon 0.12999, total time 725, loss -, compute time 13.34\n",
      "episode 2268, reward 1270, memory_length 2000, epsilon 0.12987, total time 727, loss -, compute time 12.94\n",
      "episode 2269, reward 1330, memory_length 2000, epsilon 0.12976, total time 721, loss -, compute time 13.7\n",
      "episode 2270, reward 1711, memory_length 2000, epsilon 0.12964, total time 724, loss -, compute time 13.27\n",
      "Saving model for episode: 2270\n",
      "episode 2271, reward 1376, memory_length 2000, epsilon 0.12952, total time 723, loss -, compute time 12.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2272, reward 1572, memory_length 2000, epsilon 0.12941, total time 723, loss -, compute time 12.13\n",
      "episode 2273, reward 1339, memory_length 2000, epsilon 0.12929, total time 726, loss -, compute time 11.41\n",
      "episode 2274, reward 1296, memory_length 2000, epsilon 0.12917, total time 728, loss -, compute time 13.86\n",
      "episode 2275, reward 1405, memory_length 2000, epsilon 0.12906, total time 726, loss -, compute time 11.42\n",
      "episode 2276, reward 1285, memory_length 2000, epsilon 0.12894, total time 721, loss -, compute time 10.92\n",
      "episode 2277, reward 1301, memory_length 2000, epsilon 0.12883, total time 728, loss -, compute time 11.87\n",
      "episode 2278, reward 1486, memory_length 2000, epsilon 0.12871, total time 722, loss -, compute time 13.12\n",
      "episode 2279, reward 1273, memory_length 2000, epsilon 0.12859, total time 722, loss -, compute time 12.68\n",
      "episode 2280, reward 1404, memory_length 2000, epsilon 0.12848, total time 730, loss -, compute time 11.5\n",
      "Saving model for episode: 2280\n",
      "episode 2281, reward 1548, memory_length 2000, epsilon 0.12836, total time 726, loss -, compute time 12.03\n",
      "episode 2282, reward 1058, memory_length 2000, epsilon 0.12825, total time 727, loss -, compute time 12.28\n",
      "episode 2283, reward 1364, memory_length 2000, epsilon 0.12813, total time 729, loss -, compute time 13.61\n",
      "episode 2284, reward 1025, memory_length 2000, epsilon 0.12802, total time 721, loss -, compute time 12.67\n",
      "episode 2285, reward 1339, memory_length 2000, epsilon 0.1279, total time 723, loss -, compute time 12.87\n",
      "episode 2286, reward 1665, memory_length 2000, epsilon 0.12779, total time 732, loss [98.06668853759766], compute time 12.43\n",
      "episode 2287, reward 1642, memory_length 2000, epsilon 0.12767, total time 721, loss -, compute time 13.02\n",
      "episode 2288, reward 1265, memory_length 2000, epsilon 0.12756, total time 728, loss [130.7318115234375], compute time 12.59\n",
      "episode 2289, reward 1613, memory_length 2000, epsilon 0.12744, total time 721, loss -, compute time 12.79\n",
      "episode 2290, reward 1785, memory_length 2000, epsilon 0.12733, total time 722, loss -, compute time 11.81\n",
      "Saving model for episode: 2290\n",
      "episode 2291, reward 1442, memory_length 2000, epsilon 0.12721, total time 721, loss -, compute time 11.63\n",
      "episode 2292, reward 1395, memory_length 2000, epsilon 0.1271, total time 726, loss -, compute time 11.35\n",
      "episode 2293, reward 1458, memory_length 2000, epsilon 0.12698, total time 727, loss -, compute time 13.3\n",
      "episode 2294, reward 1296, memory_length 2000, epsilon 0.12687, total time 727, loss -, compute time 13.18\n",
      "episode 2295, reward 1464, memory_length 2000, epsilon 0.12675, total time 724, loss -, compute time 11.73\n",
      "episode 2296, reward 1480, memory_length 2000, epsilon 0.12664, total time 723, loss -, compute time 13.29\n",
      "episode 2297, reward 1483, memory_length 2000, epsilon 0.12653, total time 725, loss -, compute time 13.52\n",
      "episode 2298, reward 1368, memory_length 2000, epsilon 0.12641, total time 729, loss -, compute time 11.23\n",
      "episode 2299, reward 1521, memory_length 2000, epsilon 0.1263, total time 723, loss -, compute time 12.31\n",
      "episode 2300, reward 1551, memory_length 2000, epsilon 0.12619, total time 725, loss -, compute time 12.22\n",
      "Saving model for episode: 2300\n",
      "episode 2301, reward 1464, memory_length 2000, epsilon 0.12607, total time 728, loss -, compute time 13.18\n",
      "episode 2302, reward 1489, memory_length 2000, epsilon 0.12596, total time 722, loss -, compute time 12.66\n",
      "episode 2303, reward 1601, memory_length 2000, epsilon 0.12585, total time 722, loss [1.5141700506210327], compute time 12.69\n",
      "episode 2304, reward 1450, memory_length 2000, epsilon 0.12573, total time 725, loss [125.9711685180664], compute time 13.98\n",
      "episode 2305, reward 1257, memory_length 2000, epsilon 0.12562, total time 724, loss [111.75138854980469], compute time 12.68\n",
      "episode 2306, reward 1397, memory_length 2000, epsilon 0.12551, total time 727, loss [128.7347412109375], compute time 12.42\n",
      "episode 2307, reward 1281, memory_length 2000, epsilon 0.12539, total time 722, loss -, compute time 11.43\n",
      "episode 2308, reward 1392, memory_length 2000, epsilon 0.12528, total time 724, loss -, compute time 13.28\n",
      "episode 2309, reward 1526, memory_length 2000, epsilon 0.12517, total time 721, loss [110.93986511230469], compute time 13.53\n",
      "episode 2310, reward 1240, memory_length 2000, epsilon 0.12506, total time 721, loss -, compute time 12.83\n",
      "Saving model for episode: 2310\n",
      "episode 2311, reward 1312, memory_length 2000, epsilon 0.12494, total time 728, loss -, compute time 12.51\n",
      "episode 2312, reward 1199, memory_length 2000, epsilon 0.12483, total time 726, loss -, compute time 12.19\n",
      "episode 2313, reward 1317, memory_length 2000, epsilon 0.12472, total time 722, loss -, compute time 12.32\n",
      "episode 2314, reward 1267, memory_length 2000, epsilon 0.12461, total time 728, loss -, compute time 11.73\n",
      "episode 2315, reward 1583, memory_length 2000, epsilon 0.12449, total time 726, loss [2.103830575942993], compute time 11.5\n",
      "episode 2316, reward 1267, memory_length 2000, epsilon 0.12438, total time 725, loss -, compute time 12.16\n",
      "episode 2317, reward 1762, memory_length 2000, epsilon 0.12427, total time 725, loss [2.8635377883911133], compute time 12.56\n",
      "episode 2318, reward 1553, memory_length 2000, epsilon 0.12416, total time 730, loss -, compute time 12.43\n",
      "episode 2319, reward 1428, memory_length 2000, epsilon 0.12405, total time 722, loss -, compute time 12.05\n",
      "episode 2320, reward 1482, memory_length 2000, epsilon 0.12393, total time 723, loss -, compute time 11.22\n",
      "Saving model for episode: 2320\n",
      "episode 2321, reward 1612, memory_length 2000, epsilon 0.12382, total time 723, loss -, compute time 12.11\n",
      "episode 2322, reward 1478, memory_length 2000, epsilon 0.12371, total time 722, loss -, compute time 11.67\n",
      "episode 2323, reward 1868, memory_length 2000, epsilon 0.1236, total time 723, loss -, compute time 11.54\n",
      "episode 2324, reward 1341, memory_length 2000, epsilon 0.12349, total time 731, loss -, compute time 13.19\n",
      "episode 2325, reward 1505, memory_length 2000, epsilon 0.12338, total time 728, loss -, compute time 11.53\n",
      "episode 2326, reward 1604, memory_length 2000, epsilon 0.12327, total time 722, loss -, compute time 13.46\n",
      "episode 2327, reward 1761, memory_length 2000, epsilon 0.12316, total time 730, loss -, compute time 11.35\n",
      "episode 2328, reward 1544, memory_length 2000, epsilon 0.12305, total time 725, loss -, compute time 12.33\n",
      "episode 2329, reward 1423, memory_length 2000, epsilon 0.12293, total time 723, loss -, compute time 11.82\n",
      "episode 2330, reward 1402, memory_length 2000, epsilon 0.12282, total time 722, loss -, compute time 12.29\n",
      "Saving model for episode: 2330\n",
      "episode 2331, reward 1182, memory_length 2000, epsilon 0.12271, total time 725, loss -, compute time 13.94\n",
      "episode 2332, reward 1380, memory_length 2000, epsilon 0.1226, total time 722, loss [2.2417540550231934], compute time 13.87\n",
      "episode 2333, reward 1463, memory_length 2000, epsilon 0.12249, total time 728, loss -, compute time 12.92\n",
      "episode 2334, reward 1715, memory_length 2000, epsilon 0.12238, total time 732, loss -, compute time 13.86\n",
      "episode 2335, reward 1272, memory_length 2000, epsilon 0.12227, total time 727, loss -, compute time 12.51\n",
      "episode 2336, reward 1254, memory_length 2000, epsilon 0.12216, total time 722, loss -, compute time 12.35\n",
      "episode 2337, reward 1360, memory_length 2000, epsilon 0.12205, total time 728, loss -, compute time 11.23\n",
      "episode 2338, reward 1531, memory_length 2000, epsilon 0.12194, total time 727, loss -, compute time 12.27\n",
      "episode 2339, reward 1206, memory_length 2000, epsilon 0.12183, total time 721, loss -, compute time 14.54\n",
      "episode 2340, reward 1382, memory_length 2000, epsilon 0.12172, total time 726, loss -, compute time 11.2\n",
      "Saving model for episode: 2340\n",
      "episode 2341, reward 1342, memory_length 2000, epsilon 0.12161, total time 724, loss -, compute time 12.92\n",
      "episode 2342, reward 1558, memory_length 2000, epsilon 0.1215, total time 724, loss -, compute time 13.73\n",
      "episode 2343, reward 1704, memory_length 2000, epsilon 0.1214, total time 722, loss -, compute time 11.76\n",
      "episode 2344, reward 1429, memory_length 2000, epsilon 0.12129, total time 723, loss -, compute time 12.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2345, reward 1139, memory_length 2000, epsilon 0.12118, total time 726, loss -, compute time 15.16\n",
      "episode 2346, reward 1331, memory_length 2000, epsilon 0.12107, total time 722, loss -, compute time 12.71\n",
      "episode 2347, reward 1230, memory_length 2000, epsilon 0.12096, total time 721, loss -, compute time 13.54\n",
      "episode 2348, reward 1692, memory_length 2000, epsilon 0.12085, total time 722, loss -, compute time 11.0\n",
      "episode 2349, reward 1394, memory_length 2000, epsilon 0.12074, total time 721, loss -, compute time 12.08\n",
      "episode 2350, reward 1598, memory_length 2000, epsilon 0.12063, total time 725, loss -, compute time 11.87\n",
      "Saving model for episode: 2350\n",
      "episode 2351, reward 1482, memory_length 2000, epsilon 0.12052, total time 724, loss -, compute time 11.87\n",
      "episode 2352, reward 1603, memory_length 2000, epsilon 0.12042, total time 727, loss -, compute time 12.53\n",
      "episode 2353, reward 1662, memory_length 2000, epsilon 0.12031, total time 721, loss [136.97674560546875], compute time 11.6\n",
      "episode 2354, reward 1483, memory_length 2000, epsilon 0.1202, total time 722, loss -, compute time 12.44\n",
      "episode 2355, reward 1436, memory_length 2000, epsilon 0.12009, total time 725, loss [263.9707336425781], compute time 12.29\n",
      "episode 2356, reward 1460, memory_length 2000, epsilon 0.11998, total time 722, loss -, compute time 12.69\n",
      "episode 2357, reward 1353, memory_length 2000, epsilon 0.11988, total time 722, loss [99.58377838134766], compute time 13.25\n",
      "episode 2358, reward 1693, memory_length 2000, epsilon 0.11977, total time 722, loss -, compute time 11.45\n",
      "episode 2359, reward 1546, memory_length 2000, epsilon 0.11966, total time 723, loss -, compute time 13.07\n",
      "episode 2360, reward 1650, memory_length 2000, epsilon 0.11955, total time 725, loss [2.086766004562378], compute time 11.39\n",
      "Saving model for episode: 2360\n",
      "episode 2361, reward 1436, memory_length 2000, epsilon 0.11944, total time 722, loss [68.47895812988281], compute time 12.85\n",
      "episode 2362, reward 1545, memory_length 2000, epsilon 0.11934, total time 723, loss -, compute time 12.6\n",
      "episode 2363, reward 1535, memory_length 2000, epsilon 0.11923, total time 730, loss -, compute time 12.55\n",
      "episode 2364, reward 1379, memory_length 2000, epsilon 0.11912, total time 721, loss -, compute time 14.34\n",
      "episode 2365, reward 1150, memory_length 2000, epsilon 0.11902, total time 721, loss -, compute time 12.09\n",
      "episode 2366, reward 1256, memory_length 2000, epsilon 0.11891, total time 721, loss -, compute time 13.51\n",
      "episode 2367, reward 1494, memory_length 2000, epsilon 0.1188, total time 730, loss -, compute time 12.93\n",
      "episode 2368, reward 1674, memory_length 2000, epsilon 0.11869, total time 721, loss -, compute time 12.28\n",
      "episode 2369, reward 1503, memory_length 2000, epsilon 0.11859, total time 730, loss -, compute time 12.28\n",
      "episode 2370, reward 1712, memory_length 2000, epsilon 0.11848, total time 728, loss -, compute time 12.71\n",
      "Saving model for episode: 2370\n",
      "episode 2371, reward 1767, memory_length 2000, epsilon 0.11837, total time 724, loss -, compute time 10.98\n",
      "episode 2372, reward 1400, memory_length 2000, epsilon 0.11827, total time 727, loss -, compute time 12.57\n",
      "episode 2373, reward 1350, memory_length 2000, epsilon 0.11816, total time 724, loss -, compute time 12.42\n",
      "episode 2374, reward 1424, memory_length 2000, epsilon 0.11806, total time 724, loss -, compute time 12.67\n",
      "episode 2375, reward 1414, memory_length 2000, epsilon 0.11795, total time 722, loss -, compute time 12.48\n",
      "episode 2376, reward 1433, memory_length 2000, epsilon 0.11784, total time 721, loss -, compute time 12.06\n",
      "episode 2377, reward 1528, memory_length 2000, epsilon 0.11774, total time 726, loss -, compute time 11.95\n",
      "episode 2378, reward 1391, memory_length 2000, epsilon 0.11763, total time 724, loss -, compute time 11.86\n",
      "episode 2379, reward 1490, memory_length 2000, epsilon 0.11753, total time 725, loss -, compute time 13.68\n",
      "episode 2380, reward 1495, memory_length 2000, epsilon 0.11742, total time 729, loss -, compute time 11.99\n",
      "Saving model for episode: 2380\n",
      "episode 2381, reward 1485, memory_length 2000, epsilon 0.11731, total time 727, loss -, compute time 13.08\n",
      "episode 2382, reward 1402, memory_length 2000, epsilon 0.11721, total time 721, loss -, compute time 12.42\n",
      "episode 2383, reward 1456, memory_length 2000, epsilon 0.1171, total time 726, loss -, compute time 10.51\n",
      "episode 2384, reward 1724, memory_length 2000, epsilon 0.117, total time 721, loss -, compute time 12.1\n",
      "episode 2385, reward 1598, memory_length 2000, epsilon 0.11689, total time 724, loss -, compute time 12.28\n",
      "episode 2386, reward 1409, memory_length 2000, epsilon 0.11679, total time 722, loss -, compute time 12.79\n",
      "episode 2387, reward 1379, memory_length 2000, epsilon 0.11668, total time 722, loss -, compute time 12.21\n",
      "episode 2388, reward 1510, memory_length 2000, epsilon 0.11658, total time 730, loss -, compute time 11.89\n",
      "episode 2389, reward 1350, memory_length 2000, epsilon 0.11647, total time 728, loss -, compute time 12.34\n",
      "episode 2390, reward 1172, memory_length 2000, epsilon 0.11637, total time 723, loss -, compute time 12.15\n",
      "Saving model for episode: 2390\n",
      "episode 2391, reward 1363, memory_length 2000, epsilon 0.11626, total time 721, loss -, compute time 11.21\n",
      "episode 2392, reward 1438, memory_length 2000, epsilon 0.11616, total time 722, loss -, compute time 13.12\n",
      "episode 2393, reward 1406, memory_length 2000, epsilon 0.11605, total time 722, loss -, compute time 14.43\n",
      "episode 2394, reward 1547, memory_length 2000, epsilon 0.11595, total time 721, loss -, compute time 12.84\n",
      "episode 2395, reward 1580, memory_length 2000, epsilon 0.11585, total time 723, loss -, compute time 12.22\n",
      "episode 2396, reward 1297, memory_length 2000, epsilon 0.11574, total time 726, loss -, compute time 11.5\n",
      "episode 2397, reward 958, memory_length 2000, epsilon 0.11564, total time 723, loss -, compute time 12.43\n",
      "episode 2398, reward 1764, memory_length 2000, epsilon 0.11553, total time 729, loss -, compute time 13.74\n",
      "episode 2399, reward 1161, memory_length 2000, epsilon 0.11543, total time 722, loss -, compute time 11.05\n",
      "episode 2400, reward 1539, memory_length 2000, epsilon 0.11533, total time 725, loss -, compute time 11.85\n",
      "Saving model for episode: 2400\n",
      "episode 2401, reward 1658, memory_length 2000, epsilon 0.11522, total time 722, loss -, compute time 11.45\n",
      "episode 2402, reward 1314, memory_length 2000, epsilon 0.11512, total time 726, loss [2.4914774894714355], compute time 12.59\n",
      "episode 2403, reward 1496, memory_length 2000, epsilon 0.11501, total time 729, loss -, compute time 11.87\n",
      "episode 2404, reward 1566, memory_length 2000, epsilon 0.11491, total time 722, loss -, compute time 12.87\n",
      "episode 2405, reward 1294, memory_length 2000, epsilon 0.11481, total time 722, loss -, compute time 13.39\n",
      "episode 2406, reward 1234, memory_length 2000, epsilon 0.1147, total time 724, loss -, compute time 13.4\n",
      "episode 2407, reward 1382, memory_length 2000, epsilon 0.1146, total time 725, loss -, compute time 11.81\n",
      "episode 2408, reward 1550, memory_length 2000, epsilon 0.1145, total time 722, loss -, compute time 12.76\n",
      "episode 2409, reward 1658, memory_length 2000, epsilon 0.11439, total time 722, loss -, compute time 12.65\n",
      "episode 2410, reward 1328, memory_length 2000, epsilon 0.11429, total time 734, loss -, compute time 13.45\n",
      "Saving model for episode: 2410\n",
      "episode 2411, reward 1589, memory_length 2000, epsilon 0.11419, total time 725, loss [1.5819307565689087], compute time 13.15\n",
      "episode 2412, reward 1337, memory_length 2000, epsilon 0.11409, total time 725, loss -, compute time 11.75\n",
      "episode 2413, reward 1602, memory_length 2000, epsilon 0.11398, total time 728, loss -, compute time 11.63\n",
      "episode 2414, reward 1391, memory_length 2000, epsilon 0.11388, total time 724, loss -, compute time 11.95\n",
      "episode 2415, reward 1833, memory_length 2000, epsilon 0.11378, total time 725, loss -, compute time 11.54\n",
      "episode 2416, reward 1446, memory_length 2000, epsilon 0.11368, total time 725, loss -, compute time 13.1\n",
      "episode 2417, reward 1406, memory_length 2000, epsilon 0.11357, total time 726, loss -, compute time 12.71\n",
      "episode 2418, reward 1439, memory_length 2000, epsilon 0.11347, total time 724, loss -, compute time 12.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2419, reward 1384, memory_length 2000, epsilon 0.11337, total time 721, loss -, compute time 11.97\n",
      "episode 2420, reward 1674, memory_length 2000, epsilon 0.11327, total time 724, loss -, compute time 13.19\n",
      "Saving model for episode: 2420\n",
      "episode 2421, reward 1508, memory_length 2000, epsilon 0.11317, total time 722, loss -, compute time 13.43\n",
      "episode 2422, reward 1345, memory_length 2000, epsilon 0.11306, total time 724, loss [1.0600461959838867], compute time 12.01\n",
      "episode 2423, reward 1737, memory_length 2000, epsilon 0.11296, total time 726, loss -, compute time 12.39\n",
      "episode 2424, reward 1435, memory_length 2000, epsilon 0.11286, total time 721, loss -, compute time 11.56\n",
      "episode 2425, reward 1490, memory_length 2000, epsilon 0.11276, total time 726, loss [2.5880274772644043], compute time 12.98\n",
      "episode 2426, reward 1491, memory_length 2000, epsilon 0.11266, total time 728, loss -, compute time 13.35\n",
      "episode 2427, reward 1451, memory_length 2000, epsilon 0.11256, total time 721, loss -, compute time 13.06\n",
      "episode 2428, reward 1189, memory_length 2000, epsilon 0.11246, total time 723, loss [98.96289825439453], compute time 13.75\n",
      "episode 2429, reward 1478, memory_length 2000, epsilon 0.11235, total time 722, loss -, compute time 12.9\n",
      "episode 2430, reward 1089, memory_length 2000, epsilon 0.11225, total time 724, loss -, compute time 13.43\n",
      "Saving model for episode: 2430\n",
      "episode 2431, reward 1540, memory_length 2000, epsilon 0.11215, total time 729, loss -, compute time 11.66\n",
      "episode 2432, reward 1685, memory_length 2000, epsilon 0.11205, total time 731, loss -, compute time 12.92\n",
      "episode 2433, reward 1690, memory_length 2000, epsilon 0.11195, total time 723, loss -, compute time 12.51\n",
      "episode 2434, reward 1231, memory_length 2000, epsilon 0.11185, total time 721, loss -, compute time 11.83\n",
      "episode 2435, reward 1635, memory_length 2000, epsilon 0.11175, total time 729, loss -, compute time 12.16\n",
      "episode 2436, reward 1517, memory_length 2000, epsilon 0.11165, total time 727, loss [129.70367431640625], compute time 12.16\n",
      "episode 2437, reward 1652, memory_length 2000, epsilon 0.11155, total time 730, loss -, compute time 12.81\n",
      "episode 2438, reward 1503, memory_length 2000, epsilon 0.11145, total time 726, loss -, compute time 13.57\n",
      "episode 2439, reward 1427, memory_length 2000, epsilon 0.11135, total time 722, loss -, compute time 12.72\n",
      "episode 2440, reward 1410, memory_length 2000, epsilon 0.11125, total time 728, loss -, compute time 13.26\n",
      "Saving model for episode: 2440\n",
      "episode 2441, reward 1348, memory_length 2000, epsilon 0.11115, total time 727, loss -, compute time 13.23\n",
      "episode 2442, reward 1076, memory_length 2000, epsilon 0.11105, total time 723, loss -, compute time 12.36\n",
      "episode 2443, reward 1267, memory_length 2000, epsilon 0.11095, total time 728, loss -, compute time 12.71\n",
      "episode 2444, reward 1140, memory_length 2000, epsilon 0.11085, total time 732, loss -, compute time 12.24\n",
      "episode 2445, reward 1512, memory_length 2000, epsilon 0.11075, total time 721, loss -, compute time 11.53\n",
      "episode 2446, reward 1521, memory_length 2000, epsilon 0.11065, total time 730, loss -, compute time 12.31\n",
      "episode 2447, reward 1514, memory_length 2000, epsilon 0.11055, total time 724, loss -, compute time 12.53\n",
      "episode 2448, reward 1555, memory_length 2000, epsilon 0.11045, total time 722, loss -, compute time 12.41\n",
      "episode 2449, reward 1476, memory_length 2000, epsilon 0.11035, total time 727, loss [232.30325317382812], compute time 11.62\n",
      "episode 2450, reward 1621, memory_length 2000, epsilon 0.11025, total time 724, loss -, compute time 12.66\n",
      "Saving model for episode: 2450\n",
      "episode 2451, reward 1338, memory_length 2000, epsilon 0.11015, total time 726, loss -, compute time 13.05\n",
      "episode 2452, reward 1563, memory_length 2000, epsilon 0.11005, total time 725, loss -, compute time 13.88\n",
      "episode 2453, reward 1431, memory_length 2000, epsilon 0.10995, total time 728, loss -, compute time 12.43\n",
      "episode 2454, reward 1477, memory_length 2000, epsilon 0.10985, total time 722, loss -, compute time 11.68\n",
      "episode 2455, reward 1376, memory_length 2000, epsilon 0.10976, total time 730, loss -, compute time 10.92\n",
      "episode 2456, reward 1225, memory_length 2000, epsilon 0.10966, total time 724, loss -, compute time 11.73\n",
      "episode 2457, reward 1548, memory_length 2000, epsilon 0.10956, total time 729, loss -, compute time 12.01\n",
      "episode 2458, reward 1287, memory_length 2000, epsilon 0.10946, total time 721, loss -, compute time 12.99\n",
      "episode 2459, reward 1589, memory_length 2000, epsilon 0.10936, total time 726, loss -, compute time 13.35\n",
      "episode 2460, reward 1356, memory_length 2000, epsilon 0.10926, total time 725, loss -, compute time 12.89\n",
      "Saving model for episode: 2460\n",
      "episode 2461, reward 1359, memory_length 2000, epsilon 0.10916, total time 722, loss -, compute time 12.59\n",
      "episode 2462, reward 1692, memory_length 2000, epsilon 0.10907, total time 726, loss -, compute time 13.29\n",
      "episode 2463, reward 1522, memory_length 2000, epsilon 0.10897, total time 728, loss -, compute time 11.26\n",
      "episode 2464, reward 1373, memory_length 2000, epsilon 0.10887, total time 721, loss -, compute time 12.42\n",
      "episode 2465, reward 1432, memory_length 2000, epsilon 0.10877, total time 724, loss -, compute time 14.24\n",
      "episode 2466, reward 1364, memory_length 2000, epsilon 0.10867, total time 723, loss [115.92552947998047], compute time 12.23\n",
      "episode 2467, reward 1343, memory_length 2000, epsilon 0.10858, total time 725, loss -, compute time 12.37\n",
      "episode 2468, reward 1424, memory_length 2000, epsilon 0.10848, total time 724, loss -, compute time 11.32\n",
      "episode 2469, reward 1203, memory_length 2000, epsilon 0.10838, total time 722, loss -, compute time 12.55\n",
      "episode 2470, reward 1374, memory_length 2000, epsilon 0.10828, total time 721, loss -, compute time 12.84\n",
      "Saving model for episode: 2470\n",
      "episode 2471, reward 1674, memory_length 2000, epsilon 0.10819, total time 726, loss -, compute time 12.08\n",
      "episode 2472, reward 1542, memory_length 2000, epsilon 0.10809, total time 722, loss -, compute time 12.34\n",
      "episode 2473, reward 1152, memory_length 2000, epsilon 0.10799, total time 725, loss -, compute time 12.43\n",
      "episode 2474, reward 1299, memory_length 2000, epsilon 0.10789, total time 725, loss -, compute time 13.31\n",
      "episode 2475, reward 1330, memory_length 2000, epsilon 0.1078, total time 723, loss -, compute time 11.93\n",
      "episode 2476, reward 1523, memory_length 2000, epsilon 0.1077, total time 723, loss -, compute time 11.59\n",
      "episode 2477, reward 1945, memory_length 2000, epsilon 0.1076, total time 733, loss -, compute time 12.04\n",
      "episode 2478, reward 1520, memory_length 2000, epsilon 0.10751, total time 722, loss -, compute time 11.53\n",
      "episode 2479, reward 1551, memory_length 2000, epsilon 0.10741, total time 723, loss -, compute time 12.36\n",
      "episode 2480, reward 1661, memory_length 2000, epsilon 0.10731, total time 727, loss -, compute time 11.91\n",
      "Saving model for episode: 2480\n",
      "episode 2481, reward 1743, memory_length 2000, epsilon 0.10722, total time 723, loss -, compute time 12.83\n",
      "episode 2482, reward 1254, memory_length 2000, epsilon 0.10712, total time 725, loss -, compute time 13.91\n",
      "episode 2483, reward 1522, memory_length 2000, epsilon 0.10702, total time 721, loss -, compute time 12.91\n",
      "episode 2484, reward 1473, memory_length 2000, epsilon 0.10693, total time 721, loss [1.8346564769744873], compute time 12.34\n",
      "episode 2485, reward 1571, memory_length 2000, epsilon 0.10683, total time 723, loss -, compute time 12.29\n",
      "episode 2486, reward 1891, memory_length 2000, epsilon 0.10674, total time 722, loss -, compute time 12.78\n",
      "episode 2487, reward 1646, memory_length 2000, epsilon 0.10664, total time 721, loss -, compute time 12.16\n",
      "episode 2488, reward 1334, memory_length 2000, epsilon 0.10654, total time 724, loss -, compute time 11.93\n",
      "episode 2489, reward 1508, memory_length 2000, epsilon 0.10645, total time 725, loss -, compute time 11.42\n",
      "episode 2490, reward 1379, memory_length 2000, epsilon 0.10635, total time 723, loss [1.3355891704559326], compute time 12.01\n",
      "Saving model for episode: 2490\n",
      "episode 2491, reward 1629, memory_length 2000, epsilon 0.10626, total time 721, loss -, compute time 13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2492, reward 1558, memory_length 2000, epsilon 0.10616, total time 723, loss [1.3718640804290771], compute time 11.63\n",
      "episode 2493, reward 1616, memory_length 2000, epsilon 0.10607, total time 729, loss -, compute time 12.48\n",
      "episode 2494, reward 1546, memory_length 2000, epsilon 0.10597, total time 723, loss -, compute time 13.18\n",
      "episode 2495, reward 1386, memory_length 2000, epsilon 0.10587, total time 726, loss -, compute time 11.61\n",
      "episode 2496, reward 1486, memory_length 2000, epsilon 0.10578, total time 726, loss -, compute time 11.76\n",
      "episode 2497, reward 1530, memory_length 2000, epsilon 0.10568, total time 721, loss -, compute time 13.35\n",
      "episode 2498, reward 1645, memory_length 2000, epsilon 0.10559, total time 724, loss [2.240384340286255], compute time 11.74\n",
      "episode 2499, reward 1602, memory_length 2000, epsilon 0.10549, total time 726, loss -, compute time 11.35\n",
      "episode 2500, reward 1521, memory_length 2000, epsilon 0.1054, total time 721, loss -, compute time 12.56\n",
      "Saving model for episode: 2500\n",
      "episode 2501, reward 1533, memory_length 2000, epsilon 0.1053, total time 721, loss -, compute time 12.85\n",
      "episode 2502, reward 1471, memory_length 2000, epsilon 0.10521, total time 722, loss -, compute time 12.76\n",
      "episode 2503, reward 1364, memory_length 2000, epsilon 0.10512, total time 722, loss [3.936431884765625], compute time 12.43\n",
      "episode 2504, reward 1526, memory_length 2000, epsilon 0.10502, total time 726, loss -, compute time 13.04\n",
      "episode 2505, reward 1483, memory_length 2000, epsilon 0.10493, total time 721, loss -, compute time 11.99\n",
      "episode 2506, reward 1428, memory_length 2000, epsilon 0.10483, total time 726, loss -, compute time 12.48\n",
      "episode 2507, reward 1535, memory_length 2000, epsilon 0.10474, total time 721, loss -, compute time 13.28\n",
      "episode 2508, reward 1794, memory_length 2000, epsilon 0.10464, total time 722, loss -, compute time 12.23\n",
      "episode 2509, reward 1251, memory_length 2000, epsilon 0.10455, total time 722, loss [1.6549381017684937], compute time 12.76\n",
      "episode 2510, reward 1436, memory_length 2000, epsilon 0.10445, total time 722, loss -, compute time 11.78\n",
      "Saving model for episode: 2510\n",
      "episode 2511, reward 1586, memory_length 2000, epsilon 0.10436, total time 722, loss -, compute time 11.52\n",
      "episode 2512, reward 1584, memory_length 2000, epsilon 0.10427, total time 724, loss [2.0839900970458984], compute time 12.02\n",
      "episode 2513, reward 1933, memory_length 2000, epsilon 0.10417, total time 723, loss -, compute time 12.52\n",
      "episode 2514, reward 1622, memory_length 2000, epsilon 0.10408, total time 721, loss -, compute time 12.89\n",
      "episode 2515, reward 1744, memory_length 2000, epsilon 0.10399, total time 725, loss -, compute time 11.75\n",
      "episode 2516, reward 1511, memory_length 2000, epsilon 0.10389, total time 721, loss -, compute time 12.14\n",
      "episode 2517, reward 1400, memory_length 2000, epsilon 0.1038, total time 729, loss -, compute time 12.23\n",
      "episode 2518, reward 1603, memory_length 2000, epsilon 0.10371, total time 729, loss [108.84410095214844], compute time 11.93\n",
      "episode 2519, reward 1476, memory_length 2000, epsilon 0.10361, total time 721, loss -, compute time 13.22\n",
      "episode 2520, reward 1478, memory_length 2000, epsilon 0.10352, total time 722, loss -, compute time 13.09\n",
      "Saving model for episode: 2520\n",
      "episode 2521, reward 1587, memory_length 2000, epsilon 0.10343, total time 722, loss -, compute time 11.95\n",
      "episode 2522, reward 1374, memory_length 2000, epsilon 0.10333, total time 725, loss -, compute time 12.8\n",
      "episode 2523, reward 1580, memory_length 2000, epsilon 0.10324, total time 728, loss -, compute time 12.73\n",
      "episode 2524, reward 1575, memory_length 2000, epsilon 0.10315, total time 721, loss -, compute time 12.19\n",
      "episode 2525, reward 1854, memory_length 2000, epsilon 0.10305, total time 730, loss -, compute time 12.23\n",
      "episode 2526, reward 1459, memory_length 2000, epsilon 0.10296, total time 729, loss -, compute time 12.12\n",
      "episode 2527, reward 1570, memory_length 2000, epsilon 0.10287, total time 724, loss [3.105999231338501], compute time 12.99\n",
      "episode 2528, reward 1400, memory_length 2000, epsilon 0.10278, total time 724, loss -, compute time 11.48\n",
      "episode 2529, reward 1339, memory_length 2000, epsilon 0.10268, total time 727, loss -, compute time 12.69\n",
      "episode 2530, reward 1549, memory_length 2000, epsilon 0.10259, total time 722, loss -, compute time 12.36\n",
      "Saving model for episode: 2530\n",
      "episode 2531, reward 1546, memory_length 2000, epsilon 0.1025, total time 723, loss [1.7725794315338135], compute time 12.0\n",
      "episode 2532, reward 1454, memory_length 2000, epsilon 0.10241, total time 729, loss -, compute time 13.26\n",
      "episode 2533, reward 1113, memory_length 2000, epsilon 0.10231, total time 723, loss -, compute time 11.53\n",
      "episode 2534, reward 1616, memory_length 2000, epsilon 0.10222, total time 728, loss -, compute time 12.08\n",
      "episode 2535, reward 1751, memory_length 2000, epsilon 0.10213, total time 727, loss -, compute time 11.1\n",
      "episode 2536, reward 1409, memory_length 2000, epsilon 0.10204, total time 726, loss -, compute time 12.68\n",
      "episode 2537, reward 1410, memory_length 2000, epsilon 0.10195, total time 723, loss -, compute time 12.24\n",
      "episode 2538, reward 1638, memory_length 2000, epsilon 0.10186, total time 721, loss -, compute time 12.16\n",
      "episode 2539, reward 1365, memory_length 2000, epsilon 0.10176, total time 728, loss -, compute time 12.6\n",
      "episode 2540, reward 1504, memory_length 2000, epsilon 0.10167, total time 727, loss -, compute time 12.88\n",
      "Saving model for episode: 2540\n",
      "episode 2541, reward 1594, memory_length 2000, epsilon 0.10158, total time 724, loss -, compute time 13.58\n",
      "episode 2542, reward 2012, memory_length 2000, epsilon 0.10149, total time 730, loss -, compute time 11.16\n",
      "episode 2543, reward 1316, memory_length 2000, epsilon 0.1014, total time 723, loss [2.243645429611206], compute time 11.33\n",
      "episode 2544, reward 1449, memory_length 2000, epsilon 0.10131, total time 722, loss -, compute time 12.66\n",
      "episode 2545, reward 1576, memory_length 2000, epsilon 0.10122, total time 728, loss -, compute time 11.57\n",
      "episode 2546, reward 1566, memory_length 2000, epsilon 0.10112, total time 727, loss -, compute time 11.86\n",
      "episode 2547, reward 1675, memory_length 2000, epsilon 0.10103, total time 725, loss -, compute time 11.89\n",
      "episode 2548, reward 1348, memory_length 2000, epsilon 0.10094, total time 721, loss -, compute time 11.44\n",
      "episode 2549, reward 1306, memory_length 2000, epsilon 0.10085, total time 726, loss -, compute time 11.59\n",
      "episode 2550, reward 1517, memory_length 2000, epsilon 0.10076, total time 725, loss -, compute time 13.05\n",
      "Saving model for episode: 2550\n",
      "episode 2551, reward 1405, memory_length 2000, epsilon 0.10067, total time 726, loss -, compute time 11.49\n",
      "episode 2552, reward 1372, memory_length 2000, epsilon 0.10058, total time 721, loss -, compute time 11.51\n",
      "episode 2553, reward 1414, memory_length 2000, epsilon 0.10049, total time 727, loss -, compute time 11.87\n",
      "episode 2554, reward 1389, memory_length 2000, epsilon 0.1004, total time 723, loss -, compute time 11.41\n",
      "episode 2555, reward 1155, memory_length 2000, epsilon 0.10031, total time 727, loss -, compute time 11.76\n",
      "episode 2556, reward 1321, memory_length 2000, epsilon 0.10022, total time 721, loss [1.2931334972381592], compute time 11.1\n",
      "episode 2557, reward 1625, memory_length 2000, epsilon 0.10013, total time 724, loss -, compute time 12.82\n",
      "episode 2558, reward 1577, memory_length 2000, epsilon 0.10004, total time 722, loss -, compute time 12.11\n",
      "episode 2559, reward 1661, memory_length 2000, epsilon 0.09995, total time 725, loss [1.6544671058654785], compute time 11.51\n",
      "episode 2560, reward 1441, memory_length 2000, epsilon 0.09986, total time 723, loss -, compute time 11.99\n",
      "Saving model for episode: 2560\n",
      "episode 2561, reward 1525, memory_length 2000, epsilon 0.09977, total time 723, loss -, compute time 12.01\n",
      "episode 2562, reward 1323, memory_length 2000, epsilon 0.09968, total time 731, loss -, compute time 12.04\n",
      "episode 2563, reward 2084, memory_length 2000, epsilon 0.09959, total time 728, loss -, compute time 12.64\n",
      "episode 2564, reward 1583, memory_length 2000, epsilon 0.0995, total time 730, loss [121.8601303100586], compute time 10.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2565, reward 1254, memory_length 2000, epsilon 0.09941, total time 723, loss -, compute time 11.69\n",
      "episode 2566, reward 1373, memory_length 2000, epsilon 0.09932, total time 725, loss -, compute time 12.08\n",
      "episode 2567, reward 1558, memory_length 2000, epsilon 0.09923, total time 727, loss -, compute time 11.73\n",
      "episode 2568, reward 1419, memory_length 2000, epsilon 0.09914, total time 727, loss -, compute time 13.83\n",
      "episode 2569, reward 1845, memory_length 2000, epsilon 0.09905, total time 722, loss -, compute time 11.64\n",
      "episode 2570, reward 1612, memory_length 2000, epsilon 0.09896, total time 722, loss -, compute time 12.05\n",
      "Saving model for episode: 2570\n",
      "episode 2571, reward 1365, memory_length 2000, epsilon 0.09887, total time 722, loss -, compute time 13.63\n",
      "episode 2572, reward 1470, memory_length 2000, epsilon 0.09879, total time 724, loss -, compute time 12.25\n",
      "episode 2573, reward 1368, memory_length 2000, epsilon 0.0987, total time 721, loss -, compute time 12.98\n",
      "episode 2574, reward 1430, memory_length 2000, epsilon 0.09861, total time 726, loss [1.6162898540496826], compute time 11.98\n",
      "episode 2575, reward 1791, memory_length 2000, epsilon 0.09852, total time 722, loss -, compute time 11.69\n",
      "episode 2576, reward 1668, memory_length 2000, epsilon 0.09843, total time 723, loss -, compute time 12.4\n",
      "episode 2577, reward 1495, memory_length 2000, epsilon 0.09834, total time 725, loss -, compute time 13.42\n",
      "episode 2578, reward 1322, memory_length 2000, epsilon 0.09825, total time 724, loss -, compute time 13.14\n",
      "episode 2579, reward 1253, memory_length 2000, epsilon 0.09817, total time 729, loss -, compute time 13.28\n",
      "episode 2580, reward 1746, memory_length 2000, epsilon 0.09808, total time 722, loss -, compute time 11.48\n",
      "Saving model for episode: 2580\n",
      "episode 2581, reward 1555, memory_length 2000, epsilon 0.09799, total time 729, loss -, compute time 12.91\n",
      "episode 2582, reward 1261, memory_length 2000, epsilon 0.0979, total time 730, loss -, compute time 13.2\n",
      "episode 2583, reward 1468, memory_length 2000, epsilon 0.09781, total time 728, loss [1.2592761516571045], compute time 11.95\n",
      "episode 2584, reward 1603, memory_length 2000, epsilon 0.09772, total time 726, loss -, compute time 13.29\n",
      "episode 2585, reward 1321, memory_length 2000, epsilon 0.09764, total time 724, loss -, compute time 13.17\n",
      "episode 2586, reward 1575, memory_length 2000, epsilon 0.09755, total time 723, loss -, compute time 12.03\n",
      "episode 2587, reward 1343, memory_length 2000, epsilon 0.09746, total time 729, loss -, compute time 12.85\n",
      "episode 2588, reward 1694, memory_length 2000, epsilon 0.09737, total time 722, loss -, compute time 11.02\n",
      "episode 2589, reward 1637, memory_length 2000, epsilon 0.09729, total time 726, loss -, compute time 12.89\n",
      "episode 2590, reward 1510, memory_length 2000, epsilon 0.0972, total time 723, loss -, compute time 12.37\n",
      "Saving model for episode: 2590\n",
      "episode 2591, reward 1581, memory_length 2000, epsilon 0.09711, total time 729, loss -, compute time 12.06\n",
      "episode 2592, reward 1719, memory_length 2000, epsilon 0.09702, total time 727, loss -, compute time 11.63\n",
      "episode 2593, reward 1339, memory_length 2000, epsilon 0.09694, total time 723, loss -, compute time 11.06\n",
      "episode 2594, reward 1287, memory_length 2000, epsilon 0.09685, total time 728, loss -, compute time 13.45\n",
      "episode 2595, reward 1370, memory_length 2000, epsilon 0.09676, total time 721, loss -, compute time 11.75\n",
      "episode 2596, reward 1645, memory_length 2000, epsilon 0.09668, total time 724, loss -, compute time 11.12\n",
      "episode 2597, reward 1352, memory_length 2000, epsilon 0.09659, total time 722, loss -, compute time 12.44\n",
      "episode 2598, reward 1655, memory_length 2000, epsilon 0.0965, total time 727, loss [1.8891160488128662], compute time 11.95\n",
      "episode 2599, reward 1648, memory_length 2000, epsilon 0.09641, total time 725, loss -, compute time 13.83\n",
      "episode 2600, reward 1700, memory_length 2000, epsilon 0.09633, total time 722, loss -, compute time 12.03\n",
      "Saving model for episode: 2600\n",
      "episode 2601, reward 1426, memory_length 2000, epsilon 0.09624, total time 723, loss -, compute time 11.99\n",
      "episode 2602, reward 1770, memory_length 2000, epsilon 0.09615, total time 725, loss -, compute time 13.18\n",
      "episode 2603, reward 1488, memory_length 2000, epsilon 0.09607, total time 729, loss -, compute time 12.61\n",
      "episode 2604, reward 1269, memory_length 2000, epsilon 0.09598, total time 728, loss -, compute time 11.69\n",
      "episode 2605, reward 1613, memory_length 2000, epsilon 0.0959, total time 729, loss -, compute time 11.82\n",
      "episode 2606, reward 1645, memory_length 2000, epsilon 0.09581, total time 724, loss [1.5583312511444092], compute time 12.6\n",
      "episode 2607, reward 1349, memory_length 2000, epsilon 0.09572, total time 721, loss -, compute time 13.52\n",
      "episode 2608, reward 1428, memory_length 2000, epsilon 0.09564, total time 727, loss -, compute time 12.83\n",
      "episode 2609, reward 1305, memory_length 2000, epsilon 0.09555, total time 724, loss [110.5586166381836], compute time 11.59\n",
      "episode 2610, reward 1585, memory_length 2000, epsilon 0.09546, total time 727, loss -, compute time 10.7\n",
      "Saving model for episode: 2610\n",
      "episode 2611, reward 1703, memory_length 2000, epsilon 0.09538, total time 722, loss -, compute time 11.63\n",
      "episode 2612, reward 1587, memory_length 2000, epsilon 0.09529, total time 724, loss -, compute time 11.5\n",
      "episode 2613, reward 1288, memory_length 2000, epsilon 0.09521, total time 721, loss [3.3675577640533447], compute time 11.79\n",
      "episode 2614, reward 1638, memory_length 2000, epsilon 0.09512, total time 726, loss -, compute time 13.14\n",
      "episode 2615, reward 1729, memory_length 2000, epsilon 0.09504, total time 728, loss -, compute time 13.34\n",
      "episode 2616, reward 1685, memory_length 2000, epsilon 0.09495, total time 724, loss -, compute time 12.62\n",
      "episode 2617, reward 1764, memory_length 2000, epsilon 0.09487, total time 731, loss -, compute time 12.28\n",
      "episode 2618, reward 1512, memory_length 2000, epsilon 0.09478, total time 725, loss -, compute time 12.95\n",
      "episode 2619, reward 1253, memory_length 2000, epsilon 0.09469, total time 722, loss -, compute time 13.58\n",
      "episode 2620, reward 1478, memory_length 2000, epsilon 0.09461, total time 722, loss -, compute time 15.11\n",
      "Saving model for episode: 2620\n",
      "episode 2621, reward 1450, memory_length 2000, epsilon 0.09452, total time 722, loss -, compute time 13.38\n",
      "episode 2622, reward 1600, memory_length 2000, epsilon 0.09444, total time 722, loss -, compute time 14.14\n",
      "episode 2623, reward 1476, memory_length 2000, epsilon 0.09435, total time 730, loss -, compute time 11.97\n",
      "episode 2624, reward 1545, memory_length 2000, epsilon 0.09427, total time 723, loss -, compute time 11.5\n",
      "episode 2625, reward 1536, memory_length 2000, epsilon 0.09418, total time 726, loss -, compute time 12.55\n",
      "episode 2626, reward 1738, memory_length 2000, epsilon 0.0941, total time 729, loss -, compute time 12.64\n",
      "episode 2627, reward 1684, memory_length 2000, epsilon 0.09402, total time 726, loss -, compute time 12.16\n",
      "episode 2628, reward 1782, memory_length 2000, epsilon 0.09393, total time 723, loss -, compute time 12.14\n",
      "episode 2629, reward 1409, memory_length 2000, epsilon 0.09385, total time 723, loss -, compute time 12.1\n",
      "episode 2630, reward 1764, memory_length 2000, epsilon 0.09376, total time 721, loss -, compute time 11.97\n",
      "Saving model for episode: 2630\n",
      "episode 2631, reward 1800, memory_length 2000, epsilon 0.09368, total time 727, loss -, compute time 12.43\n",
      "episode 2632, reward 1448, memory_length 2000, epsilon 0.09359, total time 722, loss [2.7565884590148926], compute time 12.15\n",
      "episode 2633, reward 1679, memory_length 2000, epsilon 0.09351, total time 721, loss -, compute time 13.77\n",
      "episode 2634, reward 1617, memory_length 2000, epsilon 0.09342, total time 722, loss -, compute time 13.25\n",
      "episode 2635, reward 1527, memory_length 2000, epsilon 0.09334, total time 722, loss -, compute time 13.16\n",
      "episode 2636, reward 1246, memory_length 2000, epsilon 0.09326, total time 723, loss -, compute time 12.63\n",
      "episode 2637, reward 1405, memory_length 2000, epsilon 0.09317, total time 724, loss -, compute time 12.98\n",
      "episode 2638, reward 1548, memory_length 2000, epsilon 0.09309, total time 722, loss -, compute time 13.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2639, reward 1365, memory_length 2000, epsilon 0.09301, total time 725, loss -, compute time 11.66\n",
      "episode 2640, reward 1667, memory_length 2000, epsilon 0.09292, total time 724, loss -, compute time 13.43\n",
      "Saving model for episode: 2640\n",
      "episode 2641, reward 1585, memory_length 2000, epsilon 0.09284, total time 724, loss -, compute time 12.73\n",
      "episode 2642, reward 1602, memory_length 2000, epsilon 0.09275, total time 725, loss -, compute time 12.16\n",
      "episode 2643, reward 1568, memory_length 2000, epsilon 0.09267, total time 723, loss -, compute time 13.15\n",
      "episode 2644, reward 1404, memory_length 2000, epsilon 0.09259, total time 726, loss -, compute time 11.91\n",
      "episode 2645, reward 1611, memory_length 2000, epsilon 0.0925, total time 729, loss -, compute time 12.89\n",
      "episode 2646, reward 1250, memory_length 2000, epsilon 0.09242, total time 722, loss -, compute time 11.7\n",
      "episode 2647, reward 1297, memory_length 2000, epsilon 0.09234, total time 724, loss -, compute time 11.54\n",
      "episode 2648, reward 1596, memory_length 2000, epsilon 0.09225, total time 724, loss -, compute time 14.22\n",
      "episode 2649, reward 1683, memory_length 2000, epsilon 0.09217, total time 721, loss -, compute time 12.19\n",
      "episode 2650, reward 1616, memory_length 2000, epsilon 0.09209, total time 724, loss -, compute time 12.37\n",
      "Saving model for episode: 2650\n",
      "episode 2651, reward 1393, memory_length 2000, epsilon 0.09201, total time 721, loss -, compute time 11.03\n",
      "episode 2652, reward 1561, memory_length 2000, epsilon 0.09192, total time 723, loss -, compute time 12.24\n",
      "episode 2653, reward 1352, memory_length 2000, epsilon 0.09184, total time 724, loss -, compute time 11.5\n",
      "episode 2654, reward 1607, memory_length 2000, epsilon 0.09176, total time 728, loss -, compute time 12.87\n",
      "episode 2655, reward 1543, memory_length 2000, epsilon 0.09168, total time 727, loss [1.53117835521698], compute time 13.39\n",
      "episode 2656, reward 1602, memory_length 2000, epsilon 0.09159, total time 721, loss -, compute time 12.78\n",
      "episode 2657, reward 1704, memory_length 2000, epsilon 0.09151, total time 721, loss -, compute time 11.3\n",
      "episode 2658, reward 1123, memory_length 2000, epsilon 0.09143, total time 724, loss -, compute time 13.09\n",
      "episode 2659, reward 1179, memory_length 2000, epsilon 0.09135, total time 730, loss -, compute time 14.04\n",
      "episode 2660, reward 1508, memory_length 2000, epsilon 0.09126, total time 722, loss -, compute time 12.93\n",
      "Saving model for episode: 2660\n",
      "episode 2661, reward 1453, memory_length 2000, epsilon 0.09118, total time 721, loss -, compute time 12.54\n",
      "episode 2662, reward 1539, memory_length 2000, epsilon 0.0911, total time 729, loss -, compute time 10.89\n",
      "episode 2663, reward 1715, memory_length 2000, epsilon 0.09102, total time 728, loss -, compute time 12.27\n",
      "episode 2664, reward 1414, memory_length 2000, epsilon 0.09094, total time 721, loss -, compute time 11.63\n",
      "episode 2665, reward 1516, memory_length 2000, epsilon 0.09085, total time 723, loss -, compute time 13.19\n",
      "episode 2666, reward 1519, memory_length 2000, epsilon 0.09077, total time 724, loss [104.54291534423828], compute time 14.04\n",
      "episode 2667, reward 1464, memory_length 2000, epsilon 0.09069, total time 723, loss [1.3756905794143677], compute time 12.08\n",
      "episode 2668, reward 1414, memory_length 2000, epsilon 0.09061, total time 728, loss -, compute time 13.13\n",
      "episode 2669, reward 1382, memory_length 2000, epsilon 0.09053, total time 725, loss -, compute time 11.79\n",
      "episode 2670, reward 1513, memory_length 2000, epsilon 0.09045, total time 732, loss [0.9808624982833862], compute time 13.44\n",
      "Saving model for episode: 2670\n",
      "episode 2671, reward 1594, memory_length 2000, epsilon 0.09036, total time 724, loss -, compute time 12.23\n",
      "episode 2672, reward 1459, memory_length 2000, epsilon 0.09028, total time 721, loss -, compute time 14.14\n",
      "episode 2673, reward 1532, memory_length 2000, epsilon 0.0902, total time 723, loss -, compute time 12.3\n",
      "episode 2674, reward 1720, memory_length 2000, epsilon 0.09012, total time 726, loss -, compute time 12.7\n",
      "episode 2675, reward 1604, memory_length 2000, epsilon 0.09004, total time 727, loss -, compute time 13.52\n",
      "episode 2676, reward 1519, memory_length 2000, epsilon 0.08996, total time 725, loss -, compute time 14.46\n",
      "episode 2677, reward 1248, memory_length 2000, epsilon 0.08988, total time 723, loss -, compute time 12.29\n",
      "episode 2678, reward 1220, memory_length 2000, epsilon 0.0898, total time 724, loss -, compute time 14.07\n",
      "episode 2679, reward 1516, memory_length 2000, epsilon 0.08972, total time 722, loss [2.448932647705078], compute time 10.91\n",
      "episode 2680, reward 1454, memory_length 2000, epsilon 0.08964, total time 728, loss -, compute time 12.84\n",
      "Saving model for episode: 2680\n",
      "episode 2681, reward 1368, memory_length 2000, epsilon 0.08956, total time 725, loss -, compute time 12.88\n",
      "episode 2682, reward 1622, memory_length 2000, epsilon 0.08947, total time 723, loss [2.3497507572174072], compute time 12.15\n",
      "episode 2683, reward 1675, memory_length 2000, epsilon 0.08939, total time 727, loss -, compute time 12.97\n",
      "episode 2684, reward 1654, memory_length 2000, epsilon 0.08931, total time 724, loss [2.3128576278686523], compute time 12.21\n",
      "episode 2685, reward 1638, memory_length 2000, epsilon 0.08923, total time 730, loss -, compute time 13.31\n",
      "episode 2686, reward 1530, memory_length 2000, epsilon 0.08915, total time 725, loss -, compute time 13.26\n",
      "episode 2687, reward 1675, memory_length 2000, epsilon 0.08907, total time 728, loss -, compute time 13.22\n",
      "episode 2688, reward 1811, memory_length 2000, epsilon 0.08899, total time 721, loss -, compute time 12.07\n",
      "episode 2689, reward 1557, memory_length 2000, epsilon 0.08891, total time 721, loss -, compute time 12.43\n",
      "episode 2690, reward 1359, memory_length 2000, epsilon 0.08883, total time 726, loss -, compute time 10.99\n",
      "Saving model for episode: 2690\n",
      "episode 2691, reward 1401, memory_length 2000, epsilon 0.08875, total time 724, loss -, compute time 12.31\n",
      "episode 2692, reward 1323, memory_length 2000, epsilon 0.08867, total time 722, loss -, compute time 13.15\n",
      "episode 2693, reward 1467, memory_length 2000, epsilon 0.08859, total time 729, loss -, compute time 13.85\n",
      "episode 2694, reward 1663, memory_length 2000, epsilon 0.08851, total time 722, loss -, compute time 11.66\n",
      "episode 2695, reward 1528, memory_length 2000, epsilon 0.08843, total time 721, loss -, compute time 13.73\n",
      "episode 2696, reward 1680, memory_length 2000, epsilon 0.08835, total time 722, loss -, compute time 13.13\n",
      "episode 2697, reward 1269, memory_length 2000, epsilon 0.08827, total time 731, loss -, compute time 11.71\n",
      "episode 2698, reward 1693, memory_length 2000, epsilon 0.0882, total time 728, loss -, compute time 13.28\n",
      "episode 2699, reward 1531, memory_length 2000, epsilon 0.08812, total time 726, loss -, compute time 12.7\n",
      "episode 2700, reward 1557, memory_length 2000, epsilon 0.08804, total time 727, loss [1.779378890991211], compute time 13.89\n",
      "Saving model for episode: 2700\n",
      "episode 2701, reward 1815, memory_length 2000, epsilon 0.08796, total time 721, loss [1.7948064804077148], compute time 12.97\n",
      "episode 2702, reward 1769, memory_length 2000, epsilon 0.08788, total time 724, loss -, compute time 13.46\n",
      "episode 2703, reward 1392, memory_length 2000, epsilon 0.0878, total time 727, loss -, compute time 12.52\n",
      "episode 2704, reward 1532, memory_length 2000, epsilon 0.08772, total time 721, loss -, compute time 12.61\n",
      "episode 2705, reward 1391, memory_length 2000, epsilon 0.08764, total time 722, loss [5.877295970916748], compute time 11.05\n",
      "episode 2706, reward 1533, memory_length 2000, epsilon 0.08756, total time 721, loss -, compute time 12.64\n",
      "episode 2707, reward 1346, memory_length 2000, epsilon 0.08748, total time 730, loss -, compute time 12.17\n",
      "episode 2708, reward 1418, memory_length 2000, epsilon 0.08741, total time 727, loss -, compute time 11.27\n",
      "episode 2709, reward 1711, memory_length 2000, epsilon 0.08733, total time 727, loss -, compute time 12.87\n",
      "episode 2710, reward 1586, memory_length 2000, epsilon 0.08725, total time 727, loss -, compute time 13.22\n",
      "Saving model for episode: 2710\n",
      "episode 2711, reward 1419, memory_length 2000, epsilon 0.08717, total time 721, loss -, compute time 12.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2712, reward 1800, memory_length 2000, epsilon 0.08709, total time 728, loss -, compute time 12.29\n",
      "episode 2713, reward 1495, memory_length 2000, epsilon 0.08701, total time 725, loss -, compute time 11.71\n",
      "episode 2714, reward 1660, memory_length 2000, epsilon 0.08693, total time 721, loss [2.196392059326172], compute time 14.05\n",
      "episode 2715, reward 1688, memory_length 2000, epsilon 0.08686, total time 726, loss -, compute time 10.7\n",
      "episode 2716, reward 1473, memory_length 2000, epsilon 0.08678, total time 725, loss -, compute time 13.17\n",
      "episode 2717, reward 1769, memory_length 2000, epsilon 0.0867, total time 725, loss -, compute time 13.26\n",
      "episode 2718, reward 1507, memory_length 2000, epsilon 0.08662, total time 721, loss -, compute time 13.21\n",
      "episode 2719, reward 1451, memory_length 2000, epsilon 0.08654, total time 724, loss -, compute time 11.0\n",
      "episode 2720, reward 1325, memory_length 2000, epsilon 0.08647, total time 722, loss -, compute time 12.45\n",
      "Saving model for episode: 2720\n",
      "episode 2721, reward 1499, memory_length 2000, epsilon 0.08639, total time 725, loss -, compute time 11.98\n",
      "episode 2722, reward 1386, memory_length 2000, epsilon 0.08631, total time 733, loss -, compute time 11.74\n",
      "episode 2723, reward 1527, memory_length 2000, epsilon 0.08623, total time 726, loss -, compute time 11.49\n",
      "episode 2724, reward 1401, memory_length 2000, epsilon 0.08616, total time 722, loss -, compute time 14.89\n",
      "episode 2725, reward 1399, memory_length 2000, epsilon 0.08608, total time 721, loss -, compute time 13.3\n",
      "episode 2726, reward 1278, memory_length 2000, epsilon 0.086, total time 732, loss -, compute time 14.4\n",
      "episode 2727, reward 1760, memory_length 2000, epsilon 0.08592, total time 725, loss -, compute time 12.79\n",
      "episode 2728, reward 1542, memory_length 2000, epsilon 0.08585, total time 723, loss [1.4858958721160889], compute time 12.9\n",
      "episode 2729, reward 1613, memory_length 2000, epsilon 0.08577, total time 722, loss -, compute time 13.13\n",
      "episode 2730, reward 1364, memory_length 2000, epsilon 0.08569, total time 725, loss -, compute time 13.08\n",
      "Saving model for episode: 2730\n",
      "episode 2731, reward 1426, memory_length 2000, epsilon 0.08561, total time 722, loss -, compute time 12.73\n",
      "episode 2732, reward 1545, memory_length 2000, epsilon 0.08554, total time 723, loss [123.64752960205078], compute time 13.88\n",
      "episode 2733, reward 1368, memory_length 2000, epsilon 0.08546, total time 727, loss -, compute time 12.09\n",
      "episode 2734, reward 1679, memory_length 2000, epsilon 0.08538, total time 721, loss -, compute time 11.58\n",
      "episode 2735, reward 1715, memory_length 2000, epsilon 0.08531, total time 721, loss -, compute time 12.48\n",
      "episode 2736, reward 1503, memory_length 2000, epsilon 0.08523, total time 723, loss -, compute time 12.53\n",
      "episode 2737, reward 1586, memory_length 2000, epsilon 0.08515, total time 724, loss -, compute time 12.97\n",
      "episode 2738, reward 1755, memory_length 2000, epsilon 0.08508, total time 730, loss -, compute time 13.22\n",
      "episode 2739, reward 1512, memory_length 2000, epsilon 0.085, total time 729, loss -, compute time 12.58\n",
      "episode 2740, reward 1576, memory_length 2000, epsilon 0.08492, total time 729, loss -, compute time 12.06\n",
      "Saving model for episode: 2740\n",
      "episode 2741, reward 1648, memory_length 2000, epsilon 0.08485, total time 728, loss -, compute time 12.46\n",
      "episode 2742, reward 1719, memory_length 2000, epsilon 0.08477, total time 726, loss -, compute time 11.82\n",
      "episode 2743, reward 1494, memory_length 2000, epsilon 0.08469, total time 724, loss -, compute time 12.4\n",
      "episode 2744, reward 1688, memory_length 2000, epsilon 0.08462, total time 728, loss -, compute time 13.39\n",
      "episode 2745, reward 1728, memory_length 2000, epsilon 0.08454, total time 729, loss -, compute time 13.38\n",
      "episode 2746, reward 1854, memory_length 2000, epsilon 0.08447, total time 726, loss -, compute time 12.41\n",
      "episode 2747, reward 1476, memory_length 2000, epsilon 0.08439, total time 726, loss -, compute time 13.42\n",
      "episode 2748, reward 1334, memory_length 2000, epsilon 0.08431, total time 727, loss [1.7694635391235352], compute time 12.26\n",
      "episode 2749, reward 1208, memory_length 2000, epsilon 0.08424, total time 726, loss -, compute time 13.51\n",
      "episode 2750, reward 1616, memory_length 2000, epsilon 0.08416, total time 725, loss -, compute time 11.97\n",
      "Saving model for episode: 2750\n",
      "episode 2751, reward 1594, memory_length 2000, epsilon 0.08409, total time 722, loss -, compute time 14.41\n",
      "episode 2752, reward 1463, memory_length 2000, epsilon 0.08401, total time 723, loss -, compute time 11.48\n",
      "episode 2753, reward 1334, memory_length 2000, epsilon 0.08394, total time 724, loss [1.8130254745483398], compute time 12.45\n",
      "episode 2754, reward 1586, memory_length 2000, epsilon 0.08386, total time 724, loss -, compute time 13.85\n",
      "episode 2755, reward 1756, memory_length 2000, epsilon 0.08379, total time 727, loss -, compute time 12.68\n",
      "episode 2756, reward 1532, memory_length 2000, epsilon 0.08371, total time 721, loss -, compute time 11.74\n",
      "episode 2757, reward 1503, memory_length 2000, epsilon 0.08363, total time 725, loss -, compute time 14.26\n",
      "episode 2758, reward 1554, memory_length 2000, epsilon 0.08356, total time 721, loss -, compute time 12.45\n",
      "episode 2759, reward 1413, memory_length 2000, epsilon 0.08348, total time 727, loss -, compute time 13.45\n",
      "episode 2760, reward 1838, memory_length 2000, epsilon 0.08341, total time 727, loss -, compute time 11.03\n",
      "Saving model for episode: 2760\n",
      "episode 2761, reward 1868, memory_length 2000, epsilon 0.08333, total time 725, loss -, compute time 12.2\n",
      "episode 2762, reward 1490, memory_length 2000, epsilon 0.08326, total time 729, loss -, compute time 12.1\n",
      "episode 2763, reward 1302, memory_length 2000, epsilon 0.08318, total time 723, loss -, compute time 11.66\n",
      "episode 2764, reward 1181, memory_length 2000, epsilon 0.08311, total time 723, loss -, compute time 12.23\n",
      "episode 2765, reward 1715, memory_length 2000, epsilon 0.08303, total time 729, loss -, compute time 12.0\n",
      "episode 2766, reward 1847, memory_length 2000, epsilon 0.08296, total time 722, loss -, compute time 13.39\n",
      "episode 2767, reward 1923, memory_length 2000, epsilon 0.08289, total time 725, loss -, compute time 12.34\n",
      "episode 2768, reward 1695, memory_length 2000, epsilon 0.08281, total time 725, loss -, compute time 12.25\n",
      "episode 2769, reward 1507, memory_length 2000, epsilon 0.08274, total time 724, loss -, compute time 12.73\n",
      "episode 2770, reward 1394, memory_length 2000, epsilon 0.08266, total time 721, loss -, compute time 12.82\n",
      "Saving model for episode: 2770\n",
      "episode 2771, reward 1800, memory_length 2000, epsilon 0.08259, total time 728, loss -, compute time 13.28\n",
      "episode 2772, reward 1384, memory_length 2000, epsilon 0.08251, total time 721, loss -, compute time 15.93\n",
      "episode 2773, reward 1379, memory_length 2000, epsilon 0.08244, total time 731, loss [1.7634687423706055], compute time 13.11\n",
      "episode 2774, reward 1301, memory_length 2000, epsilon 0.08236, total time 725, loss -, compute time 13.21\n",
      "episode 2775, reward 1738, memory_length 2000, epsilon 0.08229, total time 724, loss -, compute time 11.69\n",
      "episode 2776, reward 1676, memory_length 2000, epsilon 0.08222, total time 725, loss -, compute time 12.44\n",
      "episode 2777, reward 1764, memory_length 2000, epsilon 0.08214, total time 721, loss -, compute time 13.16\n",
      "episode 2778, reward 1359, memory_length 2000, epsilon 0.08207, total time 726, loss -, compute time 13.06\n",
      "episode 2779, reward 1358, memory_length 2000, epsilon 0.08199, total time 721, loss [1.8295363187789917], compute time 11.39\n",
      "episode 2780, reward 1472, memory_length 2000, epsilon 0.08192, total time 725, loss -, compute time 12.72\n",
      "Saving model for episode: 2780\n",
      "episode 2781, reward 1673, memory_length 2000, epsilon 0.08185, total time 721, loss -, compute time 13.52\n",
      "episode 2782, reward 1737, memory_length 2000, epsilon 0.08177, total time 729, loss -, compute time 12.82\n",
      "episode 2783, reward 1584, memory_length 2000, epsilon 0.0817, total time 729, loss -, compute time 12.52\n",
      "episode 2784, reward 1782, memory_length 2000, epsilon 0.08163, total time 721, loss -, compute time 11.87\n",
      "episode 2785, reward 1524, memory_length 2000, epsilon 0.08155, total time 721, loss -, compute time 11.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2786, reward 1595, memory_length 2000, epsilon 0.08148, total time 721, loss -, compute time 12.55\n",
      "episode 2787, reward 1833, memory_length 2000, epsilon 0.08141, total time 724, loss -, compute time 13.02\n",
      "episode 2788, reward 1346, memory_length 2000, epsilon 0.08133, total time 722, loss -, compute time 12.64\n",
      "episode 2789, reward 1727, memory_length 2000, epsilon 0.08126, total time 723, loss -, compute time 12.28\n",
      "episode 2790, reward 1673, memory_length 2000, epsilon 0.08119, total time 728, loss -, compute time 12.18\n",
      "Saving model for episode: 2790\n",
      "episode 2791, reward 1436, memory_length 2000, epsilon 0.08111, total time 723, loss -, compute time 11.69\n",
      "episode 2792, reward 1548, memory_length 2000, epsilon 0.08104, total time 722, loss -, compute time 12.95\n",
      "episode 2793, reward 1565, memory_length 2000, epsilon 0.08097, total time 721, loss [1.7044737339019775], compute time 13.31\n",
      "episode 2794, reward 1558, memory_length 2000, epsilon 0.0809, total time 725, loss -, compute time 12.12\n",
      "episode 2795, reward 1329, memory_length 2000, epsilon 0.08082, total time 723, loss -, compute time 11.87\n",
      "episode 2796, reward 1755, memory_length 2000, epsilon 0.08075, total time 721, loss -, compute time 12.13\n",
      "episode 2797, reward 1652, memory_length 2000, epsilon 0.08068, total time 722, loss -, compute time 12.77\n",
      "episode 2798, reward 1428, memory_length 2000, epsilon 0.0806, total time 724, loss -, compute time 12.05\n",
      "episode 2799, reward 1673, memory_length 2000, epsilon 0.08053, total time 721, loss -, compute time 12.58\n",
      "episode 2800, reward 1842, memory_length 2000, epsilon 0.08046, total time 730, loss [1.0647368431091309], compute time 12.95\n",
      "Saving model for episode: 2800\n",
      "episode 2801, reward 1629, memory_length 2000, epsilon 0.08039, total time 730, loss -, compute time 13.59\n",
      "episode 2802, reward 1415, memory_length 2000, epsilon 0.08031, total time 727, loss -, compute time 12.75\n",
      "episode 2803, reward 1587, memory_length 2000, epsilon 0.08024, total time 722, loss -, compute time 14.67\n",
      "episode 2804, reward 1531, memory_length 2000, epsilon 0.08017, total time 740, loss -, compute time 13.15\n",
      "episode 2805, reward 1474, memory_length 2000, epsilon 0.0801, total time 726, loss -, compute time 13.64\n",
      "episode 2806, reward 1688, memory_length 2000, epsilon 0.08003, total time 722, loss -, compute time 11.58\n",
      "episode 2807, reward 1368, memory_length 2000, epsilon 0.07995, total time 722, loss -, compute time 13.6\n",
      "episode 2808, reward 1539, memory_length 2000, epsilon 0.07988, total time 721, loss -, compute time 12.98\n",
      "episode 2809, reward 1398, memory_length 2000, epsilon 0.07981, total time 724, loss -, compute time 11.8\n",
      "episode 2810, reward 1187, memory_length 2000, epsilon 0.07974, total time 722, loss -, compute time 11.16\n",
      "Saving model for episode: 2810\n",
      "episode 2811, reward 1449, memory_length 2000, epsilon 0.07967, total time 723, loss -, compute time 11.7\n",
      "episode 2812, reward 1941, memory_length 2000, epsilon 0.0796, total time 726, loss -, compute time 12.65\n",
      "episode 2813, reward 1518, memory_length 2000, epsilon 0.07952, total time 721, loss -, compute time 14.19\n",
      "episode 2814, reward 1670, memory_length 2000, epsilon 0.07945, total time 721, loss -, compute time 12.78\n",
      "episode 2815, reward 1496, memory_length 2000, epsilon 0.07938, total time 722, loss -, compute time 11.76\n",
      "episode 2816, reward 1493, memory_length 2000, epsilon 0.07931, total time 721, loss -, compute time 12.97\n",
      "episode 2817, reward 1280, memory_length 2000, epsilon 0.07924, total time 723, loss -, compute time 12.21\n",
      "episode 2818, reward 1756, memory_length 2000, epsilon 0.07917, total time 725, loss -, compute time 12.06\n",
      "episode 2819, reward 1393, memory_length 2000, epsilon 0.0791, total time 722, loss -, compute time 13.42\n",
      "episode 2820, reward 1431, memory_length 2000, epsilon 0.07902, total time 721, loss -, compute time 12.15\n",
      "Saving model for episode: 2820\n",
      "episode 2821, reward 1536, memory_length 2000, epsilon 0.07895, total time 730, loss -, compute time 12.12\n",
      "episode 2822, reward 1683, memory_length 2000, epsilon 0.07888, total time 724, loss -, compute time 12.45\n",
      "episode 2823, reward 1427, memory_length 2000, epsilon 0.07881, total time 725, loss -, compute time 12.49\n",
      "episode 2824, reward 1518, memory_length 2000, epsilon 0.07874, total time 728, loss -, compute time 12.53\n",
      "episode 2825, reward 1653, memory_length 2000, epsilon 0.07867, total time 725, loss -, compute time 14.07\n",
      "episode 2826, reward 1428, memory_length 2000, epsilon 0.0786, total time 726, loss -, compute time 13.12\n",
      "episode 2827, reward 1684, memory_length 2000, epsilon 0.07853, total time 727, loss -, compute time 13.09\n",
      "episode 2828, reward 1409, memory_length 2000, epsilon 0.07846, total time 725, loss -, compute time 13.52\n",
      "episode 2829, reward 1314, memory_length 2000, epsilon 0.07839, total time 722, loss -, compute time 13.37\n",
      "episode 2830, reward 1472, memory_length 2000, epsilon 0.07832, total time 724, loss -, compute time 12.19\n",
      "Saving model for episode: 2830\n",
      "episode 2831, reward 1924, memory_length 2000, epsilon 0.07825, total time 722, loss -, compute time 11.81\n",
      "episode 2832, reward 1431, memory_length 2000, epsilon 0.07818, total time 726, loss -, compute time 13.16\n",
      "episode 2833, reward 1722, memory_length 2000, epsilon 0.07811, total time 724, loss -, compute time 12.68\n",
      "episode 2834, reward 1466, memory_length 2000, epsilon 0.07803, total time 721, loss -, compute time 13.32\n",
      "episode 2835, reward 1549, memory_length 2000, epsilon 0.07796, total time 729, loss -, compute time 12.81\n",
      "episode 2836, reward 1531, memory_length 2000, epsilon 0.07789, total time 724, loss -, compute time 12.72\n",
      "episode 2837, reward 1631, memory_length 2000, epsilon 0.07782, total time 724, loss -, compute time 12.46\n",
      "episode 2838, reward 1490, memory_length 2000, epsilon 0.07775, total time 729, loss -, compute time 12.81\n",
      "episode 2839, reward 1774, memory_length 2000, epsilon 0.07768, total time 726, loss -, compute time 11.71\n",
      "episode 2840, reward 1797, memory_length 2000, epsilon 0.07761, total time 721, loss -, compute time 13.07\n",
      "Saving model for episode: 2840\n",
      "episode 2841, reward 1551, memory_length 2000, epsilon 0.07754, total time 722, loss -, compute time 12.43\n",
      "episode 2842, reward 1513, memory_length 2000, epsilon 0.07747, total time 721, loss -, compute time 12.98\n",
      "episode 2843, reward 1800, memory_length 2000, epsilon 0.07741, total time 721, loss -, compute time 12.4\n",
      "episode 2844, reward 1595, memory_length 2000, epsilon 0.07734, total time 722, loss -, compute time 11.53\n",
      "episode 2845, reward 1437, memory_length 2000, epsilon 0.07727, total time 727, loss -, compute time 12.64\n",
      "episode 2846, reward 1305, memory_length 2000, epsilon 0.0772, total time 723, loss -, compute time 12.83\n",
      "episode 2847, reward 1464, memory_length 2000, epsilon 0.07713, total time 725, loss -, compute time 13.11\n",
      "episode 2848, reward 1675, memory_length 2000, epsilon 0.07706, total time 727, loss -, compute time 12.65\n",
      "episode 2849, reward 1512, memory_length 2000, epsilon 0.07699, total time 721, loss -, compute time 11.96\n",
      "episode 2850, reward 1662, memory_length 2000, epsilon 0.07692, total time 726, loss -, compute time 12.59\n",
      "Saving model for episode: 2850\n",
      "episode 2851, reward 1657, memory_length 2000, epsilon 0.07685, total time 723, loss -, compute time 11.79\n",
      "episode 2852, reward 1416, memory_length 2000, epsilon 0.07678, total time 721, loss -, compute time 12.77\n",
      "episode 2853, reward 1477, memory_length 2000, epsilon 0.07671, total time 725, loss -, compute time 12.87\n",
      "episode 2854, reward 1503, memory_length 2000, epsilon 0.07664, total time 721, loss -, compute time 11.64\n",
      "episode 2855, reward 1662, memory_length 2000, epsilon 0.07657, total time 726, loss -, compute time 12.04\n",
      "episode 2856, reward 1684, memory_length 2000, epsilon 0.0765, total time 724, loss -, compute time 12.76\n",
      "episode 2857, reward 1121, memory_length 2000, epsilon 0.07644, total time 724, loss -, compute time 11.71\n",
      "episode 2858, reward 1517, memory_length 2000, epsilon 0.07637, total time 723, loss -, compute time 12.79\n",
      "episode 2859, reward 1837, memory_length 2000, epsilon 0.0763, total time 727, loss -, compute time 12.35\n",
      "episode 2860, reward 1607, memory_length 2000, epsilon 0.07623, total time 721, loss -, compute time 13.1\n",
      "Saving model for episode: 2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2861, reward 1419, memory_length 2000, epsilon 0.07616, total time 728, loss -, compute time 12.26\n",
      "episode 2862, reward 1757, memory_length 2000, epsilon 0.07609, total time 722, loss -, compute time 13.1\n",
      "episode 2863, reward 1482, memory_length 2000, epsilon 0.07602, total time 723, loss -, compute time 13.09\n",
      "episode 2864, reward 1649, memory_length 2000, epsilon 0.07596, total time 722, loss -, compute time 13.22\n",
      "episode 2865, reward 1854, memory_length 2000, epsilon 0.07589, total time 723, loss -, compute time 12.02\n",
      "episode 2866, reward 1530, memory_length 2000, epsilon 0.07582, total time 721, loss -, compute time 13.03\n",
      "episode 2867, reward 1513, memory_length 2000, epsilon 0.07575, total time 728, loss -, compute time 11.94\n",
      "episode 2868, reward 1601, memory_length 2000, epsilon 0.07568, total time 722, loss -, compute time 11.76\n",
      "episode 2869, reward 1720, memory_length 2000, epsilon 0.07562, total time 726, loss -, compute time 13.01\n",
      "episode 2870, reward 1796, memory_length 2000, epsilon 0.07555, total time 730, loss -, compute time 13.31\n",
      "Saving model for episode: 2870\n",
      "episode 2871, reward 1484, memory_length 2000, epsilon 0.07548, total time 722, loss -, compute time 14.07\n",
      "episode 2872, reward 1322, memory_length 2000, epsilon 0.07541, total time 721, loss -, compute time 14.27\n",
      "episode 2873, reward 1359, memory_length 2000, epsilon 0.07534, total time 721, loss -, compute time 12.37\n",
      "episode 2874, reward 1591, memory_length 2000, epsilon 0.07528, total time 723, loss -, compute time 12.66\n",
      "episode 2875, reward 1440, memory_length 2000, epsilon 0.07521, total time 728, loss -, compute time 13.83\n",
      "episode 2876, reward 1605, memory_length 2000, epsilon 0.07514, total time 724, loss -, compute time 12.03\n",
      "episode 2877, reward 1746, memory_length 2000, epsilon 0.07507, total time 723, loss -, compute time 12.9\n",
      "episode 2878, reward 1206, memory_length 2000, epsilon 0.07501, total time 721, loss -, compute time 13.37\n",
      "episode 2879, reward 1656, memory_length 2000, epsilon 0.07494, total time 726, loss -, compute time 12.1\n",
      "episode 2880, reward 1553, memory_length 2000, epsilon 0.07487, total time 728, loss -, compute time 13.08\n",
      "Saving model for episode: 2880\n",
      "episode 2881, reward 1549, memory_length 2000, epsilon 0.0748, total time 724, loss -, compute time 12.81\n",
      "episode 2882, reward 1602, memory_length 2000, epsilon 0.07474, total time 723, loss -, compute time 11.82\n",
      "episode 2883, reward 1132, memory_length 2000, epsilon 0.07467, total time 722, loss -, compute time 12.59\n",
      "episode 2884, reward 1686, memory_length 2000, epsilon 0.0746, total time 721, loss -, compute time 12.68\n",
      "episode 2885, reward 1791, memory_length 2000, epsilon 0.07453, total time 725, loss -, compute time 12.53\n",
      "episode 2886, reward 1712, memory_length 2000, epsilon 0.07447, total time 723, loss -, compute time 11.31\n",
      "episode 2887, reward 1449, memory_length 2000, epsilon 0.0744, total time 726, loss -, compute time 13.23\n",
      "episode 2888, reward 1581, memory_length 2000, epsilon 0.07433, total time 721, loss -, compute time 14.08\n",
      "episode 2889, reward 1695, memory_length 2000, epsilon 0.07427, total time 721, loss -, compute time 12.33\n",
      "episode 2890, reward 1800, memory_length 2000, epsilon 0.0742, total time 726, loss -, compute time 11.5\n",
      "Saving model for episode: 2890\n",
      "episode 2891, reward 1458, memory_length 2000, epsilon 0.07413, total time 723, loss -, compute time 12.95\n",
      "episode 2892, reward 1626, memory_length 2000, epsilon 0.07407, total time 725, loss -, compute time 12.36\n",
      "episode 2893, reward 1664, memory_length 2000, epsilon 0.074, total time 722, loss -, compute time 13.59\n",
      "episode 2894, reward 1656, memory_length 2000, epsilon 0.07393, total time 727, loss -, compute time 13.24\n",
      "episode 2895, reward 1529, memory_length 2000, epsilon 0.07387, total time 726, loss -, compute time 14.34\n",
      "episode 2896, reward 1177, memory_length 2000, epsilon 0.0738, total time 725, loss -, compute time 12.32\n",
      "episode 2897, reward 1622, memory_length 2000, epsilon 0.07373, total time 726, loss -, compute time 11.85\n",
      "episode 2898, reward 1577, memory_length 2000, epsilon 0.07367, total time 728, loss -, compute time 12.15\n",
      "episode 2899, reward 1135, memory_length 2000, epsilon 0.0736, total time 726, loss -, compute time 13.89\n",
      "episode 2900, reward 1798, memory_length 2000, epsilon 0.07353, total time 722, loss -, compute time 13.41\n",
      "Saving model for episode: 2900\n",
      "episode 2901, reward 1641, memory_length 2000, epsilon 0.07347, total time 721, loss -, compute time 12.08\n",
      "episode 2902, reward 1683, memory_length 2000, epsilon 0.0734, total time 726, loss -, compute time 11.69\n",
      "episode 2903, reward 1806, memory_length 2000, epsilon 0.07334, total time 724, loss -, compute time 12.59\n",
      "episode 2904, reward 1479, memory_length 2000, epsilon 0.07327, total time 721, loss -, compute time 11.29\n",
      "episode 2905, reward 1478, memory_length 2000, epsilon 0.0732, total time 722, loss -, compute time 12.91\n",
      "episode 2906, reward 1715, memory_length 2000, epsilon 0.07314, total time 730, loss -, compute time 12.3\n",
      "episode 2907, reward 1222, memory_length 2000, epsilon 0.07307, total time 725, loss -, compute time 12.91\n",
      "episode 2908, reward 1295, memory_length 2000, epsilon 0.07301, total time 722, loss -, compute time 12.15\n",
      "episode 2909, reward 1620, memory_length 2000, epsilon 0.07294, total time 727, loss -, compute time 14.26\n",
      "episode 2910, reward 1682, memory_length 2000, epsilon 0.07288, total time 724, loss -, compute time 13.14\n",
      "Saving model for episode: 2910\n",
      "episode 2911, reward 1634, memory_length 2000, epsilon 0.07281, total time 725, loss -, compute time 13.61\n",
      "episode 2912, reward 1618, memory_length 2000, epsilon 0.07274, total time 724, loss -, compute time 12.48\n",
      "episode 2913, reward 1605, memory_length 2000, epsilon 0.07268, total time 724, loss -, compute time 11.29\n",
      "episode 2914, reward 1491, memory_length 2000, epsilon 0.07261, total time 728, loss -, compute time 12.4\n",
      "episode 2915, reward 1595, memory_length 2000, epsilon 0.07255, total time 724, loss -, compute time 12.2\n",
      "episode 2916, reward 1575, memory_length 2000, epsilon 0.07248, total time 726, loss -, compute time 13.11\n",
      "episode 2917, reward 1625, memory_length 2000, epsilon 0.07242, total time 725, loss [1.2067153453826904], compute time 11.63\n",
      "episode 2918, reward 1688, memory_length 2000, epsilon 0.07235, total time 729, loss -, compute time 12.63\n",
      "episode 2919, reward 1453, memory_length 2000, epsilon 0.07229, total time 722, loss -, compute time 13.75\n",
      "episode 2920, reward 1584, memory_length 2000, epsilon 0.07222, total time 721, loss -, compute time 13.27\n",
      "Saving model for episode: 2920\n",
      "episode 2921, reward 1402, memory_length 2000, epsilon 0.07216, total time 722, loss -, compute time 14.18\n",
      "episode 2922, reward 1528, memory_length 2000, epsilon 0.07209, total time 722, loss -, compute time 14.18\n",
      "episode 2923, reward 1553, memory_length 2000, epsilon 0.07203, total time 721, loss -, compute time 12.62\n",
      "episode 2924, reward 1503, memory_length 2000, epsilon 0.07196, total time 721, loss -, compute time 11.84\n",
      "episode 2925, reward 1683, memory_length 2000, epsilon 0.0719, total time 723, loss -, compute time 12.43\n",
      "episode 2926, reward 1545, memory_length 2000, epsilon 0.07183, total time 723, loss -, compute time 11.39\n",
      "episode 2927, reward 1171, memory_length 2000, epsilon 0.07177, total time 726, loss -, compute time 13.58\n",
      "episode 2928, reward 1579, memory_length 2000, epsilon 0.0717, total time 721, loss -, compute time 12.67\n",
      "episode 2929, reward 1604, memory_length 2000, epsilon 0.07164, total time 724, loss -, compute time 11.79\n",
      "episode 2930, reward 1585, memory_length 2000, epsilon 0.07158, total time 721, loss -, compute time 13.14\n",
      "Saving model for episode: 2930\n",
      "episode 2931, reward 1558, memory_length 2000, epsilon 0.07151, total time 728, loss -, compute time 12.07\n",
      "episode 2932, reward 1403, memory_length 2000, epsilon 0.07145, total time 724, loss -, compute time 12.28\n",
      "episode 2933, reward 1476, memory_length 2000, epsilon 0.07138, total time 729, loss -, compute time 12.2\n",
      "episode 2934, reward 1519, memory_length 2000, epsilon 0.07132, total time 721, loss -, compute time 10.74\n",
      "episode 2935, reward 1662, memory_length 2000, epsilon 0.07125, total time 723, loss -, compute time 12.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2936, reward 1335, memory_length 2000, epsilon 0.07119, total time 724, loss -, compute time 12.93\n",
      "episode 2937, reward 1824, memory_length 2000, epsilon 0.07113, total time 724, loss -, compute time 13.65\n",
      "episode 2938, reward 1362, memory_length 2000, epsilon 0.07106, total time 723, loss [0.7559899687767029], compute time 11.34\n",
      "episode 2939, reward 1381, memory_length 2000, epsilon 0.071, total time 722, loss -, compute time 14.6\n",
      "episode 2940, reward 1463, memory_length 2000, epsilon 0.07093, total time 730, loss -, compute time 13.68\n",
      "Saving model for episode: 2940\n",
      "episode 2941, reward 1382, memory_length 2000, epsilon 0.07087, total time 725, loss -, compute time 12.01\n",
      "episode 2942, reward 1728, memory_length 2000, epsilon 0.07081, total time 721, loss -, compute time 12.04\n",
      "episode 2943, reward 1335, memory_length 2000, epsilon 0.07074, total time 722, loss -, compute time 12.38\n",
      "episode 2944, reward 1616, memory_length 2000, epsilon 0.07068, total time 728, loss -, compute time 12.38\n",
      "episode 2945, reward 1308, memory_length 2000, epsilon 0.07062, total time 721, loss -, compute time 12.55\n",
      "episode 2946, reward 1712, memory_length 2000, epsilon 0.07055, total time 721, loss -, compute time 13.31\n",
      "episode 2947, reward 1550, memory_length 2000, epsilon 0.07049, total time 724, loss -, compute time 11.67\n",
      "episode 2948, reward 1670, memory_length 2000, epsilon 0.07043, total time 726, loss -, compute time 12.82\n",
      "episode 2949, reward 1517, memory_length 2000, epsilon 0.07036, total time 721, loss -, compute time 11.42\n",
      "episode 2950, reward 1296, memory_length 2000, epsilon 0.0703, total time 721, loss -, compute time 11.67\n",
      "Saving model for episode: 2950\n",
      "episode 2951, reward 1690, memory_length 2000, epsilon 0.07024, total time 722, loss -, compute time 12.36\n",
      "episode 2952, reward 1504, memory_length 2000, epsilon 0.07017, total time 721, loss -, compute time 13.67\n",
      "episode 2953, reward 1586, memory_length 2000, epsilon 0.07011, total time 722, loss -, compute time 10.79\n",
      "episode 2954, reward 1719, memory_length 2000, epsilon 0.07005, total time 726, loss -, compute time 13.61\n",
      "episode 2955, reward 1413, memory_length 2000, epsilon 0.06998, total time 729, loss -, compute time 11.76\n",
      "episode 2956, reward 1827, memory_length 2000, epsilon 0.06992, total time 722, loss -, compute time 14.14\n",
      "episode 2957, reward 1495, memory_length 2000, epsilon 0.06986, total time 728, loss -, compute time 11.13\n",
      "episode 2958, reward 1542, memory_length 2000, epsilon 0.06979, total time 721, loss -, compute time 13.27\n",
      "episode 2959, reward 1502, memory_length 2000, epsilon 0.06973, total time 722, loss -, compute time 13.55\n",
      "episode 2960, reward 1659, memory_length 2000, epsilon 0.06967, total time 723, loss -, compute time 12.82\n",
      "Saving model for episode: 2960\n",
      "episode 2961, reward 1399, memory_length 2000, epsilon 0.06961, total time 721, loss -, compute time 11.68\n",
      "episode 2962, reward 1374, memory_length 2000, epsilon 0.06954, total time 724, loss -, compute time 12.95\n",
      "episode 2963, reward 1382, memory_length 2000, epsilon 0.06948, total time 722, loss -, compute time 12.56\n",
      "episode 2964, reward 1238, memory_length 2000, epsilon 0.06942, total time 729, loss -, compute time 13.56\n",
      "episode 2965, reward 1472, memory_length 2000, epsilon 0.06936, total time 722, loss -, compute time 11.79\n",
      "episode 2966, reward 1316, memory_length 2000, epsilon 0.06929, total time 724, loss -, compute time 13.0\n",
      "episode 2967, reward 1603, memory_length 2000, epsilon 0.06923, total time 725, loss -, compute time 11.98\n",
      "episode 2968, reward 1581, memory_length 2000, epsilon 0.06917, total time 726, loss -, compute time 12.08\n",
      "episode 2969, reward 1420, memory_length 2000, epsilon 0.06911, total time 725, loss -, compute time 12.11\n",
      "episode 2970, reward 1441, memory_length 2000, epsilon 0.06904, total time 721, loss -, compute time 12.68\n",
      "Saving model for episode: 2970\n",
      "episode 2971, reward 1769, memory_length 2000, epsilon 0.06898, total time 728, loss -, compute time 12.75\n",
      "episode 2972, reward 1609, memory_length 2000, epsilon 0.06892, total time 723, loss -, compute time 12.24\n",
      "episode 2973, reward 1472, memory_length 2000, epsilon 0.06886, total time 725, loss -, compute time 12.4\n",
      "episode 2974, reward 1581, memory_length 2000, epsilon 0.0688, total time 723, loss -, compute time 11.91\n",
      "episode 2975, reward 1555, memory_length 2000, epsilon 0.06873, total time 726, loss -, compute time 13.96\n",
      "episode 2976, reward 1646, memory_length 2000, epsilon 0.06867, total time 722, loss -, compute time 13.92\n",
      "episode 2977, reward 1710, memory_length 2000, epsilon 0.06861, total time 726, loss -, compute time 13.15\n",
      "episode 2978, reward 1828, memory_length 2000, epsilon 0.06855, total time 722, loss -, compute time 11.49\n",
      "episode 2979, reward 1673, memory_length 2000, epsilon 0.06849, total time 724, loss -, compute time 11.57\n",
      "episode 2980, reward 1593, memory_length 2000, epsilon 0.06843, total time 722, loss -, compute time 12.12\n",
      "Saving model for episode: 2980\n",
      "episode 2981, reward 1688, memory_length 2000, epsilon 0.06836, total time 723, loss -, compute time 12.21\n",
      "episode 2982, reward 1499, memory_length 2000, epsilon 0.0683, total time 728, loss -, compute time 13.82\n",
      "episode 2983, reward 1550, memory_length 2000, epsilon 0.06824, total time 728, loss -, compute time 11.75\n",
      "episode 2984, reward 1391, memory_length 2000, epsilon 0.06818, total time 729, loss -, compute time 13.27\n",
      "episode 2985, reward 1675, memory_length 2000, epsilon 0.06812, total time 723, loss -, compute time 11.3\n",
      "episode 2986, reward 1621, memory_length 2000, epsilon 0.06806, total time 721, loss -, compute time 12.76\n",
      "episode 2987, reward 1711, memory_length 2000, epsilon 0.068, total time 722, loss -, compute time 12.09\n",
      "episode 2988, reward 1575, memory_length 2000, epsilon 0.06794, total time 721, loss -, compute time 13.3\n",
      "episode 2989, reward 1765, memory_length 2000, epsilon 0.06787, total time 724, loss -, compute time 11.43\n",
      "episode 2990, reward 1501, memory_length 2000, epsilon 0.06781, total time 722, loss [110.35721588134766], compute time 11.82\n",
      "Saving model for episode: 2990\n",
      "episode 2991, reward 1609, memory_length 2000, epsilon 0.06775, total time 723, loss -, compute time 11.27\n",
      "episode 2992, reward 1301, memory_length 2000, epsilon 0.06769, total time 723, loss -, compute time 12.39\n",
      "episode 2993, reward 1740, memory_length 2000, epsilon 0.06763, total time 725, loss [1.7586097717285156], compute time 11.17\n",
      "episode 2994, reward 1427, memory_length 2000, epsilon 0.06757, total time 722, loss -, compute time 12.79\n",
      "episode 2995, reward 1778, memory_length 2000, epsilon 0.06751, total time 729, loss -, compute time 11.05\n",
      "episode 2996, reward 1694, memory_length 2000, epsilon 0.06745, total time 726, loss -, compute time 12.9\n",
      "episode 2997, reward 1438, memory_length 2000, epsilon 0.06739, total time 721, loss -, compute time 13.18\n",
      "episode 2998, reward 1248, memory_length 2000, epsilon 0.06733, total time 723, loss -, compute time 12.13\n",
      "episode 2999, reward 1661, memory_length 2000, epsilon 0.06727, total time 724, loss -, compute time 13.8\n",
      "episode 3000, reward 1199, memory_length 2000, epsilon 0.06721, total time 724, loss -, compute time 13.36\n",
      "Saving model for episode: 3000\n",
      "episode 3001, reward 1215, memory_length 2000, epsilon 0.06715, total time 722, loss -, compute time 13.36\n",
      "episode 3002, reward 1702, memory_length 2000, epsilon 0.06708, total time 724, loss -, compute time 14.45\n",
      "episode 3003, reward 1583, memory_length 2000, epsilon 0.06702, total time 721, loss -, compute time 12.87\n",
      "episode 3004, reward 1368, memory_length 2000, epsilon 0.06696, total time 730, loss -, compute time 11.49\n",
      "episode 3005, reward 1432, memory_length 2000, epsilon 0.0669, total time 726, loss -, compute time 13.3\n",
      "episode 3006, reward 1560, memory_length 2000, epsilon 0.06684, total time 726, loss -, compute time 14.2\n",
      "episode 3007, reward 1625, memory_length 2000, epsilon 0.06678, total time 725, loss -, compute time 13.29\n",
      "episode 3008, reward 1438, memory_length 2000, epsilon 0.06672, total time 721, loss [122.14866638183594], compute time 12.99\n",
      "episode 3009, reward 1243, memory_length 2000, epsilon 0.06666, total time 722, loss -, compute time 13.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3010, reward 1550, memory_length 2000, epsilon 0.0666, total time 722, loss -, compute time 11.5\n",
      "Saving model for episode: 3010\n",
      "episode 3011, reward 1684, memory_length 2000, epsilon 0.06654, total time 723, loss -, compute time 12.94\n",
      "episode 3012, reward 1629, memory_length 2000, epsilon 0.06648, total time 726, loss -, compute time 10.8\n",
      "episode 3013, reward 1315, memory_length 2000, epsilon 0.06642, total time 722, loss -, compute time 12.74\n",
      "episode 3014, reward 1825, memory_length 2000, epsilon 0.06636, total time 726, loss -, compute time 12.3\n",
      "episode 3015, reward 1192, memory_length 2000, epsilon 0.0663, total time 723, loss -, compute time 13.13\n",
      "episode 3016, reward 1467, memory_length 2000, epsilon 0.06624, total time 728, loss -, compute time 11.87\n",
      "episode 3017, reward 1485, memory_length 2000, epsilon 0.06619, total time 725, loss -, compute time 11.67\n",
      "episode 3018, reward 1413, memory_length 2000, epsilon 0.06613, total time 731, loss -, compute time 12.28\n",
      "episode 3019, reward 1564, memory_length 2000, epsilon 0.06607, total time 725, loss -, compute time 13.33\n",
      "episode 3020, reward 1434, memory_length 2000, epsilon 0.06601, total time 724, loss -, compute time 12.45\n",
      "Saving model for episode: 3020\n",
      "episode 3021, reward 1580, memory_length 2000, epsilon 0.06595, total time 726, loss -, compute time 11.81\n",
      "episode 3022, reward 1494, memory_length 2000, epsilon 0.06589, total time 723, loss -, compute time 10.95\n",
      "episode 3023, reward 1179, memory_length 2000, epsilon 0.06583, total time 723, loss -, compute time 12.45\n",
      "episode 3024, reward 1400, memory_length 2000, epsilon 0.06577, total time 729, loss -, compute time 12.68\n",
      "episode 3025, reward 1688, memory_length 2000, epsilon 0.06571, total time 724, loss -, compute time 12.24\n",
      "episode 3026, reward 1742, memory_length 2000, epsilon 0.06565, total time 722, loss -, compute time 11.96\n",
      "episode 3027, reward 1608, memory_length 2000, epsilon 0.06559, total time 723, loss -, compute time 13.71\n",
      "episode 3028, reward 1620, memory_length 2000, epsilon 0.06553, total time 722, loss -, compute time 12.2\n",
      "episode 3029, reward 1582, memory_length 2000, epsilon 0.06547, total time 725, loss -, compute time 11.73\n",
      "episode 3030, reward 1589, memory_length 2000, epsilon 0.06542, total time 724, loss -, compute time 13.67\n",
      "Saving model for episode: 3030\n",
      "episode 3031, reward 1686, memory_length 2000, epsilon 0.06536, total time 731, loss -, compute time 11.43\n",
      "episode 3032, reward 1672, memory_length 2000, epsilon 0.0653, total time 725, loss -, compute time 12.81\n",
      "episode 3033, reward 1490, memory_length 2000, epsilon 0.06524, total time 726, loss -, compute time 11.61\n",
      "episode 3034, reward 1639, memory_length 2000, epsilon 0.06518, total time 723, loss -, compute time 11.66\n",
      "episode 3035, reward 1357, memory_length 2000, epsilon 0.06512, total time 721, loss -, compute time 13.14\n",
      "episode 3036, reward 1516, memory_length 2000, epsilon 0.06506, total time 721, loss -, compute time 12.66\n",
      "episode 3037, reward 1259, memory_length 2000, epsilon 0.065, total time 725, loss -, compute time 12.66\n",
      "episode 3038, reward 1715, memory_length 2000, epsilon 0.06495, total time 722, loss -, compute time 12.68\n",
      "episode 3039, reward 1588, memory_length 2000, epsilon 0.06489, total time 723, loss -, compute time 12.02\n",
      "episode 3040, reward 1431, memory_length 2000, epsilon 0.06483, total time 731, loss -, compute time 12.67\n",
      "Saving model for episode: 3040\n",
      "episode 3041, reward 1431, memory_length 2000, epsilon 0.06477, total time 725, loss -, compute time 13.34\n",
      "episode 3042, reward 1094, memory_length 2000, epsilon 0.06471, total time 726, loss -, compute time 12.87\n",
      "episode 3043, reward 1635, memory_length 2000, epsilon 0.06465, total time 723, loss -, compute time 12.67\n",
      "episode 3044, reward 1608, memory_length 2000, epsilon 0.0646, total time 724, loss -, compute time 13.37\n",
      "episode 3045, reward 1759, memory_length 2000, epsilon 0.06454, total time 722, loss -, compute time 12.41\n",
      "episode 3046, reward 1590, memory_length 2000, epsilon 0.06448, total time 724, loss -, compute time 13.03\n",
      "episode 3047, reward 1708, memory_length 2000, epsilon 0.06442, total time 724, loss -, compute time 13.61\n",
      "episode 3048, reward 1506, memory_length 2000, epsilon 0.06436, total time 723, loss -, compute time 11.45\n",
      "episode 3049, reward 1354, memory_length 2000, epsilon 0.06431, total time 723, loss -, compute time 15.02\n",
      "episode 3050, reward 1590, memory_length 2000, epsilon 0.06425, total time 723, loss -, compute time 13.14\n",
      "Saving model for episode: 3050\n",
      "episode 3051, reward 1555, memory_length 2000, epsilon 0.06419, total time 723, loss -, compute time 11.94\n",
      "episode 3052, reward 1518, memory_length 2000, epsilon 0.06413, total time 721, loss [0.8331217765808105], compute time 13.01\n",
      "episode 3053, reward 1806, memory_length 2000, epsilon 0.06408, total time 725, loss -, compute time 12.61\n",
      "episode 3054, reward 1422, memory_length 2000, epsilon 0.06402, total time 728, loss -, compute time 12.1\n",
      "episode 3055, reward 1597, memory_length 2000, epsilon 0.06396, total time 723, loss [3.2114057540893555], compute time 12.47\n",
      "episode 3056, reward 1696, memory_length 2000, epsilon 0.0639, total time 724, loss -, compute time 13.21\n",
      "episode 3057, reward 1916, memory_length 2000, epsilon 0.06384, total time 723, loss -, compute time 13.01\n",
      "episode 3058, reward 1362, memory_length 2000, epsilon 0.06379, total time 724, loss -, compute time 12.3\n",
      "episode 3059, reward 1544, memory_length 2000, epsilon 0.06373, total time 721, loss -, compute time 13.04\n",
      "episode 3060, reward 1559, memory_length 2000, epsilon 0.06367, total time 724, loss -, compute time 11.65\n",
      "Saving model for episode: 3060\n",
      "episode 3061, reward 1571, memory_length 2000, epsilon 0.06362, total time 725, loss -, compute time 12.83\n",
      "episode 3062, reward 1544, memory_length 2000, epsilon 0.06356, total time 721, loss -, compute time 13.03\n",
      "episode 3063, reward 1764, memory_length 2000, epsilon 0.0635, total time 730, loss -, compute time 12.34\n",
      "episode 3064, reward 1540, memory_length 2000, epsilon 0.06344, total time 725, loss -, compute time 12.97\n",
      "episode 3065, reward 1717, memory_length 2000, epsilon 0.06339, total time 723, loss -, compute time 10.77\n",
      "episode 3066, reward 1568, memory_length 2000, epsilon 0.06333, total time 723, loss -, compute time 13.2\n",
      "episode 3067, reward 1337, memory_length 2000, epsilon 0.06327, total time 723, loss -, compute time 12.85\n",
      "episode 3068, reward 1629, memory_length 2000, epsilon 0.06322, total time 727, loss -, compute time 12.65\n",
      "episode 3069, reward 1522, memory_length 2000, epsilon 0.06316, total time 726, loss -, compute time 12.17\n",
      "episode 3070, reward 1611, memory_length 2000, epsilon 0.0631, total time 729, loss -, compute time 12.49\n",
      "Saving model for episode: 3070\n",
      "episode 3071, reward 1567, memory_length 2000, epsilon 0.06305, total time 727, loss -, compute time 11.77\n",
      "episode 3072, reward 1181, memory_length 2000, epsilon 0.06299, total time 722, loss -, compute time 12.0\n",
      "episode 3073, reward 1548, memory_length 2000, epsilon 0.06293, total time 724, loss -, compute time 13.11\n",
      "episode 3074, reward 1508, memory_length 2000, epsilon 0.06288, total time 723, loss -, compute time 12.6\n",
      "episode 3075, reward 1820, memory_length 2000, epsilon 0.06282, total time 722, loss -, compute time 12.88\n",
      "episode 3076, reward 1554, memory_length 2000, epsilon 0.06276, total time 731, loss -, compute time 11.99\n",
      "episode 3077, reward 1665, memory_length 2000, epsilon 0.06271, total time 721, loss -, compute time 12.36\n",
      "episode 3078, reward 1701, memory_length 2000, epsilon 0.06265, total time 723, loss -, compute time 11.74\n",
      "episode 3079, reward 1513, memory_length 2000, epsilon 0.06259, total time 728, loss -, compute time 11.53\n",
      "episode 3080, reward 1819, memory_length 2000, epsilon 0.06254, total time 724, loss -, compute time 12.39\n",
      "Saving model for episode: 3080\n",
      "episode 3081, reward 1823, memory_length 2000, epsilon 0.06248, total time 724, loss -, compute time 12.13\n",
      "episode 3082, reward 1478, memory_length 2000, epsilon 0.06242, total time 726, loss -, compute time 12.3\n",
      "episode 3083, reward 1321, memory_length 2000, epsilon 0.06237, total time 721, loss -, compute time 12.44\n",
      "episode 3084, reward 1564, memory_length 2000, epsilon 0.06231, total time 723, loss -, compute time 12.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3085, reward 1682, memory_length 2000, epsilon 0.06226, total time 723, loss -, compute time 12.74\n",
      "episode 3086, reward 1410, memory_length 2000, epsilon 0.0622, total time 721, loss -, compute time 11.08\n",
      "episode 3087, reward 1409, memory_length 2000, epsilon 0.06214, total time 730, loss -, compute time 13.28\n",
      "episode 3088, reward 1663, memory_length 2000, epsilon 0.06209, total time 725, loss -, compute time 12.2\n",
      "episode 3089, reward 1748, memory_length 2000, epsilon 0.06203, total time 724, loss -, compute time 14.47\n",
      "episode 3090, reward 1724, memory_length 2000, epsilon 0.06198, total time 722, loss -, compute time 12.85\n",
      "Saving model for episode: 3090\n",
      "episode 3091, reward 1962, memory_length 2000, epsilon 0.06192, total time 729, loss -, compute time 12.74\n",
      "episode 3092, reward 1619, memory_length 2000, epsilon 0.06187, total time 727, loss -, compute time 11.43\n",
      "episode 3093, reward 1959, memory_length 2000, epsilon 0.06181, total time 722, loss -, compute time 13.89\n",
      "episode 3094, reward 1581, memory_length 2000, epsilon 0.06175, total time 725, loss -, compute time 12.68\n",
      "episode 3095, reward 1493, memory_length 2000, epsilon 0.0617, total time 722, loss -, compute time 12.59\n",
      "episode 3096, reward 1811, memory_length 2000, epsilon 0.06164, total time 724, loss [1.5212931632995605], compute time 14.12\n",
      "episode 3097, reward 1533, memory_length 2000, epsilon 0.06159, total time 721, loss -, compute time 12.93\n",
      "episode 3098, reward 1600, memory_length 2000, epsilon 0.06153, total time 722, loss -, compute time 13.05\n",
      "episode 3099, reward 1588, memory_length 2000, epsilon 0.06148, total time 722, loss -, compute time 11.21\n",
      "episode 3100, reward 1692, memory_length 2000, epsilon 0.06142, total time 724, loss -, compute time 12.07\n",
      "Saving model for episode: 3100\n",
      "episode 3101, reward 1620, memory_length 2000, epsilon 0.06137, total time 721, loss -, compute time 11.69\n",
      "episode 3102, reward 1540, memory_length 2000, epsilon 0.06131, total time 726, loss -, compute time 12.81\n",
      "episode 3103, reward 1651, memory_length 2000, epsilon 0.06126, total time 727, loss -, compute time 11.58\n",
      "episode 3104, reward 1719, memory_length 2000, epsilon 0.0612, total time 721, loss -, compute time 12.89\n",
      "episode 3105, reward 1816, memory_length 2000, epsilon 0.06115, total time 723, loss [1.4760898351669312], compute time 12.24\n",
      "episode 3106, reward 1513, memory_length 2000, epsilon 0.06109, total time 721, loss -, compute time 13.53\n",
      "episode 3107, reward 1924, memory_length 2000, epsilon 0.06104, total time 725, loss -, compute time 12.3\n",
      "episode 3108, reward 1593, memory_length 2000, epsilon 0.06098, total time 721, loss -, compute time 12.26\n",
      "episode 3109, reward 1609, memory_length 2000, epsilon 0.06093, total time 723, loss -, compute time 12.31\n",
      "episode 3110, reward 1683, memory_length 2000, epsilon 0.06087, total time 726, loss -, compute time 12.4\n",
      "Saving model for episode: 3110\n",
      "episode 3111, reward 1887, memory_length 2000, epsilon 0.06082, total time 724, loss -, compute time 11.81\n",
      "episode 3112, reward 1737, memory_length 2000, epsilon 0.06076, total time 727, loss -, compute time 13.26\n",
      "episode 3113, reward 1605, memory_length 2000, epsilon 0.06071, total time 721, loss -, compute time 11.83\n",
      "episode 3114, reward 1715, memory_length 2000, epsilon 0.06065, total time 726, loss -, compute time 12.81\n",
      "episode 3115, reward 1329, memory_length 2000, epsilon 0.0606, total time 726, loss -, compute time 12.62\n",
      "episode 3116, reward 1364, memory_length 2000, epsilon 0.06054, total time 725, loss -, compute time 12.91\n",
      "episode 3117, reward 1324, memory_length 2000, epsilon 0.06049, total time 724, loss [1.4669969081878662], compute time 11.77\n",
      "episode 3118, reward 1353, memory_length 2000, epsilon 0.06043, total time 724, loss -, compute time 13.56\n",
      "episode 3119, reward 1637, memory_length 2000, epsilon 0.06038, total time 723, loss -, compute time 12.14\n",
      "episode 3120, reward 1756, memory_length 2000, epsilon 0.06033, total time 727, loss -, compute time 11.81\n",
      "Saving model for episode: 3120\n",
      "episode 3121, reward 1476, memory_length 2000, epsilon 0.06027, total time 723, loss -, compute time 13.77\n",
      "episode 3122, reward 1751, memory_length 2000, epsilon 0.06022, total time 728, loss -, compute time 11.27\n",
      "episode 3123, reward 1796, memory_length 2000, epsilon 0.06016, total time 724, loss -, compute time 13.47\n",
      "episode 3124, reward 1568, memory_length 2000, epsilon 0.06011, total time 725, loss -, compute time 12.24\n",
      "episode 3125, reward 1388, memory_length 2000, epsilon 0.06005, total time 726, loss -, compute time 13.25\n",
      "episode 3126, reward 1625, memory_length 2000, epsilon 0.06, total time 727, loss -, compute time 13.79\n",
      "episode 3127, reward 1371, memory_length 2000, epsilon 0.05995, total time 723, loss -, compute time 13.15\n",
      "episode 3128, reward 1536, memory_length 2000, epsilon 0.05989, total time 724, loss -, compute time 13.13\n",
      "episode 3129, reward 1532, memory_length 2000, epsilon 0.05984, total time 722, loss -, compute time 13.47\n",
      "episode 3130, reward 1593, memory_length 2000, epsilon 0.05979, total time 721, loss -, compute time 11.76\n",
      "Saving model for episode: 3130\n",
      "episode 3131, reward 1553, memory_length 2000, epsilon 0.05973, total time 721, loss -, compute time 11.84\n",
      "episode 3132, reward 1470, memory_length 2000, epsilon 0.05968, total time 722, loss -, compute time 12.72\n",
      "episode 3133, reward 1827, memory_length 2000, epsilon 0.05962, total time 726, loss -, compute time 12.99\n",
      "episode 3134, reward 1532, memory_length 2000, epsilon 0.05957, total time 722, loss -, compute time 12.75\n",
      "episode 3135, reward 1548, memory_length 2000, epsilon 0.05952, total time 723, loss -, compute time 12.08\n",
      "episode 3136, reward 1611, memory_length 2000, epsilon 0.05946, total time 731, loss -, compute time 12.48\n",
      "episode 3137, reward 1474, memory_length 2000, epsilon 0.05941, total time 721, loss -, compute time 12.59\n",
      "episode 3138, reward 1567, memory_length 2000, epsilon 0.05936, total time 721, loss -, compute time 13.14\n",
      "episode 3139, reward 1509, memory_length 2000, epsilon 0.0593, total time 727, loss -, compute time 12.17\n",
      "episode 3140, reward 1724, memory_length 2000, epsilon 0.05925, total time 731, loss -, compute time 13.87\n",
      "Saving model for episode: 3140\n",
      "episode 3141, reward 1631, memory_length 2000, epsilon 0.0592, total time 725, loss -, compute time 12.33\n",
      "episode 3142, reward 1662, memory_length 2000, epsilon 0.05914, total time 724, loss -, compute time 12.66\n",
      "episode 3143, reward 1534, memory_length 2000, epsilon 0.05909, total time 726, loss -, compute time 12.91\n",
      "episode 3144, reward 1603, memory_length 2000, epsilon 0.05904, total time 723, loss -, compute time 12.33\n",
      "episode 3145, reward 1783, memory_length 2000, epsilon 0.05898, total time 725, loss -, compute time 11.2\n",
      "episode 3146, reward 1296, memory_length 2000, epsilon 0.05893, total time 721, loss -, compute time 11.61\n",
      "episode 3147, reward 1832, memory_length 2000, epsilon 0.05888, total time 721, loss -, compute time 14.22\n",
      "episode 3148, reward 1443, memory_length 2000, epsilon 0.05882, total time 722, loss -, compute time 13.46\n",
      "episode 3149, reward 1566, memory_length 2000, epsilon 0.05877, total time 727, loss -, compute time 12.86\n",
      "episode 3150, reward 1705, memory_length 2000, epsilon 0.05872, total time 721, loss -, compute time 12.71\n",
      "Saving model for episode: 3150\n",
      "episode 3151, reward 1609, memory_length 2000, epsilon 0.05867, total time 728, loss -, compute time 13.53\n",
      "episode 3152, reward 1227, memory_length 2000, epsilon 0.05861, total time 725, loss -, compute time 11.76\n",
      "episode 3153, reward 1526, memory_length 2000, epsilon 0.05856, total time 728, loss -, compute time 13.43\n",
      "episode 3154, reward 1584, memory_length 2000, epsilon 0.05851, total time 724, loss [2.0953569412231445], compute time 12.48\n",
      "episode 3155, reward 1827, memory_length 2000, epsilon 0.05845, total time 722, loss -, compute time 11.7\n",
      "episode 3156, reward 1652, memory_length 2000, epsilon 0.0584, total time 725, loss -, compute time 12.72\n",
      "episode 3157, reward 1611, memory_length 2000, epsilon 0.05835, total time 727, loss -, compute time 12.92\n",
      "episode 3158, reward 1340, memory_length 2000, epsilon 0.0583, total time 723, loss -, compute time 11.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3159, reward 1671, memory_length 2000, epsilon 0.05824, total time 725, loss -, compute time 11.63\n",
      "episode 3160, reward 1572, memory_length 2000, epsilon 0.05819, total time 728, loss -, compute time 13.82\n",
      "Saving model for episode: 3160\n",
      "episode 3161, reward 1495, memory_length 2000, epsilon 0.05814, total time 722, loss -, compute time 13.15\n",
      "episode 3162, reward 1524, memory_length 2000, epsilon 0.05809, total time 725, loss -, compute time 10.86\n",
      "episode 3163, reward 1540, memory_length 2000, epsilon 0.05804, total time 726, loss -, compute time 11.88\n",
      "episode 3164, reward 1199, memory_length 2000, epsilon 0.05798, total time 722, loss -, compute time 13.13\n",
      "episode 3165, reward 1800, memory_length 2000, epsilon 0.05793, total time 726, loss -, compute time 12.15\n",
      "episode 3166, reward 1625, memory_length 2000, epsilon 0.05788, total time 723, loss -, compute time 12.69\n",
      "episode 3167, reward 1655, memory_length 2000, epsilon 0.05783, total time 726, loss [1.8093433380126953], compute time 11.72\n",
      "episode 3168, reward 1240, memory_length 2000, epsilon 0.05777, total time 721, loss -, compute time 13.05\n",
      "episode 3169, reward 1725, memory_length 2000, epsilon 0.05772, total time 723, loss -, compute time 12.83\n",
      "episode 3170, reward 1628, memory_length 2000, epsilon 0.05767, total time 721, loss -, compute time 12.94\n",
      "Saving model for episode: 3170\n",
      "episode 3171, reward 1451, memory_length 2000, epsilon 0.05762, total time 723, loss -, compute time 13.72\n",
      "episode 3172, reward 1635, memory_length 2000, epsilon 0.05757, total time 727, loss -, compute time 13.01\n",
      "episode 3173, reward 1692, memory_length 2000, epsilon 0.05752, total time 725, loss -, compute time 11.34\n",
      "episode 3174, reward 1967, memory_length 2000, epsilon 0.05746, total time 726, loss -, compute time 13.02\n",
      "episode 3175, reward 1720, memory_length 2000, epsilon 0.05741, total time 724, loss -, compute time 13.04\n",
      "episode 3176, reward 1628, memory_length 2000, epsilon 0.05736, total time 721, loss -, compute time 12.68\n",
      "episode 3177, reward 1616, memory_length 2000, epsilon 0.05731, total time 725, loss -, compute time 11.4\n",
      "episode 3178, reward 1575, memory_length 2000, epsilon 0.05726, total time 721, loss -, compute time 12.76\n",
      "episode 3179, reward 1570, memory_length 2000, epsilon 0.05721, total time 724, loss -, compute time 13.83\n",
      "episode 3180, reward 1555, memory_length 2000, epsilon 0.05715, total time 726, loss -, compute time 11.68\n",
      "Saving model for episode: 3180\n",
      "episode 3181, reward 1654, memory_length 2000, epsilon 0.0571, total time 721, loss -, compute time 13.0\n",
      "episode 3182, reward 1670, memory_length 2000, epsilon 0.05705, total time 727, loss -, compute time 12.93\n",
      "episode 3183, reward 1415, memory_length 2000, epsilon 0.057, total time 725, loss -, compute time 11.73\n",
      "episode 3184, reward 1690, memory_length 2000, epsilon 0.05695, total time 724, loss -, compute time 13.19\n",
      "episode 3185, reward 1532, memory_length 2000, epsilon 0.0569, total time 722, loss -, compute time 11.86\n",
      "episode 3186, reward 1476, memory_length 2000, epsilon 0.05685, total time 728, loss -, compute time 12.34\n",
      "episode 3187, reward 1554, memory_length 2000, epsilon 0.0568, total time 722, loss -, compute time 12.85\n",
      "episode 3188, reward 1534, memory_length 2000, epsilon 0.05674, total time 723, loss -, compute time 12.13\n",
      "episode 3189, reward 1662, memory_length 2000, epsilon 0.05669, total time 724, loss -, compute time 12.76\n",
      "episode 3190, reward 1439, memory_length 2000, epsilon 0.05664, total time 721, loss -, compute time 12.24\n",
      "Saving model for episode: 3190\n",
      "episode 3191, reward 1679, memory_length 2000, epsilon 0.05659, total time 728, loss -, compute time 13.07\n",
      "episode 3192, reward 1258, memory_length 2000, epsilon 0.05654, total time 723, loss -, compute time 11.78\n",
      "episode 3193, reward 1605, memory_length 2000, epsilon 0.05649, total time 727, loss -, compute time 12.92\n",
      "episode 3194, reward 1830, memory_length 2000, epsilon 0.05644, total time 721, loss -, compute time 12.68\n",
      "episode 3195, reward 1441, memory_length 2000, epsilon 0.05639, total time 721, loss -, compute time 12.28\n",
      "episode 3196, reward 1704, memory_length 2000, epsilon 0.05634, total time 724, loss -, compute time 12.27\n",
      "episode 3197, reward 1608, memory_length 2000, epsilon 0.05629, total time 722, loss -, compute time 12.88\n",
      "episode 3198, reward 1722, memory_length 2000, epsilon 0.05624, total time 724, loss -, compute time 10.33\n",
      "episode 3199, reward 1558, memory_length 2000, epsilon 0.05619, total time 728, loss -, compute time 14.35\n",
      "episode 3200, reward 1507, memory_length 2000, epsilon 0.05613, total time 722, loss -, compute time 12.78\n",
      "Saving model for episode: 3200\n",
      "episode 3201, reward 1419, memory_length 2000, epsilon 0.05608, total time 723, loss -, compute time 13.64\n",
      "episode 3202, reward 1598, memory_length 2000, epsilon 0.05603, total time 721, loss -, compute time 13.2\n",
      "episode 3203, reward 1446, memory_length 2000, epsilon 0.05598, total time 722, loss -, compute time 11.19\n",
      "episode 3204, reward 1657, memory_length 2000, epsilon 0.05593, total time 726, loss -, compute time 13.2\n",
      "episode 3205, reward 1668, memory_length 2000, epsilon 0.05588, total time 726, loss -, compute time 14.25\n",
      "episode 3206, reward 1604, memory_length 2000, epsilon 0.05583, total time 731, loss -, compute time 13.45\n",
      "episode 3207, reward 1552, memory_length 2000, epsilon 0.05578, total time 721, loss -, compute time 13.43\n",
      "episode 3208, reward 1617, memory_length 2000, epsilon 0.05573, total time 724, loss -, compute time 11.65\n",
      "episode 3209, reward 1562, memory_length 2000, epsilon 0.05568, total time 726, loss -, compute time 14.29\n",
      "episode 3210, reward 1477, memory_length 2000, epsilon 0.05563, total time 724, loss -, compute time 13.93\n",
      "Saving model for episode: 3210\n",
      "episode 3211, reward 1391, memory_length 2000, epsilon 0.05558, total time 722, loss -, compute time 14.34\n",
      "episode 3212, reward 1542, memory_length 2000, epsilon 0.05553, total time 721, loss -, compute time 11.87\n",
      "episode 3213, reward 1436, memory_length 2000, epsilon 0.05548, total time 728, loss -, compute time 12.99\n",
      "episode 3214, reward 1383, memory_length 2000, epsilon 0.05543, total time 722, loss -, compute time 11.87\n",
      "episode 3215, reward 1659, memory_length 2000, epsilon 0.05538, total time 722, loss -, compute time 12.47\n",
      "episode 3216, reward 1515, memory_length 2000, epsilon 0.05533, total time 723, loss -, compute time 13.5\n",
      "episode 3217, reward 1476, memory_length 2000, epsilon 0.05528, total time 727, loss -, compute time 13.68\n",
      "episode 3218, reward 1631, memory_length 2000, epsilon 0.05523, total time 724, loss -, compute time 13.15\n",
      "episode 3219, reward 1424, memory_length 2000, epsilon 0.05518, total time 725, loss -, compute time 13.79\n",
      "episode 3220, reward 1585, memory_length 2000, epsilon 0.05513, total time 724, loss -, compute time 11.93\n",
      "Saving model for episode: 3220\n",
      "episode 3221, reward 1677, memory_length 2000, epsilon 0.05508, total time 724, loss -, compute time 12.55\n",
      "episode 3222, reward 1405, memory_length 2000, epsilon 0.05503, total time 723, loss -, compute time 12.59\n",
      "episode 3223, reward 1520, memory_length 2000, epsilon 0.05498, total time 723, loss -, compute time 12.73\n",
      "episode 3224, reward 1413, memory_length 2000, epsilon 0.05494, total time 722, loss -, compute time 12.38\n",
      "episode 3225, reward 1545, memory_length 2000, epsilon 0.05489, total time 731, loss [146.39743041992188], compute time 13.78\n",
      "episode 3226, reward 1535, memory_length 2000, epsilon 0.05484, total time 724, loss -, compute time 13.24\n",
      "episode 3227, reward 1583, memory_length 2000, epsilon 0.05479, total time 723, loss -, compute time 12.74\n",
      "episode 3228, reward 1420, memory_length 2000, epsilon 0.05474, total time 724, loss -, compute time 13.34\n",
      "episode 3229, reward 1629, memory_length 2000, epsilon 0.05469, total time 728, loss -, compute time 12.94\n",
      "episode 3230, reward 1455, memory_length 2000, epsilon 0.05464, total time 725, loss -, compute time 13.29\n",
      "Saving model for episode: 3230\n",
      "episode 3231, reward 1869, memory_length 2000, epsilon 0.05459, total time 726, loss -, compute time 13.71\n",
      "episode 3232, reward 1547, memory_length 2000, epsilon 0.05454, total time 721, loss -, compute time 12.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3233, reward 1532, memory_length 2000, epsilon 0.05449, total time 726, loss -, compute time 12.56\n",
      "episode 3234, reward 1621, memory_length 2000, epsilon 0.05444, total time 727, loss -, compute time 12.92\n",
      "episode 3235, reward 1666, memory_length 2000, epsilon 0.05439, total time 727, loss -, compute time 12.71\n",
      "episode 3236, reward 1342, memory_length 2000, epsilon 0.05435, total time 726, loss -, compute time 11.83\n",
      "episode 3237, reward 1438, memory_length 2000, epsilon 0.0543, total time 721, loss [2.1110193729400635], compute time 13.07\n",
      "episode 3238, reward 1620, memory_length 2000, epsilon 0.05425, total time 721, loss -, compute time 12.7\n",
      "episode 3239, reward 1366, memory_length 2000, epsilon 0.0542, total time 726, loss -, compute time 12.47\n",
      "episode 3240, reward 1521, memory_length 2000, epsilon 0.05415, total time 721, loss -, compute time 12.21\n",
      "Saving model for episode: 3240\n",
      "episode 3241, reward 1614, memory_length 2000, epsilon 0.0541, total time 725, loss -, compute time 12.91\n",
      "episode 3242, reward 1364, memory_length 2000, epsilon 0.05405, total time 722, loss -, compute time 11.87\n",
      "episode 3243, reward 1467, memory_length 2000, epsilon 0.054, total time 724, loss -, compute time 12.26\n",
      "episode 3244, reward 1684, memory_length 2000, epsilon 0.05396, total time 729, loss -, compute time 11.93\n",
      "episode 3245, reward 1680, memory_length 2000, epsilon 0.05391, total time 725, loss -, compute time 13.24\n",
      "episode 3246, reward 1476, memory_length 2000, epsilon 0.05386, total time 732, loss -, compute time 14.39\n",
      "episode 3247, reward 1652, memory_length 2000, epsilon 0.05381, total time 736, loss -, compute time 12.12\n",
      "episode 3248, reward 1287, memory_length 2000, epsilon 0.05376, total time 731, loss -, compute time 11.69\n",
      "episode 3249, reward 1504, memory_length 2000, epsilon 0.05371, total time 721, loss -, compute time 13.28\n",
      "episode 3250, reward 1970, memory_length 2000, epsilon 0.05366, total time 721, loss -, compute time 11.52\n",
      "Saving model for episode: 3250\n",
      "episode 3251, reward 1835, memory_length 2000, epsilon 0.05362, total time 724, loss -, compute time 11.53\n",
      "episode 3252, reward 1710, memory_length 2000, epsilon 0.05357, total time 721, loss -, compute time 12.03\n",
      "episode 3253, reward 1370, memory_length 2000, epsilon 0.05352, total time 729, loss -, compute time 12.36\n",
      "episode 3254, reward 1892, memory_length 2000, epsilon 0.05347, total time 723, loss -, compute time 13.29\n",
      "episode 3255, reward 1787, memory_length 2000, epsilon 0.05342, total time 725, loss -, compute time 11.49\n",
      "episode 3256, reward 1353, memory_length 2000, epsilon 0.05338, total time 721, loss -, compute time 11.77\n",
      "episode 3257, reward 1670, memory_length 2000, epsilon 0.05333, total time 725, loss -, compute time 12.87\n",
      "episode 3258, reward 1895, memory_length 2000, epsilon 0.05328, total time 722, loss -, compute time 13.91\n",
      "episode 3259, reward 1541, memory_length 2000, epsilon 0.05323, total time 725, loss -, compute time 13.49\n",
      "episode 3260, reward 1620, memory_length 2000, epsilon 0.05318, total time 725, loss -, compute time 10.99\n",
      "Saving model for episode: 3260\n",
      "episode 3261, reward 1312, memory_length 2000, epsilon 0.05314, total time 724, loss -, compute time 12.36\n",
      "episode 3262, reward 1235, memory_length 2000, epsilon 0.05309, total time 724, loss -, compute time 12.27\n",
      "episode 3263, reward 1630, memory_length 2000, epsilon 0.05304, total time 729, loss -, compute time 13.18\n",
      "episode 3264, reward 1832, memory_length 2000, epsilon 0.05299, total time 722, loss -, compute time 13.39\n",
      "episode 3265, reward 1810, memory_length 2000, epsilon 0.05295, total time 725, loss -, compute time 12.64\n",
      "episode 3266, reward 1660, memory_length 2000, epsilon 0.0529, total time 723, loss -, compute time 12.73\n",
      "episode 3267, reward 1575, memory_length 2000, epsilon 0.05285, total time 725, loss -, compute time 13.75\n",
      "episode 3268, reward 1665, memory_length 2000, epsilon 0.0528, total time 723, loss -, compute time 12.77\n",
      "episode 3269, reward 1661, memory_length 2000, epsilon 0.05275, total time 721, loss -, compute time 12.79\n",
      "episode 3270, reward 1703, memory_length 2000, epsilon 0.05271, total time 721, loss -, compute time 13.81\n",
      "Saving model for episode: 3270\n",
      "episode 3271, reward 1715, memory_length 2000, epsilon 0.05266, total time 727, loss -, compute time 13.8\n",
      "episode 3272, reward 1652, memory_length 2000, epsilon 0.05261, total time 722, loss -, compute time 12.79\n",
      "episode 3273, reward 1647, memory_length 2000, epsilon 0.05257, total time 723, loss -, compute time 14.06\n",
      "episode 3274, reward 1630, memory_length 2000, epsilon 0.05252, total time 727, loss -, compute time 12.56\n",
      "episode 3275, reward 1722, memory_length 2000, epsilon 0.05247, total time 722, loss -, compute time 12.66\n",
      "episode 3276, reward 1599, memory_length 2000, epsilon 0.05242, total time 724, loss -, compute time 12.8\n",
      "episode 3277, reward 1628, memory_length 2000, epsilon 0.05238, total time 724, loss -, compute time 12.87\n",
      "episode 3278, reward 1474, memory_length 2000, epsilon 0.05233, total time 722, loss -, compute time 13.11\n",
      "episode 3279, reward 1547, memory_length 2000, epsilon 0.05228, total time 722, loss -, compute time 11.58\n",
      "episode 3280, reward 1682, memory_length 2000, epsilon 0.05224, total time 723, loss -, compute time 13.42\n",
      "Saving model for episode: 3280\n",
      "episode 3281, reward 1612, memory_length 2000, epsilon 0.05219, total time 727, loss -, compute time 14.23\n",
      "episode 3282, reward 1614, memory_length 2000, epsilon 0.05214, total time 723, loss -, compute time 14.63\n",
      "episode 3283, reward 1652, memory_length 2000, epsilon 0.05209, total time 725, loss [1.554602861404419], compute time 12.76\n",
      "episode 3284, reward 1433, memory_length 2000, epsilon 0.05205, total time 726, loss -, compute time 13.03\n",
      "episode 3285, reward 1656, memory_length 2000, epsilon 0.052, total time 727, loss -, compute time 12.83\n",
      "episode 3286, reward 1582, memory_length 2000, epsilon 0.05195, total time 721, loss [1.2544251680374146], compute time 13.15\n",
      "episode 3287, reward 1344, memory_length 2000, epsilon 0.05191, total time 724, loss -, compute time 13.44\n",
      "episode 3288, reward 1256, memory_length 2000, epsilon 0.05186, total time 727, loss -, compute time 14.28\n",
      "episode 3289, reward 1305, memory_length 2000, epsilon 0.05181, total time 725, loss -, compute time 14.61\n",
      "episode 3290, reward 1455, memory_length 2000, epsilon 0.05177, total time 726, loss -, compute time 12.47\n",
      "Saving model for episode: 3290\n",
      "episode 3291, reward 1575, memory_length 2000, epsilon 0.05172, total time 721, loss -, compute time 12.32\n",
      "episode 3292, reward 1404, memory_length 2000, epsilon 0.05167, total time 721, loss -, compute time 13.23\n",
      "episode 3293, reward 1743, memory_length 2000, epsilon 0.05163, total time 727, loss -, compute time 14.99\n",
      "episode 3294, reward 1500, memory_length 2000, epsilon 0.05158, total time 724, loss -, compute time 12.21\n",
      "episode 3295, reward 1706, memory_length 2000, epsilon 0.05153, total time 730, loss -, compute time 12.28\n",
      "episode 3296, reward 1522, memory_length 2000, epsilon 0.05149, total time 728, loss -, compute time 13.1\n",
      "episode 3297, reward 1841, memory_length 2000, epsilon 0.05144, total time 724, loss -, compute time 12.43\n",
      "episode 3298, reward 1686, memory_length 2000, epsilon 0.0514, total time 725, loss -, compute time 12.03\n",
      "episode 3299, reward 1833, memory_length 2000, epsilon 0.05135, total time 725, loss -, compute time 13.91\n",
      "episode 3300, reward 1595, memory_length 2000, epsilon 0.0513, total time 722, loss -, compute time 11.51\n",
      "Saving model for episode: 3300\n",
      "episode 3301, reward 1800, memory_length 2000, epsilon 0.05126, total time 721, loss -, compute time 14.24\n",
      "episode 3302, reward 1636, memory_length 2000, epsilon 0.05121, total time 721, loss -, compute time 12.89\n",
      "episode 3303, reward 1454, memory_length 2000, epsilon 0.05116, total time 726, loss [1.6398377418518066], compute time 13.67\n",
      "episode 3304, reward 1656, memory_length 2000, epsilon 0.05112, total time 725, loss -, compute time 12.54\n",
      "episode 3305, reward 1604, memory_length 2000, epsilon 0.05107, total time 724, loss -, compute time 13.36\n",
      "episode 3306, reward 1385, memory_length 2000, epsilon 0.05103, total time 722, loss -, compute time 14.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3307, reward 1950, memory_length 2000, epsilon 0.05098, total time 722, loss -, compute time 12.58\n",
      "episode 3308, reward 1190, memory_length 2000, epsilon 0.05094, total time 727, loss -, compute time 12.3\n",
      "episode 3309, reward 1512, memory_length 2000, epsilon 0.05089, total time 723, loss -, compute time 12.7\n",
      "episode 3310, reward 1656, memory_length 2000, epsilon 0.05084, total time 721, loss -, compute time 12.43\n",
      "Saving model for episode: 3310\n",
      "episode 3311, reward 1536, memory_length 2000, epsilon 0.0508, total time 728, loss -, compute time 13.11\n",
      "episode 3312, reward 1593, memory_length 2000, epsilon 0.05075, total time 726, loss -, compute time 12.41\n",
      "episode 3313, reward 1474, memory_length 2000, epsilon 0.05071, total time 727, loss -, compute time 12.56\n",
      "episode 3314, reward 1612, memory_length 2000, epsilon 0.05066, total time 724, loss -, compute time 13.93\n",
      "episode 3315, reward 1352, memory_length 2000, epsilon 0.05062, total time 722, loss -, compute time 11.35\n",
      "episode 3316, reward 1688, memory_length 2000, epsilon 0.05057, total time 724, loss -, compute time 14.16\n",
      "episode 3317, reward 1699, memory_length 2000, epsilon 0.05052, total time 723, loss -, compute time 13.99\n",
      "episode 3318, reward 1338, memory_length 2000, epsilon 0.05048, total time 730, loss -, compute time 14.09\n",
      "episode 3319, reward 1521, memory_length 2000, epsilon 0.05043, total time 721, loss -, compute time 13.5\n",
      "episode 3320, reward 1755, memory_length 2000, epsilon 0.05039, total time 724, loss -, compute time 11.57\n",
      "Saving model for episode: 3320\n",
      "episode 3321, reward 1741, memory_length 2000, epsilon 0.05034, total time 726, loss -, compute time 12.16\n",
      "episode 3322, reward 1422, memory_length 2000, epsilon 0.0503, total time 721, loss -, compute time 14.13\n",
      "episode 3323, reward 1927, memory_length 2000, epsilon 0.05025, total time 724, loss -, compute time 13.12\n",
      "episode 3324, reward 1658, memory_length 2000, epsilon 0.05021, total time 721, loss -, compute time 13.94\n",
      "episode 3325, reward 1337, memory_length 2000, epsilon 0.05016, total time 725, loss -, compute time 12.84\n",
      "episode 3326, reward 1499, memory_length 2000, epsilon 0.05012, total time 721, loss -, compute time 14.13\n",
      "episode 3327, reward 1598, memory_length 2000, epsilon 0.05007, total time 727, loss -, compute time 11.95\n",
      "episode 3328, reward 1582, memory_length 2000, epsilon 0.05003, total time 728, loss -, compute time 12.2\n",
      "episode 3329, reward 1809, memory_length 2000, epsilon 0.04998, total time 730, loss -, compute time 12.9\n",
      "episode 3330, reward 1505, memory_length 2000, epsilon 0.04994, total time 722, loss -, compute time 13.11\n",
      "Saving model for episode: 3330\n",
      "episode 3331, reward 1672, memory_length 2000, epsilon 0.04989, total time 728, loss -, compute time 12.77\n",
      "episode 3332, reward 1594, memory_length 2000, epsilon 0.04985, total time 724, loss -, compute time 13.95\n",
      "episode 3333, reward 1587, memory_length 2000, epsilon 0.0498, total time 728, loss -, compute time 14.17\n",
      "episode 3334, reward 1540, memory_length 2000, epsilon 0.04976, total time 726, loss -, compute time 13.4\n",
      "episode 3335, reward 1818, memory_length 2000, epsilon 0.04971, total time 721, loss -, compute time 14.67\n",
      "episode 3336, reward 1495, memory_length 2000, epsilon 0.04967, total time 727, loss -, compute time 14.17\n",
      "episode 3337, reward 1531, memory_length 2000, epsilon 0.04962, total time 722, loss -, compute time 12.71\n",
      "episode 3338, reward 1577, memory_length 2000, epsilon 0.04958, total time 722, loss -, compute time 12.03\n",
      "episode 3339, reward 1622, memory_length 2000, epsilon 0.04953, total time 721, loss -, compute time 12.96\n",
      "episode 3340, reward 1494, memory_length 2000, epsilon 0.04949, total time 721, loss -, compute time 13.47\n",
      "Saving model for episode: 3340\n",
      "episode 3341, reward 1349, memory_length 2000, epsilon 0.04944, total time 721, loss -, compute time 12.92\n",
      "episode 3342, reward 1708, memory_length 2000, epsilon 0.0494, total time 725, loss -, compute time 12.73\n",
      "episode 3343, reward 1584, memory_length 2000, epsilon 0.04936, total time 722, loss -, compute time 12.99\n",
      "episode 3344, reward 1411, memory_length 2000, epsilon 0.04931, total time 726, loss -, compute time 11.63\n",
      "episode 3345, reward 1643, memory_length 2000, epsilon 0.04927, total time 727, loss -, compute time 13.29\n",
      "episode 3346, reward 1404, memory_length 2000, epsilon 0.04922, total time 729, loss -, compute time 13.41\n",
      "episode 3347, reward 1658, memory_length 2000, epsilon 0.04918, total time 721, loss -, compute time 13.3\n",
      "episode 3348, reward 1390, memory_length 2000, epsilon 0.04913, total time 723, loss -, compute time 14.25\n",
      "episode 3349, reward 1537, memory_length 2000, epsilon 0.04909, total time 721, loss -, compute time 13.08\n",
      "episode 3350, reward 1662, memory_length 2000, epsilon 0.04905, total time 723, loss -, compute time 13.47\n",
      "Saving model for episode: 3350\n",
      "episode 3351, reward 1766, memory_length 2000, epsilon 0.049, total time 721, loss -, compute time 12.67\n",
      "episode 3352, reward 1720, memory_length 2000, epsilon 0.04896, total time 724, loss -, compute time 11.73\n",
      "episode 3353, reward 1719, memory_length 2000, epsilon 0.04891, total time 728, loss [1.1413414478302002], compute time 11.75\n",
      "episode 3354, reward 1630, memory_length 2000, epsilon 0.04887, total time 724, loss -, compute time 12.72\n",
      "episode 3355, reward 1413, memory_length 2000, epsilon 0.04883, total time 731, loss -, compute time 13.5\n",
      "episode 3356, reward 1331, memory_length 2000, epsilon 0.04878, total time 723, loss -, compute time 12.19\n",
      "episode 3357, reward 1580, memory_length 2000, epsilon 0.04874, total time 722, loss -, compute time 12.38\n",
      "episode 3358, reward 1527, memory_length 2000, epsilon 0.04869, total time 727, loss -, compute time 12.51\n",
      "episode 3359, reward 1634, memory_length 2000, epsilon 0.04865, total time 730, loss -, compute time 13.4\n",
      "episode 3360, reward 1638, memory_length 2000, epsilon 0.04861, total time 723, loss -, compute time 12.88\n",
      "Saving model for episode: 3360\n",
      "episode 3361, reward 1652, memory_length 2000, epsilon 0.04856, total time 731, loss -, compute time 13.09\n",
      "episode 3362, reward 1272, memory_length 2000, epsilon 0.04852, total time 724, loss -, compute time 12.17\n",
      "episode 3363, reward 1546, memory_length 2000, epsilon 0.04848, total time 724, loss -, compute time 13.44\n",
      "episode 3364, reward 1711, memory_length 2000, epsilon 0.04843, total time 721, loss -, compute time 13.0\n",
      "episode 3365, reward 1752, memory_length 2000, epsilon 0.04839, total time 721, loss -, compute time 13.48\n",
      "episode 3366, reward 1627, memory_length 2000, epsilon 0.04834, total time 721, loss -, compute time 11.9\n",
      "episode 3367, reward 1337, memory_length 2000, epsilon 0.0483, total time 728, loss -, compute time 12.33\n",
      "episode 3368, reward 1816, memory_length 2000, epsilon 0.04826, total time 721, loss -, compute time 11.82\n",
      "episode 3369, reward 1881, memory_length 2000, epsilon 0.04821, total time 726, loss -, compute time 15.13\n",
      "episode 3370, reward 1548, memory_length 2000, epsilon 0.04817, total time 721, loss -, compute time 12.44\n",
      "Saving model for episode: 3370\n",
      "episode 3371, reward 1518, memory_length 2000, epsilon 0.04813, total time 727, loss -, compute time 11.73\n",
      "episode 3372, reward 1869, memory_length 2000, epsilon 0.04808, total time 724, loss -, compute time 12.71\n",
      "episode 3373, reward 1635, memory_length 2000, epsilon 0.04804, total time 728, loss -, compute time 11.57\n",
      "episode 3374, reward 1747, memory_length 2000, epsilon 0.048, total time 729, loss -, compute time 12.65\n",
      "episode 3375, reward 1531, memory_length 2000, epsilon 0.04795, total time 735, loss -, compute time 12.49\n",
      "episode 3376, reward 1850, memory_length 2000, epsilon 0.04791, total time 726, loss -, compute time 12.1\n",
      "episode 3377, reward 1665, memory_length 2000, epsilon 0.04787, total time 721, loss -, compute time 13.34\n",
      "episode 3378, reward 1585, memory_length 2000, epsilon 0.04783, total time 725, loss -, compute time 13.21\n",
      "episode 3379, reward 1709, memory_length 2000, epsilon 0.04778, total time 723, loss -, compute time 13.16\n",
      "episode 3380, reward 1637, memory_length 2000, epsilon 0.04774, total time 723, loss -, compute time 10.59\n",
      "Saving model for episode: 3380\n",
      "episode 3381, reward 1251, memory_length 2000, epsilon 0.0477, total time 729, loss -, compute time 13.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3382, reward 1476, memory_length 2000, epsilon 0.04765, total time 727, loss -, compute time 11.63\n",
      "episode 3383, reward 1499, memory_length 2000, epsilon 0.04761, total time 724, loss -, compute time 12.96\n",
      "episode 3384, reward 1733, memory_length 2000, epsilon 0.04757, total time 727, loss [2.2563023567199707], compute time 12.64\n",
      "episode 3385, reward 1461, memory_length 2000, epsilon 0.04752, total time 721, loss -, compute time 11.43\n",
      "episode 3386, reward 1468, memory_length 2000, epsilon 0.04748, total time 724, loss -, compute time 12.31\n",
      "episode 3387, reward 1632, memory_length 2000, epsilon 0.04744, total time 722, loss -, compute time 12.03\n",
      "episode 3388, reward 1715, memory_length 2000, epsilon 0.0474, total time 728, loss -, compute time 14.04\n",
      "episode 3389, reward 1257, memory_length 2000, epsilon 0.04735, total time 725, loss -, compute time 12.97\n",
      "episode 3390, reward 1374, memory_length 2000, epsilon 0.04731, total time 729, loss -, compute time 13.33\n",
      "Saving model for episode: 3390\n",
      "episode 3391, reward 1660, memory_length 2000, epsilon 0.04727, total time 725, loss -, compute time 11.65\n",
      "episode 3392, reward 1809, memory_length 2000, epsilon 0.04723, total time 729, loss -, compute time 12.29\n",
      "episode 3393, reward 1769, memory_length 2000, epsilon 0.04718, total time 726, loss -, compute time 13.38\n",
      "episode 3394, reward 1648, memory_length 2000, epsilon 0.04714, total time 728, loss -, compute time 12.25\n",
      "episode 3395, reward 1863, memory_length 2000, epsilon 0.0471, total time 723, loss -, compute time 12.6\n",
      "episode 3396, reward 1735, memory_length 2000, epsilon 0.04706, total time 722, loss -, compute time 13.52\n",
      "episode 3397, reward 1813, memory_length 2000, epsilon 0.04701, total time 724, loss -, compute time 12.75\n",
      "episode 3398, reward 1664, memory_length 2000, epsilon 0.04697, total time 731, loss [0.8723055720329285], compute time 13.54\n",
      "episode 3399, reward 1324, memory_length 2000, epsilon 0.04693, total time 731, loss -, compute time 12.6\n",
      "episode 3400, reward 1662, memory_length 2000, epsilon 0.04689, total time 732, loss -, compute time 12.83\n",
      "Saving model for episode: 3400\n",
      "episode 3401, reward 1917, memory_length 2000, epsilon 0.04685, total time 721, loss -, compute time 12.6\n",
      "episode 3402, reward 1775, memory_length 2000, epsilon 0.0468, total time 725, loss -, compute time 13.18\n",
      "episode 3403, reward 1658, memory_length 2000, epsilon 0.04676, total time 725, loss -, compute time 13.46\n",
      "episode 3404, reward 1693, memory_length 2000, epsilon 0.04672, total time 725, loss -, compute time 12.73\n",
      "episode 3405, reward 1738, memory_length 2000, epsilon 0.04668, total time 721, loss -, compute time 11.22\n",
      "episode 3406, reward 1503, memory_length 2000, epsilon 0.04664, total time 726, loss [1.2015094757080078], compute time 13.06\n",
      "episode 3407, reward 1674, memory_length 2000, epsilon 0.04659, total time 728, loss -, compute time 12.61\n",
      "episode 3408, reward 1362, memory_length 2000, epsilon 0.04655, total time 729, loss -, compute time 12.32\n",
      "episode 3409, reward 1728, memory_length 2000, epsilon 0.04651, total time 721, loss -, compute time 14.22\n",
      "episode 3410, reward 1539, memory_length 2000, epsilon 0.04647, total time 722, loss -, compute time 13.13\n",
      "Saving model for episode: 3410\n",
      "episode 3411, reward 1844, memory_length 2000, epsilon 0.04643, total time 722, loss -, compute time 12.64\n",
      "episode 3412, reward 1584, memory_length 2000, epsilon 0.04638, total time 721, loss -, compute time 13.54\n",
      "episode 3413, reward 1689, memory_length 2000, epsilon 0.04634, total time 721, loss -, compute time 11.67\n",
      "episode 3414, reward 1622, memory_length 2000, epsilon 0.0463, total time 726, loss -, compute time 13.4\n",
      "episode 3415, reward 1414, memory_length 2000, epsilon 0.04626, total time 725, loss -, compute time 13.73\n",
      "episode 3416, reward 1694, memory_length 2000, epsilon 0.04622, total time 725, loss -, compute time 14.85\n",
      "episode 3417, reward 1719, memory_length 2000, epsilon 0.04618, total time 726, loss -, compute time 12.72\n",
      "episode 3418, reward 1247, memory_length 2000, epsilon 0.04613, total time 721, loss -, compute time 13.02\n",
      "episode 3419, reward 1598, memory_length 2000, epsilon 0.04609, total time 730, loss -, compute time 12.59\n",
      "episode 3420, reward 1663, memory_length 2000, epsilon 0.04605, total time 725, loss -, compute time 12.68\n",
      "Saving model for episode: 3420\n",
      "episode 3421, reward 1652, memory_length 2000, epsilon 0.04601, total time 729, loss -, compute time 12.39\n",
      "episode 3422, reward 1609, memory_length 2000, epsilon 0.04597, total time 727, loss -, compute time 12.21\n",
      "episode 3423, reward 1692, memory_length 2000, epsilon 0.04593, total time 721, loss -, compute time 13.16\n",
      "episode 3424, reward 1427, memory_length 2000, epsilon 0.04589, total time 728, loss -, compute time 12.49\n",
      "episode 3425, reward 1792, memory_length 2000, epsilon 0.04584, total time 725, loss -, compute time 11.09\n",
      "episode 3426, reward 1375, memory_length 2000, epsilon 0.0458, total time 722, loss -, compute time 13.39\n",
      "episode 3427, reward 1728, memory_length 2000, epsilon 0.04576, total time 721, loss -, compute time 13.74\n",
      "episode 3428, reward 1422, memory_length 2000, epsilon 0.04572, total time 721, loss -, compute time 13.8\n",
      "episode 3429, reward 1436, memory_length 2000, epsilon 0.04568, total time 725, loss -, compute time 13.61\n",
      "episode 3430, reward 1477, memory_length 2000, epsilon 0.04564, total time 721, loss -, compute time 11.82\n",
      "Saving model for episode: 3430\n",
      "episode 3431, reward 1581, memory_length 2000, epsilon 0.0456, total time 724, loss -, compute time 12.9\n",
      "episode 3432, reward 1283, memory_length 2000, epsilon 0.04556, total time 724, loss -, compute time 13.5\n",
      "episode 3433, reward 1827, memory_length 2000, epsilon 0.04552, total time 726, loss -, compute time 12.69\n",
      "episode 3434, reward 1532, memory_length 2000, epsilon 0.04547, total time 723, loss -, compute time 13.25\n",
      "episode 3435, reward 1370, memory_length 2000, epsilon 0.04543, total time 724, loss -, compute time 12.51\n",
      "episode 3436, reward 1918, memory_length 2000, epsilon 0.04539, total time 721, loss -, compute time 13.14\n",
      "episode 3437, reward 1923, memory_length 2000, epsilon 0.04535, total time 724, loss -, compute time 13.06\n",
      "episode 3438, reward 1639, memory_length 2000, epsilon 0.04531, total time 721, loss -, compute time 11.52\n",
      "episode 3439, reward 1530, memory_length 2000, epsilon 0.04527, total time 729, loss -, compute time 14.15\n",
      "episode 3440, reward 1689, memory_length 2000, epsilon 0.04523, total time 724, loss -, compute time 11.75\n",
      "Saving model for episode: 3440\n",
      "episode 3441, reward 1618, memory_length 2000, epsilon 0.04519, total time 726, loss -, compute time 13.49\n",
      "episode 3442, reward 1747, memory_length 2000, epsilon 0.04515, total time 722, loss -, compute time 11.5\n",
      "episode 3443, reward 1769, memory_length 2000, epsilon 0.04511, total time 727, loss -, compute time 13.2\n",
      "episode 3444, reward 1638, memory_length 2000, epsilon 0.04507, total time 721, loss -, compute time 13.51\n",
      "episode 3445, reward 1676, memory_length 2000, epsilon 0.04503, total time 725, loss -, compute time 11.67\n",
      "episode 3446, reward 1391, memory_length 2000, epsilon 0.04499, total time 731, loss -, compute time 13.21\n",
      "episode 3447, reward 1555, memory_length 2000, epsilon 0.04495, total time 727, loss -, compute time 13.01\n",
      "episode 3448, reward 1698, memory_length 2000, epsilon 0.04491, total time 723, loss -, compute time 13.06\n",
      "episode 3449, reward 1540, memory_length 2000, epsilon 0.04486, total time 727, loss -, compute time 12.52\n",
      "episode 3450, reward 1413, memory_length 2000, epsilon 0.04482, total time 729, loss -, compute time 12.11\n",
      "Saving model for episode: 3450\n",
      "episode 3451, reward 1585, memory_length 2000, epsilon 0.04478, total time 725, loss -, compute time 12.74\n",
      "episode 3452, reward 1533, memory_length 2000, epsilon 0.04474, total time 723, loss -, compute time 11.09\n",
      "episode 3453, reward 1609, memory_length 2000, epsilon 0.0447, total time 727, loss [2.14835524559021], compute time 14.67\n",
      "episode 3454, reward 1718, memory_length 2000, epsilon 0.04466, total time 722, loss -, compute time 12.14\n",
      "episode 3455, reward 1346, memory_length 2000, epsilon 0.04462, total time 725, loss -, compute time 12.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3456, reward 1796, memory_length 2000, epsilon 0.04458, total time 721, loss -, compute time 13.12\n",
      "episode 3457, reward 1507, memory_length 2000, epsilon 0.04454, total time 722, loss -, compute time 12.4\n",
      "episode 3458, reward 1690, memory_length 2000, epsilon 0.0445, total time 721, loss -, compute time 13.48\n",
      "episode 3459, reward 1552, memory_length 2000, epsilon 0.04446, total time 725, loss -, compute time 12.03\n",
      "episode 3460, reward 1469, memory_length 2000, epsilon 0.04442, total time 726, loss -, compute time 12.95\n",
      "Saving model for episode: 3460\n",
      "episode 3461, reward 1657, memory_length 2000, epsilon 0.04438, total time 724, loss -, compute time 13.44\n",
      "episode 3462, reward 1436, memory_length 2000, epsilon 0.04434, total time 729, loss -, compute time 11.12\n",
      "episode 3463, reward 1661, memory_length 2000, epsilon 0.0443, total time 721, loss -, compute time 13.78\n",
      "episode 3464, reward 1582, memory_length 2000, epsilon 0.04426, total time 730, loss [1.8037166595458984], compute time 12.51\n",
      "episode 3465, reward 1748, memory_length 2000, epsilon 0.04422, total time 728, loss -, compute time 13.75\n",
      "episode 3466, reward 1721, memory_length 2000, epsilon 0.04418, total time 722, loss -, compute time 12.07\n",
      "episode 3467, reward 1474, memory_length 2000, epsilon 0.04414, total time 724, loss -, compute time 11.83\n",
      "episode 3468, reward 1532, memory_length 2000, epsilon 0.0441, total time 723, loss -, compute time 11.91\n",
      "episode 3469, reward 1647, memory_length 2000, epsilon 0.04406, total time 721, loss -, compute time 13.5\n",
      "episode 3470, reward 1641, memory_length 2000, epsilon 0.04402, total time 724, loss -, compute time 12.09\n",
      "Saving model for episode: 3470\n",
      "episode 3471, reward 1536, memory_length 2000, epsilon 0.04399, total time 725, loss -, compute time 12.82\n",
      "episode 3472, reward 1415, memory_length 2000, epsilon 0.04395, total time 725, loss -, compute time 14.01\n",
      "episode 3473, reward 1605, memory_length 2000, epsilon 0.04391, total time 723, loss -, compute time 13.93\n",
      "episode 3474, reward 1658, memory_length 2000, epsilon 0.04387, total time 724, loss -, compute time 13.45\n",
      "episode 3475, reward 1585, memory_length 2000, epsilon 0.04383, total time 721, loss -, compute time 13.68\n",
      "episode 3476, reward 1539, memory_length 2000, epsilon 0.04379, total time 727, loss -, compute time 15.49\n",
      "episode 3477, reward 1580, memory_length 2000, epsilon 0.04375, total time 727, loss -, compute time 11.89\n",
      "episode 3478, reward 1451, memory_length 2000, epsilon 0.04371, total time 721, loss -, compute time 12.17\n",
      "episode 3479, reward 1749, memory_length 2000, epsilon 0.04367, total time 722, loss -, compute time 12.42\n",
      "episode 3480, reward 1539, memory_length 2000, epsilon 0.04363, total time 721, loss -, compute time 11.8\n",
      "Saving model for episode: 3480\n",
      "episode 3481, reward 1546, memory_length 2000, epsilon 0.04359, total time 726, loss -, compute time 11.82\n",
      "episode 3482, reward 1375, memory_length 2000, epsilon 0.04355, total time 724, loss -, compute time 11.5\n",
      "episode 3483, reward 1680, memory_length 2000, epsilon 0.04351, total time 723, loss -, compute time 13.06\n",
      "episode 3484, reward 1805, memory_length 2000, epsilon 0.04347, total time 727, loss -, compute time 13.45\n",
      "episode 3485, reward 1770, memory_length 2000, epsilon 0.04343, total time 726, loss -, compute time 12.24\n",
      "episode 3486, reward 1751, memory_length 2000, epsilon 0.0434, total time 728, loss -, compute time 12.97\n",
      "episode 3487, reward 1621, memory_length 2000, epsilon 0.04336, total time 731, loss -, compute time 11.73\n",
      "episode 3488, reward 1508, memory_length 2000, epsilon 0.04332, total time 722, loss -, compute time 12.9\n",
      "episode 3489, reward 1731, memory_length 2000, epsilon 0.04328, total time 724, loss -, compute time 12.17\n",
      "episode 3490, reward 1571, memory_length 2000, epsilon 0.04324, total time 725, loss -, compute time 12.45\n",
      "Saving model for episode: 3490\n",
      "episode 3491, reward 1495, memory_length 2000, epsilon 0.0432, total time 721, loss -, compute time 12.57\n",
      "episode 3492, reward 1371, memory_length 2000, epsilon 0.04316, total time 722, loss -, compute time 11.85\n",
      "episode 3493, reward 1463, memory_length 2000, epsilon 0.04312, total time 724, loss -, compute time 15.1\n",
      "episode 3494, reward 1670, memory_length 2000, epsilon 0.04308, total time 726, loss -, compute time 11.54\n",
      "episode 3495, reward 1618, memory_length 2000, epsilon 0.04305, total time 724, loss -, compute time 12.72\n",
      "episode 3496, reward 1634, memory_length 2000, epsilon 0.04301, total time 725, loss -, compute time 12.57\n",
      "episode 3497, reward 1632, memory_length 2000, epsilon 0.04297, total time 723, loss -, compute time 10.87\n",
      "episode 3498, reward 1654, memory_length 2000, epsilon 0.04293, total time 727, loss [1.4299156665802002], compute time 11.71\n",
      "episode 3499, reward 1618, memory_length 2000, epsilon 0.04289, total time 724, loss -, compute time 12.49\n",
      "episode 3500, reward 1467, memory_length 2000, epsilon 0.04285, total time 721, loss -, compute time 11.98\n",
      "Saving model for episode: 3500\n",
      "episode 3501, reward 1662, memory_length 2000, epsilon 0.04281, total time 728, loss -, compute time 13.31\n",
      "episode 3502, reward 1811, memory_length 2000, epsilon 0.04278, total time 722, loss -, compute time 12.14\n",
      "episode 3503, reward 1410, memory_length 2000, epsilon 0.04274, total time 723, loss -, compute time 11.85\n",
      "episode 3504, reward 1725, memory_length 2000, epsilon 0.0427, total time 723, loss -, compute time 12.37\n",
      "episode 3505, reward 1648, memory_length 2000, epsilon 0.04266, total time 731, loss -, compute time 13.32\n",
      "episode 3506, reward 1854, memory_length 2000, epsilon 0.04262, total time 727, loss -, compute time 13.18\n",
      "episode 3507, reward 1809, memory_length 2000, epsilon 0.04258, total time 729, loss -, compute time 13.51\n",
      "episode 3508, reward 1775, memory_length 2000, epsilon 0.04254, total time 726, loss -, compute time 13.14\n",
      "episode 3509, reward 1581, memory_length 2000, epsilon 0.04251, total time 725, loss -, compute time 12.9\n",
      "episode 3510, reward 1656, memory_length 2000, epsilon 0.04247, total time 724, loss -, compute time 13.58\n",
      "Saving model for episode: 3510\n",
      "episode 3511, reward 1712, memory_length 2000, epsilon 0.04243, total time 722, loss -, compute time 12.4\n",
      "episode 3512, reward 1834, memory_length 2000, epsilon 0.04239, total time 723, loss -, compute time 12.38\n",
      "episode 3513, reward 1635, memory_length 2000, epsilon 0.04235, total time 721, loss -, compute time 12.84\n",
      "episode 3514, reward 1672, memory_length 2000, epsilon 0.04232, total time 724, loss -, compute time 12.99\n",
      "episode 3515, reward 1701, memory_length 2000, epsilon 0.04228, total time 722, loss -, compute time 13.04\n",
      "episode 3516, reward 1675, memory_length 2000, epsilon 0.04224, total time 722, loss -, compute time 11.56\n",
      "episode 3517, reward 1661, memory_length 2000, epsilon 0.0422, total time 728, loss -, compute time 13.02\n",
      "episode 3518, reward 1694, memory_length 2000, epsilon 0.04216, total time 722, loss -, compute time 13.69\n",
      "episode 3519, reward 1724, memory_length 2000, epsilon 0.04213, total time 728, loss -, compute time 13.03\n",
      "episode 3520, reward 1490, memory_length 2000, epsilon 0.04209, total time 726, loss -, compute time 13.55\n",
      "Saving model for episode: 3520\n",
      "episode 3521, reward 1653, memory_length 2000, epsilon 0.04205, total time 725, loss [0.8438177704811096], compute time 11.74\n",
      "episode 3522, reward 1644, memory_length 2000, epsilon 0.04201, total time 723, loss -, compute time 12.67\n",
      "episode 3523, reward 1797, memory_length 2000, epsilon 0.04197, total time 726, loss -, compute time 13.59\n",
      "episode 3524, reward 1586, memory_length 2000, epsilon 0.04194, total time 724, loss -, compute time 13.0\n",
      "episode 3525, reward 1310, memory_length 2000, epsilon 0.0419, total time 724, loss -, compute time 12.66\n",
      "episode 3526, reward 1559, memory_length 2000, epsilon 0.04186, total time 728, loss -, compute time 12.55\n",
      "episode 3527, reward 1690, memory_length 2000, epsilon 0.04182, total time 724, loss -, compute time 12.58\n",
      "episode 3528, reward 1436, memory_length 2000, epsilon 0.04179, total time 721, loss -, compute time 14.98\n",
      "episode 3529, reward 1549, memory_length 2000, epsilon 0.04175, total time 723, loss -, compute time 12.44\n",
      "episode 3530, reward 1700, memory_length 2000, epsilon 0.04171, total time 724, loss -, compute time 12.76\n",
      "Saving model for episode: 3530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3531, reward 1781, memory_length 2000, epsilon 0.04167, total time 725, loss -, compute time 13.1\n",
      "episode 3532, reward 1762, memory_length 2000, epsilon 0.04164, total time 724, loss -, compute time 13.9\n",
      "episode 3533, reward 1470, memory_length 2000, epsilon 0.0416, total time 723, loss -, compute time 12.29\n",
      "episode 3534, reward 1603, memory_length 2000, epsilon 0.04156, total time 721, loss -, compute time 14.38\n",
      "episode 3535, reward 1500, memory_length 2000, epsilon 0.04152, total time 723, loss -, compute time 13.32\n",
      "episode 3536, reward 1688, memory_length 2000, epsilon 0.04149, total time 726, loss -, compute time 13.91\n",
      "episode 3537, reward 1740, memory_length 2000, epsilon 0.04145, total time 722, loss -, compute time 12.66\n",
      "episode 3538, reward 1343, memory_length 2000, epsilon 0.04141, total time 724, loss [1.6430740356445312], compute time 13.85\n",
      "episode 3539, reward 1734, memory_length 2000, epsilon 0.04137, total time 728, loss -, compute time 11.95\n",
      "episode 3540, reward 1914, memory_length 2000, epsilon 0.04134, total time 723, loss -, compute time 13.12\n",
      "Saving model for episode: 3540\n",
      "episode 3541, reward 1688, memory_length 2000, epsilon 0.0413, total time 722, loss -, compute time 12.19\n",
      "episode 3542, reward 1406, memory_length 2000, epsilon 0.04126, total time 726, loss -, compute time 12.85\n",
      "episode 3543, reward 1582, memory_length 2000, epsilon 0.04123, total time 724, loss -, compute time 13.99\n",
      "episode 3544, reward 1623, memory_length 2000, epsilon 0.04119, total time 722, loss -, compute time 13.62\n",
      "episode 3545, reward 1617, memory_length 2000, epsilon 0.04115, total time 725, loss -, compute time 12.31\n",
      "episode 3546, reward 1625, memory_length 2000, epsilon 0.04111, total time 721, loss -, compute time 13.52\n",
      "episode 3547, reward 1573, memory_length 2000, epsilon 0.04108, total time 721, loss -, compute time 12.09\n",
      "episode 3548, reward 1427, memory_length 2000, epsilon 0.04104, total time 725, loss [101.84596252441406], compute time 11.4\n",
      "episode 3549, reward 1434, memory_length 2000, epsilon 0.041, total time 727, loss -, compute time 12.98\n",
      "episode 3550, reward 1893, memory_length 2000, epsilon 0.04097, total time 725, loss -, compute time 11.23\n",
      "Saving model for episode: 3550\n",
      "episode 3551, reward 1512, memory_length 2000, epsilon 0.04093, total time 728, loss -, compute time 13.05\n",
      "episode 3552, reward 1855, memory_length 2000, epsilon 0.04089, total time 728, loss -, compute time 12.47\n",
      "episode 3553, reward 1697, memory_length 2000, epsilon 0.04086, total time 722, loss -, compute time 12.51\n",
      "episode 3554, reward 1733, memory_length 2000, epsilon 0.04082, total time 721, loss -, compute time 12.27\n",
      "episode 3555, reward 1702, memory_length 2000, epsilon 0.04078, total time 726, loss -, compute time 12.16\n",
      "episode 3556, reward 1949, memory_length 2000, epsilon 0.04075, total time 723, loss [1.3942153453826904], compute time 12.52\n",
      "episode 3557, reward 1613, memory_length 2000, epsilon 0.04071, total time 721, loss -, compute time 13.07\n",
      "episode 3558, reward 1752, memory_length 2000, epsilon 0.04067, total time 729, loss -, compute time 11.44\n",
      "episode 3559, reward 1615, memory_length 2000, epsilon 0.04064, total time 721, loss -, compute time 13.61\n",
      "episode 3560, reward 1687, memory_length 2000, epsilon 0.0406, total time 721, loss -, compute time 13.08\n",
      "Saving model for episode: 3560\n",
      "episode 3561, reward 1665, memory_length 2000, epsilon 0.04056, total time 721, loss -, compute time 13.17\n",
      "episode 3562, reward 1744, memory_length 2000, epsilon 0.04053, total time 728, loss -, compute time 13.02\n",
      "episode 3563, reward 1746, memory_length 2000, epsilon 0.04049, total time 728, loss -, compute time 12.27\n",
      "episode 3564, reward 1577, memory_length 2000, epsilon 0.04045, total time 723, loss -, compute time 11.83\n",
      "episode 3565, reward 1766, memory_length 2000, epsilon 0.04042, total time 727, loss -, compute time 12.67\n",
      "episode 3566, reward 1419, memory_length 2000, epsilon 0.04038, total time 725, loss -, compute time 12.83\n",
      "episode 3567, reward 1726, memory_length 2000, epsilon 0.04034, total time 725, loss -, compute time 12.76\n",
      "episode 3568, reward 1409, memory_length 2000, epsilon 0.04031, total time 726, loss -, compute time 11.74\n",
      "episode 3569, reward 1671, memory_length 2000, epsilon 0.04027, total time 723, loss -, compute time 12.29\n",
      "episode 3570, reward 1832, memory_length 2000, epsilon 0.04024, total time 728, loss -, compute time 11.66\n",
      "Saving model for episode: 3570\n",
      "episode 3571, reward 1767, memory_length 2000, epsilon 0.0402, total time 723, loss -, compute time 12.76\n",
      "episode 3572, reward 1807, memory_length 2000, epsilon 0.04016, total time 723, loss -, compute time 11.49\n",
      "episode 3573, reward 1909, memory_length 2000, epsilon 0.04013, total time 721, loss -, compute time 12.11\n",
      "episode 3574, reward 1726, memory_length 2000, epsilon 0.04009, total time 723, loss -, compute time 13.95\n",
      "episode 3575, reward 1519, memory_length 2000, epsilon 0.04006, total time 721, loss -, compute time 13.56\n",
      "episode 3576, reward 1563, memory_length 2000, epsilon 0.04002, total time 723, loss -, compute time 11.28\n",
      "episode 3577, reward 1869, memory_length 2000, epsilon 0.03998, total time 722, loss -, compute time 12.44\n",
      "episode 3578, reward 1692, memory_length 2000, epsilon 0.03995, total time 721, loss -, compute time 13.47\n",
      "episode 3579, reward 1624, memory_length 2000, epsilon 0.03991, total time 721, loss -, compute time 13.25\n",
      "episode 3580, reward 1398, memory_length 2000, epsilon 0.03988, total time 723, loss -, compute time 11.64\n",
      "Saving model for episode: 3580\n",
      "episode 3581, reward 1425, memory_length 2000, epsilon 0.03984, total time 723, loss -, compute time 12.68\n",
      "episode 3582, reward 1553, memory_length 2000, epsilon 0.0398, total time 728, loss -, compute time 13.07\n",
      "episode 3583, reward 1596, memory_length 2000, epsilon 0.03977, total time 728, loss -, compute time 11.56\n",
      "episode 3584, reward 1107, memory_length 2000, epsilon 0.03973, total time 733, loss [2.077073097229004], compute time 11.46\n",
      "episode 3585, reward 1571, memory_length 2000, epsilon 0.0397, total time 725, loss -, compute time 13.25\n",
      "episode 3586, reward 1647, memory_length 2000, epsilon 0.03966, total time 728, loss -, compute time 12.94\n",
      "episode 3587, reward 1809, memory_length 2000, epsilon 0.03962, total time 722, loss -, compute time 13.07\n",
      "episode 3588, reward 1644, memory_length 2000, epsilon 0.03959, total time 727, loss [1.9844944477081299], compute time 12.94\n",
      "episode 3589, reward 1634, memory_length 2000, epsilon 0.03955, total time 726, loss -, compute time 13.42\n",
      "episode 3590, reward 1730, memory_length 2000, epsilon 0.03952, total time 721, loss [3.0164504051208496], compute time 12.72\n",
      "Saving model for episode: 3590\n",
      "episode 3591, reward 1664, memory_length 2000, epsilon 0.03948, total time 721, loss -, compute time 14.57\n",
      "episode 3592, reward 1692, memory_length 2000, epsilon 0.03945, total time 727, loss -, compute time 11.43\n",
      "episode 3593, reward 1612, memory_length 2000, epsilon 0.03941, total time 722, loss -, compute time 12.43\n",
      "episode 3594, reward 1746, memory_length 2000, epsilon 0.03938, total time 726, loss -, compute time 12.98\n",
      "episode 3595, reward 1653, memory_length 2000, epsilon 0.03934, total time 728, loss -, compute time 13.6\n",
      "episode 3596, reward 1906, memory_length 2000, epsilon 0.03931, total time 726, loss -, compute time 12.6\n",
      "episode 3597, reward 1247, memory_length 2000, epsilon 0.03927, total time 721, loss -, compute time 11.95\n",
      "episode 3598, reward 1527, memory_length 2000, epsilon 0.03923, total time 724, loss -, compute time 14.58\n",
      "episode 3599, reward 1693, memory_length 2000, epsilon 0.0392, total time 725, loss -, compute time 12.43\n",
      "episode 3600, reward 1644, memory_length 2000, epsilon 0.03916, total time 725, loss -, compute time 12.41\n",
      "Saving model for episode: 3600\n",
      "episode 3601, reward 1501, memory_length 2000, epsilon 0.03913, total time 728, loss -, compute time 13.54\n",
      "episode 3602, reward 1580, memory_length 2000, epsilon 0.03909, total time 728, loss -, compute time 13.71\n",
      "episode 3603, reward 1820, memory_length 2000, epsilon 0.03906, total time 727, loss -, compute time 12.26\n",
      "episode 3604, reward 1590, memory_length 2000, epsilon 0.03902, total time 723, loss -, compute time 11.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3605, reward 1467, memory_length 2000, epsilon 0.03899, total time 733, loss -, compute time 14.7\n",
      "episode 3606, reward 1550, memory_length 2000, epsilon 0.03895, total time 727, loss -, compute time 13.19\n",
      "episode 3607, reward 1780, memory_length 2000, epsilon 0.03892, total time 726, loss -, compute time 13.83\n",
      "episode 3608, reward 1788, memory_length 2000, epsilon 0.03888, total time 724, loss -, compute time 13.33\n",
      "episode 3609, reward 1429, memory_length 2000, epsilon 0.03885, total time 724, loss -, compute time 15.74\n",
      "episode 3610, reward 1592, memory_length 2000, epsilon 0.03881, total time 723, loss -, compute time 13.19\n",
      "Saving model for episode: 3610\n",
      "episode 3611, reward 1761, memory_length 2000, epsilon 0.03878, total time 725, loss -, compute time 13.08\n",
      "episode 3612, reward 1623, memory_length 2000, epsilon 0.03874, total time 724, loss -, compute time 13.84\n",
      "episode 3613, reward 1645, memory_length 2000, epsilon 0.03871, total time 721, loss -, compute time 13.5\n",
      "episode 3614, reward 1485, memory_length 2000, epsilon 0.03867, total time 723, loss -, compute time 13.16\n",
      "episode 3615, reward 1477, memory_length 2000, epsilon 0.03864, total time 723, loss -, compute time 14.35\n",
      "episode 3616, reward 1582, memory_length 2000, epsilon 0.0386, total time 726, loss -, compute time 13.96\n",
      "episode 3617, reward 1346, memory_length 2000, epsilon 0.03857, total time 726, loss -, compute time 14.17\n",
      "episode 3618, reward 1397, memory_length 2000, epsilon 0.03853, total time 724, loss -, compute time 12.81\n",
      "episode 3619, reward 1681, memory_length 2000, epsilon 0.0385, total time 721, loss -, compute time 13.91\n",
      "episode 3620, reward 1530, memory_length 2000, epsilon 0.03847, total time 729, loss -, compute time 14.21\n",
      "Saving model for episode: 3620\n",
      "episode 3621, reward 1621, memory_length 2000, epsilon 0.03843, total time 723, loss -, compute time 11.61\n",
      "episode 3622, reward 1642, memory_length 2000, epsilon 0.0384, total time 726, loss -, compute time 13.69\n",
      "episode 3623, reward 1746, memory_length 2000, epsilon 0.03836, total time 721, loss -, compute time 12.88\n",
      "episode 3624, reward 1701, memory_length 2000, epsilon 0.03833, total time 721, loss -, compute time 12.19\n",
      "episode 3625, reward 1470, memory_length 2000, epsilon 0.03829, total time 722, loss -, compute time 12.82\n",
      "episode 3626, reward 1657, memory_length 2000, epsilon 0.03826, total time 726, loss -, compute time 11.78\n",
      "episode 3627, reward 1600, memory_length 2000, epsilon 0.03822, total time 721, loss -, compute time 13.54\n",
      "episode 3628, reward 1492, memory_length 2000, epsilon 0.03819, total time 721, loss [2.294329881668091], compute time 13.14\n",
      "episode 3629, reward 1560, memory_length 2000, epsilon 0.03815, total time 723, loss -, compute time 13.28\n",
      "episode 3630, reward 1589, memory_length 2000, epsilon 0.03812, total time 729, loss -, compute time 13.78\n",
      "Saving model for episode: 3630\n",
      "episode 3631, reward 1756, memory_length 2000, epsilon 0.03809, total time 722, loss -, compute time 15.17\n",
      "episode 3632, reward 1554, memory_length 2000, epsilon 0.03805, total time 724, loss -, compute time 13.51\n",
      "episode 3633, reward 1692, memory_length 2000, epsilon 0.03802, total time 727, loss -, compute time 12.82\n",
      "episode 3634, reward 1733, memory_length 2000, epsilon 0.03798, total time 727, loss -, compute time 12.73\n",
      "episode 3635, reward 1672, memory_length 2000, epsilon 0.03795, total time 729, loss -, compute time 13.06\n",
      "episode 3636, reward 1694, memory_length 2000, epsilon 0.03792, total time 721, loss [3.5466318130493164], compute time 11.08\n",
      "episode 3637, reward 1719, memory_length 2000, epsilon 0.03788, total time 724, loss -, compute time 12.0\n",
      "episode 3638, reward 1617, memory_length 2000, epsilon 0.03785, total time 727, loss -, compute time 14.39\n",
      "episode 3639, reward 1755, memory_length 2000, epsilon 0.03781, total time 726, loss -, compute time 12.1\n",
      "episode 3640, reward 1530, memory_length 2000, epsilon 0.03778, total time 721, loss -, compute time 12.54\n",
      "Saving model for episode: 3640\n",
      "episode 3641, reward 1647, memory_length 2000, epsilon 0.03775, total time 728, loss -, compute time 12.25\n",
      "episode 3642, reward 1535, memory_length 2000, epsilon 0.03771, total time 721, loss -, compute time 12.85\n",
      "episode 3643, reward 1665, memory_length 2000, epsilon 0.03768, total time 728, loss -, compute time 13.43\n",
      "episode 3644, reward 1726, memory_length 2000, epsilon 0.03764, total time 727, loss -, compute time 13.24\n",
      "episode 3645, reward 1624, memory_length 2000, epsilon 0.03761, total time 721, loss -, compute time 12.8\n",
      "episode 3646, reward 1586, memory_length 2000, epsilon 0.03758, total time 723, loss -, compute time 12.47\n",
      "episode 3647, reward 1376, memory_length 2000, epsilon 0.03754, total time 722, loss [2.258527994155884], compute time 11.94\n",
      "episode 3648, reward 1354, memory_length 2000, epsilon 0.03751, total time 725, loss [0.8661051988601685], compute time 12.32\n",
      "episode 3649, reward 1654, memory_length 2000, epsilon 0.03747, total time 723, loss -, compute time 12.77\n",
      "episode 3650, reward 1704, memory_length 2000, epsilon 0.03744, total time 721, loss -, compute time 11.97\n",
      "Saving model for episode: 3650\n",
      "episode 3651, reward 1658, memory_length 2000, epsilon 0.03741, total time 726, loss -, compute time 14.54\n",
      "episode 3652, reward 1652, memory_length 2000, epsilon 0.03737, total time 726, loss -, compute time 13.44\n",
      "episode 3653, reward 1537, memory_length 2000, epsilon 0.03734, total time 723, loss -, compute time 13.6\n",
      "episode 3654, reward 1614, memory_length 2000, epsilon 0.03731, total time 726, loss -, compute time 11.94\n",
      "episode 3655, reward 1516, memory_length 2000, epsilon 0.03727, total time 725, loss -, compute time 13.97\n",
      "episode 3656, reward 1753, memory_length 2000, epsilon 0.03724, total time 725, loss -, compute time 13.72\n",
      "episode 3657, reward 1665, memory_length 2000, epsilon 0.03721, total time 728, loss -, compute time 13.84\n",
      "episode 3658, reward 1688, memory_length 2000, epsilon 0.03717, total time 729, loss -, compute time 13.88\n",
      "episode 3659, reward 1795, memory_length 2000, epsilon 0.03714, total time 723, loss -, compute time 14.08\n",
      "episode 3660, reward 1492, memory_length 2000, epsilon 0.03711, total time 723, loss -, compute time 14.76\n",
      "Saving model for episode: 3660\n",
      "episode 3661, reward 1675, memory_length 2000, epsilon 0.03707, total time 725, loss -, compute time 12.39\n",
      "episode 3662, reward 1775, memory_length 2000, epsilon 0.03704, total time 723, loss -, compute time 12.36\n",
      "episode 3663, reward 1625, memory_length 2000, epsilon 0.03701, total time 723, loss -, compute time 14.27\n",
      "episode 3664, reward 1516, memory_length 2000, epsilon 0.03697, total time 722, loss [3.0885109901428223], compute time 12.33\n",
      "episode 3665, reward 1483, memory_length 2000, epsilon 0.03694, total time 721, loss [66.68389129638672], compute time 12.86\n",
      "episode 3666, reward 1735, memory_length 2000, epsilon 0.03691, total time 726, loss -, compute time 11.95\n",
      "episode 3667, reward 1770, memory_length 2000, epsilon 0.03687, total time 722, loss -, compute time 12.96\n",
      "episode 3668, reward 1491, memory_length 2000, epsilon 0.03684, total time 724, loss -, compute time 13.38\n",
      "episode 3669, reward 1445, memory_length 2000, epsilon 0.03681, total time 724, loss -, compute time 12.29\n",
      "episode 3670, reward 1882, memory_length 2000, epsilon 0.03677, total time 726, loss -, compute time 12.34\n",
      "Saving model for episode: 3670\n",
      "episode 3671, reward 1946, memory_length 2000, epsilon 0.03674, total time 723, loss -, compute time 13.67\n",
      "episode 3672, reward 1472, memory_length 2000, epsilon 0.03671, total time 723, loss -, compute time 11.54\n",
      "episode 3673, reward 1446, memory_length 2000, epsilon 0.03667, total time 724, loss -, compute time 12.68\n",
      "episode 3674, reward 1666, memory_length 2000, epsilon 0.03664, total time 724, loss -, compute time 12.45\n",
      "episode 3675, reward 1521, memory_length 2000, epsilon 0.03661, total time 727, loss -, compute time 12.91\n",
      "episode 3676, reward 1675, memory_length 2000, epsilon 0.03657, total time 728, loss -, compute time 13.35\n",
      "episode 3677, reward 1863, memory_length 2000, epsilon 0.03654, total time 721, loss [96.05782318115234], compute time 14.57\n",
      "episode 3678, reward 1636, memory_length 2000, epsilon 0.03651, total time 724, loss -, compute time 11.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3679, reward 1575, memory_length 2000, epsilon 0.03648, total time 721, loss -, compute time 13.22\n",
      "episode 3680, reward 1485, memory_length 2000, epsilon 0.03644, total time 726, loss -, compute time 12.63\n",
      "Saving model for episode: 3680\n",
      "episode 3681, reward 1779, memory_length 2000, epsilon 0.03641, total time 727, loss -, compute time 12.75\n",
      "episode 3682, reward 1692, memory_length 2000, epsilon 0.03638, total time 728, loss -, compute time 12.13\n",
      "episode 3683, reward 1506, memory_length 2000, epsilon 0.03634, total time 722, loss -, compute time 12.82\n",
      "episode 3684, reward 1368, memory_length 2000, epsilon 0.03631, total time 730, loss -, compute time 12.12\n",
      "episode 3685, reward 1577, memory_length 2000, epsilon 0.03628, total time 724, loss -, compute time 11.1\n",
      "episode 3686, reward 1619, memory_length 2000, epsilon 0.03625, total time 721, loss -, compute time 13.11\n",
      "episode 3687, reward 1580, memory_length 2000, epsilon 0.03621, total time 731, loss -, compute time 12.91\n",
      "episode 3688, reward 1603, memory_length 2000, epsilon 0.03618, total time 729, loss -, compute time 11.29\n",
      "episode 3689, reward 1720, memory_length 2000, epsilon 0.03615, total time 727, loss -, compute time 13.45\n",
      "episode 3690, reward 1581, memory_length 2000, epsilon 0.03612, total time 727, loss -, compute time 12.44\n",
      "Saving model for episode: 3690\n",
      "episode 3691, reward 1611, memory_length 2000, epsilon 0.03608, total time 728, loss -, compute time 13.4\n",
      "episode 3692, reward 1587, memory_length 2000, epsilon 0.03605, total time 724, loss -, compute time 13.31\n",
      "episode 3693, reward 1757, memory_length 2000, epsilon 0.03602, total time 721, loss -, compute time 13.59\n",
      "episode 3694, reward 1774, memory_length 2000, epsilon 0.03599, total time 728, loss -, compute time 12.75\n",
      "episode 3695, reward 1577, memory_length 2000, epsilon 0.03595, total time 722, loss -, compute time 13.08\n",
      "episode 3696, reward 1593, memory_length 2000, epsilon 0.03592, total time 721, loss -, compute time 12.73\n",
      "episode 3697, reward 1733, memory_length 2000, epsilon 0.03589, total time 722, loss -, compute time 11.62\n",
      "episode 3698, reward 1667, memory_length 2000, epsilon 0.03586, total time 724, loss -, compute time 12.83\n",
      "episode 3699, reward 1694, memory_length 2000, epsilon 0.03583, total time 721, loss -, compute time 13.76\n",
      "episode 3700, reward 1790, memory_length 2000, epsilon 0.03579, total time 722, loss -, compute time 11.59\n",
      "Saving model for episode: 3700\n",
      "episode 3701, reward 1900, memory_length 2000, epsilon 0.03576, total time 725, loss -, compute time 12.83\n",
      "episode 3702, reward 1521, memory_length 2000, epsilon 0.03573, total time 735, loss -, compute time 12.21\n",
      "episode 3703, reward 1591, memory_length 2000, epsilon 0.0357, total time 730, loss -, compute time 13.34\n",
      "episode 3704, reward 1542, memory_length 2000, epsilon 0.03566, total time 722, loss -, compute time 13.24\n",
      "episode 3705, reward 1602, memory_length 2000, epsilon 0.03563, total time 731, loss -, compute time 12.06\n",
      "episode 3706, reward 1771, memory_length 2000, epsilon 0.0356, total time 726, loss -, compute time 12.85\n",
      "episode 3707, reward 1546, memory_length 2000, epsilon 0.03557, total time 726, loss -, compute time 13.33\n",
      "episode 3708, reward 1908, memory_length 2000, epsilon 0.03554, total time 726, loss -, compute time 11.72\n",
      "episode 3709, reward 1592, memory_length 2000, epsilon 0.0355, total time 725, loss -, compute time 13.4\n",
      "episode 3710, reward 1262, memory_length 2000, epsilon 0.03547, total time 723, loss -, compute time 12.89\n",
      "Saving model for episode: 3710\n",
      "episode 3711, reward 1440, memory_length 2000, epsilon 0.03544, total time 723, loss -, compute time 13.49\n",
      "episode 3712, reward 1625, memory_length 2000, epsilon 0.03541, total time 727, loss -, compute time 12.29\n",
      "episode 3713, reward 1799, memory_length 2000, epsilon 0.03538, total time 722, loss -, compute time 12.62\n",
      "episode 3714, reward 1593, memory_length 2000, epsilon 0.03534, total time 731, loss -, compute time 13.26\n",
      "episode 3715, reward 1443, memory_length 2000, epsilon 0.03531, total time 721, loss -, compute time 13.61\n",
      "episode 3716, reward 1629, memory_length 2000, epsilon 0.03528, total time 724, loss -, compute time 13.63\n",
      "episode 3717, reward 1859, memory_length 2000, epsilon 0.03525, total time 721, loss -, compute time 11.88\n",
      "episode 3718, reward 1669, memory_length 2000, epsilon 0.03522, total time 721, loss -, compute time 14.0\n",
      "episode 3719, reward 1518, memory_length 2000, epsilon 0.03519, total time 722, loss -, compute time 12.92\n",
      "episode 3720, reward 1526, memory_length 2000, epsilon 0.03515, total time 721, loss -, compute time 12.07\n",
      "Saving model for episode: 3720\n",
      "episode 3721, reward 1755, memory_length 2000, epsilon 0.03512, total time 725, loss -, compute time 13.08\n",
      "episode 3722, reward 1837, memory_length 2000, epsilon 0.03509, total time 722, loss -, compute time 12.92\n",
      "episode 3723, reward 1656, memory_length 2000, epsilon 0.03506, total time 729, loss -, compute time 12.02\n",
      "episode 3724, reward 1875, memory_length 2000, epsilon 0.03503, total time 722, loss -, compute time 12.74\n",
      "episode 3725, reward 1689, memory_length 2000, epsilon 0.035, total time 722, loss -, compute time 13.54\n",
      "episode 3726, reward 1582, memory_length 2000, epsilon 0.03497, total time 722, loss -, compute time 12.81\n",
      "episode 3727, reward 1829, memory_length 2000, epsilon 0.03493, total time 722, loss [2.402916431427002], compute time 14.42\n",
      "episode 3728, reward 1534, memory_length 2000, epsilon 0.0349, total time 723, loss -, compute time 11.88\n",
      "episode 3729, reward 1800, memory_length 2000, epsilon 0.03487, total time 724, loss -, compute time 12.9\n",
      "episode 3730, reward 1536, memory_length 2000, epsilon 0.03484, total time 725, loss -, compute time 12.49\n",
      "Saving model for episode: 3730\n",
      "episode 3731, reward 1510, memory_length 2000, epsilon 0.03481, total time 722, loss [1.7266781330108643], compute time 12.94\n",
      "episode 3732, reward 1631, memory_length 2000, epsilon 0.03478, total time 723, loss -, compute time 11.72\n",
      "episode 3733, reward 1914, memory_length 2000, epsilon 0.03475, total time 723, loss -, compute time 12.47\n",
      "episode 3734, reward 1509, memory_length 2000, epsilon 0.03471, total time 723, loss -, compute time 12.15\n",
      "episode 3735, reward 1566, memory_length 2000, epsilon 0.03468, total time 729, loss -, compute time 12.62\n",
      "episode 3736, reward 1296, memory_length 2000, epsilon 0.03465, total time 727, loss -, compute time 11.81\n",
      "episode 3737, reward 1585, memory_length 2000, epsilon 0.03462, total time 725, loss -, compute time 14.51\n",
      "episode 3738, reward 1643, memory_length 2000, epsilon 0.03459, total time 723, loss -, compute time 10.78\n",
      "episode 3739, reward 1954, memory_length 2000, epsilon 0.03456, total time 724, loss -, compute time 12.56\n",
      "episode 3740, reward 1676, memory_length 2000, epsilon 0.03453, total time 724, loss -, compute time 12.13\n",
      "Saving model for episode: 3740\n",
      "episode 3741, reward 1684, memory_length 2000, epsilon 0.0345, total time 723, loss -, compute time 11.96\n",
      "episode 3742, reward 1352, memory_length 2000, epsilon 0.03447, total time 725, loss -, compute time 12.57\n",
      "episode 3743, reward 1809, memory_length 2000, epsilon 0.03443, total time 728, loss -, compute time 13.79\n",
      "episode 3744, reward 1647, memory_length 2000, epsilon 0.0344, total time 731, loss -, compute time 12.65\n",
      "episode 3745, reward 1577, memory_length 2000, epsilon 0.03437, total time 726, loss -, compute time 13.22\n",
      "episode 3746, reward 1776, memory_length 2000, epsilon 0.03434, total time 725, loss -, compute time 13.07\n",
      "episode 3747, reward 1362, memory_length 2000, epsilon 0.03431, total time 722, loss -, compute time 11.47\n",
      "episode 3748, reward 1680, memory_length 2000, epsilon 0.03428, total time 723, loss -, compute time 13.56\n",
      "episode 3749, reward 1533, memory_length 2000, epsilon 0.03425, total time 725, loss -, compute time 12.72\n",
      "episode 3750, reward 1472, memory_length 2000, epsilon 0.03422, total time 728, loss -, compute time 14.17\n",
      "Saving model for episode: 3750\n",
      "episode 3751, reward 1604, memory_length 2000, epsilon 0.03419, total time 724, loss -, compute time 12.53\n",
      "episode 3752, reward 1450, memory_length 2000, epsilon 0.03416, total time 721, loss -, compute time 13.91\n",
      "episode 3753, reward 1650, memory_length 2000, epsilon 0.03413, total time 721, loss -, compute time 12.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3754, reward 1509, memory_length 2000, epsilon 0.0341, total time 729, loss -, compute time 14.69\n",
      "episode 3755, reward 1602, memory_length 2000, epsilon 0.03406, total time 725, loss -, compute time 12.39\n",
      "episode 3756, reward 1573, memory_length 2000, epsilon 0.03403, total time 721, loss -, compute time 14.2\n",
      "episode 3757, reward 1562, memory_length 2000, epsilon 0.034, total time 723, loss -, compute time 13.68\n",
      "episode 3758, reward 1886, memory_length 2000, epsilon 0.03397, total time 725, loss -, compute time 13.03\n",
      "episode 3759, reward 1766, memory_length 2000, epsilon 0.03394, total time 723, loss -, compute time 12.92\n",
      "episode 3760, reward 1563, memory_length 2000, epsilon 0.03391, total time 727, loss -, compute time 13.38\n",
      "Saving model for episode: 3760\n",
      "episode 3761, reward 1753, memory_length 2000, epsilon 0.03388, total time 724, loss -, compute time 12.18\n",
      "episode 3762, reward 1833, memory_length 2000, epsilon 0.03385, total time 724, loss -, compute time 13.78\n",
      "episode 3763, reward 1787, memory_length 2000, epsilon 0.03382, total time 722, loss -, compute time 11.47\n",
      "episode 3764, reward 1976, memory_length 2000, epsilon 0.03379, total time 734, loss -, compute time 12.86\n",
      "episode 3765, reward 1774, memory_length 2000, epsilon 0.03376, total time 726, loss -, compute time 13.58\n",
      "episode 3766, reward 1511, memory_length 2000, epsilon 0.03373, total time 721, loss -, compute time 13.86\n",
      "episode 3767, reward 1706, memory_length 2000, epsilon 0.0337, total time 726, loss -, compute time 13.37\n",
      "episode 3768, reward 1845, memory_length 2000, epsilon 0.03367, total time 721, loss -, compute time 11.7\n",
      "episode 3769, reward 2025, memory_length 2000, epsilon 0.03364, total time 728, loss -, compute time 11.19\n",
      "episode 3770, reward 1647, memory_length 2000, epsilon 0.03361, total time 722, loss -, compute time 13.41\n",
      "Saving model for episode: 3770\n",
      "episode 3771, reward 1873, memory_length 2000, epsilon 0.03358, total time 724, loss -, compute time 12.6\n",
      "episode 3772, reward 1685, memory_length 2000, epsilon 0.03355, total time 722, loss -, compute time 11.34\n",
      "episode 3773, reward 1763, memory_length 2000, epsilon 0.03352, total time 722, loss -, compute time 12.48\n",
      "episode 3774, reward 1643, memory_length 2000, epsilon 0.03349, total time 723, loss -, compute time 12.48\n",
      "episode 3775, reward 1564, memory_length 2000, epsilon 0.03346, total time 721, loss -, compute time 11.82\n",
      "episode 3776, reward 1598, memory_length 2000, epsilon 0.03343, total time 721, loss -, compute time 12.95\n",
      "episode 3777, reward 1653, memory_length 2000, epsilon 0.0334, total time 725, loss -, compute time 12.56\n",
      "episode 3778, reward 1599, memory_length 2000, epsilon 0.03337, total time 721, loss -, compute time 11.73\n",
      "episode 3779, reward 1718, memory_length 2000, epsilon 0.03334, total time 730, loss -, compute time 13.38\n",
      "episode 3780, reward 1770, memory_length 2000, epsilon 0.03331, total time 721, loss -, compute time 13.02\n",
      "Saving model for episode: 3780\n",
      "episode 3781, reward 1710, memory_length 2000, epsilon 0.03328, total time 722, loss -, compute time 11.89\n",
      "episode 3782, reward 1490, memory_length 2000, epsilon 0.03325, total time 729, loss -, compute time 12.17\n",
      "episode 3783, reward 1353, memory_length 2000, epsilon 0.03322, total time 721, loss -, compute time 12.41\n",
      "episode 3784, reward 1467, memory_length 2000, epsilon 0.03319, total time 726, loss -, compute time 13.9\n",
      "episode 3785, reward 1584, memory_length 2000, epsilon 0.03316, total time 724, loss -, compute time 13.18\n",
      "episode 3786, reward 1468, memory_length 2000, epsilon 0.03313, total time 724, loss -, compute time 13.96\n",
      "episode 3787, reward 1633, memory_length 2000, epsilon 0.0331, total time 721, loss -, compute time 13.65\n",
      "episode 3788, reward 1687, memory_length 2000, epsilon 0.03307, total time 723, loss -, compute time 13.62\n",
      "episode 3789, reward 1618, memory_length 2000, epsilon 0.03304, total time 721, loss -, compute time 13.79\n",
      "episode 3790, reward 1422, memory_length 2000, epsilon 0.03301, total time 724, loss -, compute time 15.05\n",
      "Saving model for episode: 3790\n",
      "episode 3791, reward 1752, memory_length 2000, epsilon 0.03298, total time 726, loss -, compute time 12.61\n",
      "episode 3792, reward 1717, memory_length 2000, epsilon 0.03295, total time 722, loss [99.46424102783203], compute time 11.22\n",
      "episode 3793, reward 1548, memory_length 2000, epsilon 0.03292, total time 721, loss -, compute time 13.72\n",
      "episode 3794, reward 1629, memory_length 2000, epsilon 0.03289, total time 726, loss -, compute time 12.45\n",
      "episode 3795, reward 1757, memory_length 2000, epsilon 0.03286, total time 725, loss -, compute time 13.0\n",
      "episode 3796, reward 1785, memory_length 2000, epsilon 0.03283, total time 723, loss -, compute time 13.39\n",
      "episode 3797, reward 1720, memory_length 2000, epsilon 0.0328, total time 727, loss -, compute time 13.73\n",
      "episode 3798, reward 1674, memory_length 2000, epsilon 0.03277, total time 726, loss -, compute time 14.22\n",
      "episode 3799, reward 1496, memory_length 2000, epsilon 0.03274, total time 728, loss -, compute time 13.95\n",
      "episode 3800, reward 1779, memory_length 2000, epsilon 0.03271, total time 721, loss -, compute time 12.03\n",
      "Saving model for episode: 3800\n",
      "episode 3801, reward 1702, memory_length 2000, epsilon 0.03268, total time 723, loss -, compute time 12.91\n",
      "episode 3802, reward 1675, memory_length 2000, epsilon 0.03265, total time 727, loss -, compute time 12.89\n",
      "episode 3803, reward 1460, memory_length 2000, epsilon 0.03262, total time 726, loss -, compute time 13.08\n",
      "episode 3804, reward 1599, memory_length 2000, epsilon 0.03259, total time 723, loss -, compute time 13.21\n",
      "episode 3805, reward 1623, memory_length 2000, epsilon 0.03257, total time 721, loss -, compute time 12.52\n",
      "episode 3806, reward 1376, memory_length 2000, epsilon 0.03254, total time 722, loss -, compute time 11.9\n",
      "episode 3807, reward 1724, memory_length 2000, epsilon 0.03251, total time 726, loss -, compute time 13.24\n",
      "episode 3808, reward 1826, memory_length 2000, epsilon 0.03248, total time 725, loss -, compute time 12.54\n",
      "episode 3809, reward 1602, memory_length 2000, epsilon 0.03245, total time 724, loss -, compute time 12.77\n",
      "episode 3810, reward 1259, memory_length 2000, epsilon 0.03242, total time 724, loss -, compute time 13.81\n",
      "Saving model for episode: 3810\n",
      "episode 3811, reward 1770, memory_length 2000, epsilon 0.03239, total time 723, loss -, compute time 11.66\n",
      "episode 3812, reward 1564, memory_length 2000, epsilon 0.03236, total time 727, loss -, compute time 11.83\n",
      "episode 3813, reward 1580, memory_length 2000, epsilon 0.03233, total time 727, loss -, compute time 13.03\n",
      "episode 3814, reward 1779, memory_length 2000, epsilon 0.0323, total time 725, loss -, compute time 12.29\n",
      "episode 3815, reward 1521, memory_length 2000, epsilon 0.03227, total time 730, loss -, compute time 12.12\n",
      "episode 3816, reward 1582, memory_length 2000, epsilon 0.03224, total time 722, loss -, compute time 12.73\n",
      "episode 3817, reward 1486, memory_length 2000, epsilon 0.03222, total time 726, loss -, compute time 11.67\n",
      "episode 3818, reward 1653, memory_length 2000, epsilon 0.03219, total time 726, loss -, compute time 13.15\n",
      "episode 3819, reward 1697, memory_length 2000, epsilon 0.03216, total time 727, loss -, compute time 13.33\n",
      "episode 3820, reward 1575, memory_length 2000, epsilon 0.03213, total time 726, loss -, compute time 12.28\n",
      "Saving model for episode: 3820\n",
      "episode 3821, reward 1675, memory_length 2000, epsilon 0.0321, total time 724, loss -, compute time 13.98\n",
      "episode 3822, reward 1775, memory_length 2000, epsilon 0.03207, total time 730, loss -, compute time 12.92\n",
      "episode 3823, reward 1476, memory_length 2000, epsilon 0.03204, total time 721, loss -, compute time 13.33\n",
      "episode 3824, reward 1600, memory_length 2000, epsilon 0.03201, total time 724, loss -, compute time 14.27\n",
      "episode 3825, reward 1431, memory_length 2000, epsilon 0.03198, total time 730, loss -, compute time 12.86\n",
      "episode 3826, reward 1703, memory_length 2000, epsilon 0.03196, total time 724, loss -, compute time 12.47\n",
      "episode 3827, reward 1694, memory_length 2000, epsilon 0.03193, total time 724, loss -, compute time 11.42\n",
      "episode 3828, reward 1620, memory_length 2000, epsilon 0.0319, total time 729, loss -, compute time 13.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3829, reward 1701, memory_length 2000, epsilon 0.03187, total time 722, loss -, compute time 13.03\n",
      "episode 3830, reward 1609, memory_length 2000, epsilon 0.03184, total time 723, loss -, compute time 13.83\n",
      "Saving model for episode: 3830\n",
      "episode 3831, reward 1660, memory_length 2000, epsilon 0.03181, total time 727, loss -, compute time 12.69\n",
      "episode 3832, reward 1535, memory_length 2000, epsilon 0.03178, total time 729, loss -, compute time 12.53\n",
      "episode 3833, reward 1641, memory_length 2000, epsilon 0.03176, total time 723, loss [1.5848016738891602], compute time 11.89\n",
      "episode 3834, reward 1698, memory_length 2000, epsilon 0.03173, total time 723, loss -, compute time 11.59\n",
      "episode 3835, reward 1809, memory_length 2000, epsilon 0.0317, total time 727, loss -, compute time 13.78\n",
      "episode 3836, reward 1656, memory_length 2000, epsilon 0.03167, total time 722, loss -, compute time 14.26\n",
      "episode 3837, reward 1671, memory_length 2000, epsilon 0.03164, total time 723, loss -, compute time 12.03\n",
      "episode 3838, reward 1627, memory_length 2000, epsilon 0.03161, total time 724, loss -, compute time 13.14\n",
      "episode 3839, reward 1758, memory_length 2000, epsilon 0.03158, total time 727, loss -, compute time 13.47\n",
      "episode 3840, reward 1465, memory_length 2000, epsilon 0.03156, total time 724, loss -, compute time 13.64\n",
      "Saving model for episode: 3840\n",
      "episode 3841, reward 1454, memory_length 2000, epsilon 0.03153, total time 733, loss -, compute time 12.5\n",
      "episode 3842, reward 1734, memory_length 2000, epsilon 0.0315, total time 721, loss -, compute time 12.85\n",
      "episode 3843, reward 1782, memory_length 2000, epsilon 0.03147, total time 727, loss -, compute time 11.3\n",
      "episode 3844, reward 1495, memory_length 2000, epsilon 0.03144, total time 727, loss -, compute time 12.14\n",
      "episode 3845, reward 1494, memory_length 2000, epsilon 0.03141, total time 727, loss -, compute time 11.94\n",
      "episode 3846, reward 1744, memory_length 2000, epsilon 0.03139, total time 721, loss -, compute time 14.24\n",
      "episode 3847, reward 1526, memory_length 2000, epsilon 0.03136, total time 721, loss -, compute time 12.32\n",
      "episode 3848, reward 1774, memory_length 2000, epsilon 0.03133, total time 728, loss -, compute time 13.53\n",
      "episode 3849, reward 1455, memory_length 2000, epsilon 0.0313, total time 727, loss -, compute time 12.85\n",
      "episode 3850, reward 1599, memory_length 2000, epsilon 0.03127, total time 722, loss -, compute time 13.99\n",
      "Saving model for episode: 3850\n",
      "episode 3851, reward 1463, memory_length 2000, epsilon 0.03124, total time 724, loss -, compute time 12.88\n",
      "episode 3852, reward 1688, memory_length 2000, epsilon 0.03122, total time 726, loss -, compute time 13.26\n",
      "episode 3853, reward 1836, memory_length 2000, epsilon 0.03119, total time 726, loss -, compute time 12.87\n",
      "episode 3854, reward 1542, memory_length 2000, epsilon 0.03116, total time 721, loss -, compute time 14.28\n",
      "episode 3855, reward 1580, memory_length 2000, epsilon 0.03113, total time 723, loss -, compute time 13.11\n",
      "episode 3856, reward 1760, memory_length 2000, epsilon 0.0311, total time 723, loss -, compute time 11.64\n",
      "episode 3857, reward 1818, memory_length 2000, epsilon 0.03108, total time 725, loss -, compute time 12.1\n",
      "episode 3858, reward 1742, memory_length 2000, epsilon 0.03105, total time 723, loss -, compute time 12.0\n",
      "episode 3859, reward 1586, memory_length 2000, epsilon 0.03102, total time 729, loss -, compute time 14.38\n",
      "episode 3860, reward 1804, memory_length 2000, epsilon 0.03099, total time 725, loss -, compute time 13.14\n",
      "Saving model for episode: 3860\n",
      "episode 3861, reward 1564, memory_length 2000, epsilon 0.03096, total time 726, loss -, compute time 12.32\n",
      "episode 3862, reward 1691, memory_length 2000, epsilon 0.03094, total time 721, loss -, compute time 13.54\n",
      "episode 3863, reward 1680, memory_length 2000, epsilon 0.03091, total time 725, loss -, compute time 11.02\n",
      "episode 3864, reward 1679, memory_length 2000, epsilon 0.03088, total time 723, loss -, compute time 13.66\n",
      "episode 3865, reward 1570, memory_length 2000, epsilon 0.03085, total time 723, loss -, compute time 13.24\n",
      "episode 3866, reward 1581, memory_length 2000, epsilon 0.03083, total time 726, loss -, compute time 11.97\n",
      "episode 3867, reward 1725, memory_length 2000, epsilon 0.0308, total time 722, loss -, compute time 13.24\n",
      "episode 3868, reward 1379, memory_length 2000, epsilon 0.03077, total time 727, loss -, compute time 12.99\n",
      "episode 3869, reward 1841, memory_length 2000, epsilon 0.03074, total time 721, loss -, compute time 14.3\n",
      "episode 3870, reward 1703, memory_length 2000, epsilon 0.03072, total time 725, loss -, compute time 13.55\n",
      "Saving model for episode: 3870\n",
      "episode 3871, reward 1833, memory_length 2000, epsilon 0.03069, total time 725, loss -, compute time 12.76\n",
      "episode 3872, reward 1620, memory_length 2000, epsilon 0.03066, total time 721, loss -, compute time 14.37\n",
      "episode 3873, reward 1797, memory_length 2000, epsilon 0.03063, total time 725, loss -, compute time 13.81\n",
      "episode 3874, reward 1423, memory_length 2000, epsilon 0.0306, total time 729, loss -, compute time 13.92\n",
      "episode 3875, reward 1935, memory_length 2000, epsilon 0.03058, total time 721, loss -, compute time 13.85\n",
      "episode 3876, reward 2036, memory_length 2000, epsilon 0.03055, total time 726, loss -, compute time 12.75\n",
      "episode 3877, reward 1794, memory_length 2000, epsilon 0.03052, total time 721, loss -, compute time 14.12\n",
      "episode 3878, reward 1576, memory_length 2000, epsilon 0.03049, total time 722, loss -, compute time 13.33\n",
      "episode 3879, reward 1367, memory_length 2000, epsilon 0.03047, total time 727, loss -, compute time 13.92\n",
      "episode 3880, reward 2012, memory_length 2000, epsilon 0.03044, total time 725, loss -, compute time 13.96\n",
      "Saving model for episode: 3880\n",
      "episode 3881, reward 1815, memory_length 2000, epsilon 0.03041, total time 721, loss -, compute time 11.8\n",
      "episode 3882, reward 1762, memory_length 2000, epsilon 0.03039, total time 725, loss -, compute time 13.34\n",
      "episode 3883, reward 1589, memory_length 2000, epsilon 0.03036, total time 721, loss -, compute time 11.61\n",
      "episode 3884, reward 1820, memory_length 2000, epsilon 0.03033, total time 727, loss -, compute time 13.1\n",
      "episode 3885, reward 1634, memory_length 2000, epsilon 0.0303, total time 725, loss -, compute time 13.04\n",
      "episode 3886, reward 1854, memory_length 2000, epsilon 0.03028, total time 725, loss -, compute time 12.06\n",
      "episode 3887, reward 1610, memory_length 2000, epsilon 0.03025, total time 721, loss -, compute time 11.64\n",
      "episode 3888, reward 1873, memory_length 2000, epsilon 0.03022, total time 724, loss -, compute time 12.55\n",
      "episode 3889, reward 1674, memory_length 2000, epsilon 0.03019, total time 721, loss -, compute time 13.11\n",
      "episode 3890, reward 1582, memory_length 2000, epsilon 0.03017, total time 723, loss -, compute time 11.94\n",
      "Saving model for episode: 3890\n",
      "episode 3891, reward 1638, memory_length 2000, epsilon 0.03014, total time 728, loss -, compute time 14.68\n",
      "episode 3892, reward 1780, memory_length 2000, epsilon 0.03011, total time 724, loss -, compute time 12.44\n",
      "episode 3893, reward 1536, memory_length 2000, epsilon 0.03009, total time 723, loss -, compute time 11.7\n",
      "episode 3894, reward 1892, memory_length 2000, epsilon 0.03006, total time 722, loss -, compute time 13.19\n",
      "episode 3895, reward 1844, memory_length 2000, epsilon 0.03003, total time 722, loss -, compute time 12.83\n",
      "episode 3896, reward 1641, memory_length 2000, epsilon 0.03, total time 723, loss -, compute time 11.87\n",
      "episode 3897, reward 1482, memory_length 2000, epsilon 0.02998, total time 722, loss -, compute time 13.47\n",
      "episode 3898, reward 1229, memory_length 2000, epsilon 0.02995, total time 730, loss -, compute time 11.04\n",
      "episode 3899, reward 1841, memory_length 2000, epsilon 0.02992, total time 724, loss -, compute time 14.33\n",
      "episode 3900, reward 1684, memory_length 2000, epsilon 0.0299, total time 724, loss [2.4151995182037354], compute time 12.82\n",
      "Saving model for episode: 3900\n",
      "episode 3901, reward 1494, memory_length 2000, epsilon 0.02987, total time 724, loss -, compute time 13.65\n",
      "episode 3902, reward 1522, memory_length 2000, epsilon 0.02984, total time 724, loss -, compute time 13.46\n",
      "episode 3903, reward 1504, memory_length 2000, epsilon 0.02982, total time 730, loss -, compute time 11.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3904, reward 1416, memory_length 2000, epsilon 0.02979, total time 724, loss -, compute time 13.09\n",
      "episode 3905, reward 1499, memory_length 2000, epsilon 0.02976, total time 725, loss -, compute time 12.93\n",
      "episode 3906, reward 1447, memory_length 2000, epsilon 0.02974, total time 722, loss -, compute time 12.31\n",
      "episode 3907, reward 1734, memory_length 2000, epsilon 0.02971, total time 723, loss -, compute time 12.29\n",
      "episode 3908, reward 1649, memory_length 2000, epsilon 0.02968, total time 725, loss -, compute time 12.21\n",
      "episode 3909, reward 1868, memory_length 2000, epsilon 0.02966, total time 727, loss -, compute time 12.53\n",
      "episode 3910, reward 1638, memory_length 2000, epsilon 0.02963, total time 725, loss -, compute time 13.33\n",
      "Saving model for episode: 3910\n",
      "episode 3911, reward 1649, memory_length 2000, epsilon 0.0296, total time 730, loss -, compute time 12.39\n",
      "episode 3912, reward 1569, memory_length 2000, epsilon 0.02958, total time 727, loss -, compute time 13.4\n",
      "episode 3913, reward 1558, memory_length 2000, epsilon 0.02955, total time 725, loss -, compute time 12.82\n",
      "episode 3914, reward 1479, memory_length 2000, epsilon 0.02952, total time 721, loss -, compute time 14.83\n",
      "episode 3915, reward 1387, memory_length 2000, epsilon 0.0295, total time 730, loss -, compute time 12.03\n",
      "episode 3916, reward 1833, memory_length 2000, epsilon 0.02947, total time 722, loss [1.6303409337997437], compute time 11.81\n",
      "episode 3917, reward 1746, memory_length 2000, epsilon 0.02944, total time 732, loss [1.6725633144378662], compute time 12.79\n",
      "episode 3918, reward 1433, memory_length 2000, epsilon 0.02942, total time 722, loss -, compute time 14.07\n",
      "episode 3919, reward 1648, memory_length 2000, epsilon 0.02939, total time 726, loss -, compute time 14.86\n",
      "episode 3920, reward 1461, memory_length 2000, epsilon 0.02936, total time 722, loss -, compute time 13.79\n",
      "Saving model for episode: 3920\n",
      "episode 3921, reward 1532, memory_length 2000, epsilon 0.02934, total time 724, loss -, compute time 13.1\n",
      "episode 3922, reward 1686, memory_length 2000, epsilon 0.02931, total time 722, loss -, compute time 13.73\n",
      "episode 3923, reward 1379, memory_length 2000, epsilon 0.02928, total time 727, loss -, compute time 14.57\n",
      "episode 3924, reward 1548, memory_length 2000, epsilon 0.02926, total time 729, loss -, compute time 14.14\n",
      "episode 3925, reward 1662, memory_length 2000, epsilon 0.02923, total time 721, loss -, compute time 14.06\n",
      "episode 3926, reward 1667, memory_length 2000, epsilon 0.02921, total time 726, loss -, compute time 13.36\n",
      "episode 3927, reward 1583, memory_length 2000, epsilon 0.02918, total time 722, loss -, compute time 11.28\n",
      "episode 3928, reward 1883, memory_length 2000, epsilon 0.02915, total time 723, loss [1.9865174293518066], compute time 13.12\n",
      "episode 3929, reward 1773, memory_length 2000, epsilon 0.02913, total time 728, loss -, compute time 11.89\n",
      "episode 3930, reward 1596, memory_length 2000, epsilon 0.0291, total time 724, loss -, compute time 12.91\n",
      "Saving model for episode: 3930\n",
      "episode 3931, reward 1759, memory_length 2000, epsilon 0.02907, total time 725, loss -, compute time 14.06\n",
      "episode 3932, reward 1632, memory_length 2000, epsilon 0.02905, total time 724, loss -, compute time 13.14\n",
      "episode 3933, reward 1704, memory_length 2000, epsilon 0.02902, total time 726, loss -, compute time 12.37\n",
      "episode 3934, reward 1495, memory_length 2000, epsilon 0.029, total time 727, loss -, compute time 13.1\n",
      "episode 3935, reward 1616, memory_length 2000, epsilon 0.02897, total time 726, loss [2.543330192565918], compute time 12.23\n",
      "episode 3936, reward 1638, memory_length 2000, epsilon 0.02894, total time 722, loss -, compute time 11.73\n",
      "episode 3937, reward 1563, memory_length 2000, epsilon 0.02892, total time 722, loss -, compute time 13.71\n",
      "episode 3938, reward 1838, memory_length 2000, epsilon 0.02889, total time 722, loss -, compute time 11.93\n",
      "episode 3939, reward 1459, memory_length 2000, epsilon 0.02887, total time 724, loss -, compute time 12.27\n",
      "episode 3940, reward 1595, memory_length 2000, epsilon 0.02884, total time 725, loss -, compute time 13.31\n",
      "Saving model for episode: 3940\n",
      "episode 3941, reward 1576, memory_length 2000, epsilon 0.02881, total time 724, loss -, compute time 12.76\n",
      "episode 3942, reward 1504, memory_length 2000, epsilon 0.02879, total time 723, loss -, compute time 13.39\n",
      "episode 3943, reward 1658, memory_length 2000, epsilon 0.02876, total time 722, loss -, compute time 12.12\n",
      "episode 3944, reward 1462, memory_length 2000, epsilon 0.02874, total time 726, loss -, compute time 14.24\n",
      "episode 3945, reward 1612, memory_length 2000, epsilon 0.02871, total time 725, loss -, compute time 12.09\n",
      "episode 3946, reward 1586, memory_length 2000, epsilon 0.02868, total time 727, loss -, compute time 12.26\n",
      "episode 3947, reward 1474, memory_length 2000, epsilon 0.02866, total time 727, loss -, compute time 13.46\n",
      "episode 3948, reward 1768, memory_length 2000, epsilon 0.02863, total time 722, loss -, compute time 11.27\n",
      "episode 3949, reward 1153, memory_length 2000, epsilon 0.02861, total time 725, loss -, compute time 14.26\n",
      "episode 3950, reward 1537, memory_length 2000, epsilon 0.02858, total time 722, loss -, compute time 11.04\n",
      "Saving model for episode: 3950\n",
      "episode 3951, reward 1681, memory_length 2000, epsilon 0.02856, total time 725, loss -, compute time 13.24\n",
      "episode 3952, reward 1851, memory_length 2000, epsilon 0.02853, total time 726, loss -, compute time 11.23\n",
      "episode 3953, reward 1685, memory_length 2000, epsilon 0.0285, total time 726, loss -, compute time 12.79\n",
      "episode 3954, reward 1566, memory_length 2000, epsilon 0.02848, total time 728, loss -, compute time 12.65\n",
      "episode 3955, reward 1900, memory_length 2000, epsilon 0.02845, total time 724, loss -, compute time 12.97\n",
      "episode 3956, reward 1927, memory_length 2000, epsilon 0.02843, total time 723, loss -, compute time 12.29\n",
      "episode 3957, reward 1528, memory_length 2000, epsilon 0.0284, total time 721, loss -, compute time 12.11\n",
      "episode 3958, reward 1927, memory_length 2000, epsilon 0.02838, total time 725, loss -, compute time 12.35\n",
      "episode 3959, reward 1557, memory_length 2000, epsilon 0.02835, total time 721, loss -, compute time 12.71\n",
      "episode 3960, reward 1761, memory_length 2000, epsilon 0.02833, total time 724, loss -, compute time 12.84\n",
      "Saving model for episode: 3960\n",
      "episode 3961, reward 2024, memory_length 2000, epsilon 0.0283, total time 721, loss -, compute time 12.67\n",
      "episode 3962, reward 1649, memory_length 2000, epsilon 0.02827, total time 725, loss -, compute time 12.75\n",
      "episode 3963, reward 1864, memory_length 2000, epsilon 0.02825, total time 722, loss -, compute time 13.48\n",
      "episode 3964, reward 1400, memory_length 2000, epsilon 0.02822, total time 726, loss -, compute time 13.76\n",
      "episode 3965, reward 1842, memory_length 2000, epsilon 0.0282, total time 723, loss -, compute time 14.68\n",
      "episode 3966, reward 1559, memory_length 2000, epsilon 0.02817, total time 723, loss -, compute time 11.66\n",
      "episode 3967, reward 1501, memory_length 2000, epsilon 0.02815, total time 727, loss -, compute time 12.99\n",
      "episode 3968, reward 1432, memory_length 2000, epsilon 0.02812, total time 722, loss -, compute time 13.47\n",
      "episode 3969, reward 1686, memory_length 2000, epsilon 0.0281, total time 723, loss -, compute time 13.58\n",
      "episode 3970, reward 1522, memory_length 2000, epsilon 0.02807, total time 721, loss -, compute time 13.26\n",
      "Saving model for episode: 3970\n",
      "episode 3971, reward 1599, memory_length 2000, epsilon 0.02805, total time 724, loss -, compute time 13.17\n",
      "episode 3972, reward 1692, memory_length 2000, epsilon 0.02802, total time 724, loss -, compute time 12.76\n",
      "episode 3973, reward 1829, memory_length 2000, epsilon 0.028, total time 722, loss -, compute time 13.76\n",
      "episode 3974, reward 1638, memory_length 2000, epsilon 0.02797, total time 731, loss -, compute time 13.74\n",
      "episode 3975, reward 1701, memory_length 2000, epsilon 0.02795, total time 721, loss -, compute time 14.41\n",
      "episode 3976, reward 1963, memory_length 2000, epsilon 0.02792, total time 724, loss -, compute time 13.74\n",
      "episode 3977, reward 1477, memory_length 2000, epsilon 0.0279, total time 724, loss -, compute time 13.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3978, reward 1485, memory_length 2000, epsilon 0.02787, total time 728, loss -, compute time 14.66\n",
      "episode 3979, reward 1631, memory_length 2000, epsilon 0.02785, total time 724, loss -, compute time 13.54\n",
      "episode 3980, reward 1837, memory_length 2000, epsilon 0.02782, total time 724, loss -, compute time 12.73\n",
      "Saving model for episode: 3980\n",
      "episode 3981, reward 1510, memory_length 2000, epsilon 0.02779, total time 725, loss -, compute time 11.95\n",
      "episode 3982, reward 1554, memory_length 2000, epsilon 0.02777, total time 723, loss -, compute time 11.53\n",
      "episode 3983, reward 1797, memory_length 2000, epsilon 0.02774, total time 727, loss -, compute time 13.02\n",
      "episode 3984, reward 1553, memory_length 2000, epsilon 0.02772, total time 730, loss -, compute time 15.11\n",
      "episode 3985, reward 1845, memory_length 2000, epsilon 0.0277, total time 721, loss -, compute time 14.98\n",
      "episode 3986, reward 1608, memory_length 2000, epsilon 0.02767, total time 721, loss -, compute time 13.49\n",
      "episode 3987, reward 1710, memory_length 2000, epsilon 0.02765, total time 722, loss -, compute time 12.55\n",
      "episode 3988, reward 1417, memory_length 2000, epsilon 0.02762, total time 722, loss -, compute time 13.21\n",
      "episode 3989, reward 1797, memory_length 2000, epsilon 0.0276, total time 722, loss -, compute time 13.17\n",
      "episode 3990, reward 1513, memory_length 2000, epsilon 0.02757, total time 722, loss -, compute time 14.55\n",
      "Saving model for episode: 3990\n",
      "episode 3991, reward 1755, memory_length 2000, epsilon 0.02755, total time 729, loss -, compute time 12.44\n",
      "episode 3992, reward 1879, memory_length 2000, epsilon 0.02752, total time 721, loss -, compute time 13.03\n",
      "episode 3993, reward 1616, memory_length 2000, epsilon 0.0275, total time 728, loss -, compute time 12.19\n",
      "episode 3994, reward 1487, memory_length 2000, epsilon 0.02747, total time 727, loss -, compute time 13.82\n",
      "episode 3995, reward 1680, memory_length 2000, epsilon 0.02745, total time 725, loss -, compute time 12.76\n",
      "episode 3996, reward 1814, memory_length 2000, epsilon 0.02742, total time 725, loss -, compute time 13.31\n",
      "episode 3997, reward 1521, memory_length 2000, epsilon 0.0274, total time 728, loss -, compute time 12.73\n",
      "episode 3998, reward 1608, memory_length 2000, epsilon 0.02737, total time 725, loss -, compute time 14.11\n",
      "episode 3999, reward 1482, memory_length 2000, epsilon 0.02735, total time 722, loss -, compute time 12.24\n",
      "episode 4000, reward 1720, memory_length 2000, epsilon 0.02732, total time 724, loss -, compute time 14.82\n",
      "Saving model for episode: 4000\n",
      "episode 4001, reward 1747, memory_length 2000, epsilon 0.0273, total time 730, loss -, compute time 13.7\n",
      "episode 4002, reward 1927, memory_length 2000, epsilon 0.02727, total time 722, loss -, compute time 13.4\n",
      "episode 4003, reward 1774, memory_length 2000, epsilon 0.02725, total time 721, loss -, compute time 13.19\n",
      "episode 4004, reward 1761, memory_length 2000, epsilon 0.02723, total time 730, loss -, compute time 15.46\n",
      "episode 4005, reward 1810, memory_length 2000, epsilon 0.0272, total time 722, loss -, compute time 12.18\n",
      "episode 4006, reward 1508, memory_length 2000, epsilon 0.02718, total time 723, loss -, compute time 12.89\n",
      "episode 4007, reward 1884, memory_length 2000, epsilon 0.02715, total time 722, loss -, compute time 13.18\n",
      "episode 4008, reward 1570, memory_length 2000, epsilon 0.02713, total time 721, loss -, compute time 12.24\n",
      "episode 4009, reward 1570, memory_length 2000, epsilon 0.0271, total time 723, loss -, compute time 14.14\n",
      "episode 4010, reward 1675, memory_length 2000, epsilon 0.02708, total time 721, loss -, compute time 12.24\n",
      "Saving model for episode: 4010\n",
      "episode 4011, reward 1769, memory_length 2000, epsilon 0.02705, total time 728, loss -, compute time 12.68\n",
      "episode 4012, reward 1761, memory_length 2000, epsilon 0.02703, total time 721, loss -, compute time 13.47\n",
      "episode 4013, reward 1568, memory_length 2000, epsilon 0.02701, total time 729, loss -, compute time 13.19\n",
      "episode 4014, reward 1714, memory_length 2000, epsilon 0.02698, total time 721, loss -, compute time 13.47\n",
      "episode 4015, reward 1901, memory_length 2000, epsilon 0.02696, total time 722, loss -, compute time 11.47\n",
      "episode 4016, reward 1334, memory_length 2000, epsilon 0.02693, total time 723, loss -, compute time 12.61\n",
      "episode 4017, reward 1958, memory_length 2000, epsilon 0.02691, total time 723, loss [3.8511877059936523], compute time 11.93\n",
      "episode 4018, reward 1693, memory_length 2000, epsilon 0.02688, total time 722, loss -, compute time 11.99\n",
      "episode 4019, reward 1820, memory_length 2000, epsilon 0.02686, total time 724, loss -, compute time 14.32\n",
      "episode 4020, reward 1474, memory_length 2000, epsilon 0.02684, total time 725, loss -, compute time 13.25\n",
      "Saving model for episode: 4020\n",
      "episode 4021, reward 1498, memory_length 2000, epsilon 0.02681, total time 729, loss -, compute time 12.35\n",
      "episode 4022, reward 1449, memory_length 2000, epsilon 0.02679, total time 721, loss -, compute time 12.92\n",
      "episode 4023, reward 1724, memory_length 2000, epsilon 0.02676, total time 728, loss -, compute time 12.91\n",
      "episode 4024, reward 1535, memory_length 2000, epsilon 0.02674, total time 727, loss -, compute time 13.54\n",
      "episode 4025, reward 1729, memory_length 2000, epsilon 0.02672, total time 727, loss -, compute time 13.91\n",
      "episode 4026, reward 1567, memory_length 2000, epsilon 0.02669, total time 725, loss -, compute time 13.27\n",
      "episode 4027, reward 1833, memory_length 2000, epsilon 0.02667, total time 723, loss -, compute time 14.33\n",
      "episode 4028, reward 1608, memory_length 2000, epsilon 0.02664, total time 724, loss -, compute time 14.37\n",
      "episode 4029, reward 1866, memory_length 2000, epsilon 0.02662, total time 724, loss -, compute time 14.05\n",
      "episode 4030, reward 1603, memory_length 2000, epsilon 0.0266, total time 721, loss -, compute time 12.9\n",
      "Saving model for episode: 4030\n",
      "episode 4031, reward 1738, memory_length 2000, epsilon 0.02657, total time 723, loss -, compute time 11.91\n",
      "episode 4032, reward 1789, memory_length 2000, epsilon 0.02655, total time 721, loss -, compute time 14.13\n",
      "episode 4033, reward 1807, memory_length 2000, epsilon 0.02652, total time 722, loss -, compute time 12.74\n",
      "episode 4034, reward 1720, memory_length 2000, epsilon 0.0265, total time 725, loss -, compute time 12.78\n",
      "episode 4035, reward 1663, memory_length 2000, epsilon 0.02648, total time 724, loss -, compute time 13.71\n",
      "episode 4036, reward 1693, memory_length 2000, epsilon 0.02645, total time 724, loss -, compute time 12.16\n",
      "episode 4037, reward 1809, memory_length 2000, epsilon 0.02643, total time 721, loss -, compute time 14.17\n",
      "episode 4038, reward 1814, memory_length 2000, epsilon 0.02641, total time 726, loss -, compute time 13.29\n",
      "episode 4039, reward 1592, memory_length 2000, epsilon 0.02638, total time 721, loss -, compute time 12.35\n",
      "episode 4040, reward 1467, memory_length 2000, epsilon 0.02636, total time 726, loss -, compute time 11.94\n",
      "Saving model for episode: 4040\n",
      "episode 4041, reward 1782, memory_length 2000, epsilon 0.02633, total time 725, loss -, compute time 11.37\n",
      "episode 4042, reward 1611, memory_length 2000, epsilon 0.02631, total time 721, loss -, compute time 12.66\n",
      "episode 4043, reward 1373, memory_length 2000, epsilon 0.02629, total time 729, loss -, compute time 12.85\n",
      "episode 4044, reward 1494, memory_length 2000, epsilon 0.02626, total time 726, loss -, compute time 13.51\n",
      "episode 4045, reward 1629, memory_length 2000, epsilon 0.02624, total time 726, loss -, compute time 11.73\n",
      "episode 4046, reward 1547, memory_length 2000, epsilon 0.02622, total time 721, loss -, compute time 12.06\n",
      "episode 4047, reward 1341, memory_length 2000, epsilon 0.02619, total time 725, loss -, compute time 13.26\n",
      "episode 4048, reward 1818, memory_length 2000, epsilon 0.02617, total time 725, loss -, compute time 13.43\n",
      "episode 4049, reward 1687, memory_length 2000, epsilon 0.02614, total time 721, loss -, compute time 13.76\n",
      "episode 4050, reward 1523, memory_length 2000, epsilon 0.02612, total time 728, loss -, compute time 12.18\n",
      "Saving model for episode: 4050\n",
      "episode 4051, reward 1614, memory_length 2000, epsilon 0.0261, total time 721, loss -, compute time 14.07\n",
      "episode 4052, reward 1611, memory_length 2000, epsilon 0.02607, total time 727, loss -, compute time 14.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4053, reward 1596, memory_length 2000, epsilon 0.02605, total time 726, loss -, compute time 12.41\n",
      "episode 4054, reward 1698, memory_length 2000, epsilon 0.02603, total time 723, loss -, compute time 12.7\n",
      "episode 4055, reward 1733, memory_length 2000, epsilon 0.026, total time 725, loss -, compute time 12.0\n",
      "episode 4056, reward 1503, memory_length 2000, epsilon 0.02598, total time 726, loss -, compute time 12.15\n",
      "episode 4057, reward 1391, memory_length 2000, epsilon 0.02596, total time 722, loss -, compute time 14.13\n",
      "episode 4058, reward 1429, memory_length 2000, epsilon 0.02593, total time 724, loss -, compute time 12.91\n",
      "episode 4059, reward 1580, memory_length 2000, epsilon 0.02591, total time 723, loss [1.2945544719696045], compute time 13.45\n",
      "episode 4060, reward 1627, memory_length 2000, epsilon 0.02589, total time 726, loss -, compute time 13.87\n",
      "Saving model for episode: 4060\n",
      "episode 4061, reward 1581, memory_length 2000, epsilon 0.02586, total time 726, loss -, compute time 13.24\n",
      "episode 4062, reward 1634, memory_length 2000, epsilon 0.02584, total time 722, loss -, compute time 13.11\n",
      "episode 4063, reward 1782, memory_length 2000, epsilon 0.02582, total time 729, loss -, compute time 11.9\n",
      "episode 4064, reward 1834, memory_length 2000, epsilon 0.02579, total time 724, loss -, compute time 13.05\n",
      "episode 4065, reward 1855, memory_length 2000, epsilon 0.02577, total time 728, loss -, compute time 12.85\n",
      "episode 4066, reward 1779, memory_length 2000, epsilon 0.02575, total time 723, loss -, compute time 12.66\n",
      "episode 4067, reward 1535, memory_length 2000, epsilon 0.02572, total time 725, loss -, compute time 14.17\n",
      "episode 4068, reward 1384, memory_length 2000, epsilon 0.0257, total time 726, loss -, compute time 14.75\n",
      "episode 4069, reward 1699, memory_length 2000, epsilon 0.02568, total time 721, loss -, compute time 12.48\n",
      "episode 4070, reward 1440, memory_length 2000, epsilon 0.02566, total time 731, loss -, compute time 11.99\n",
      "Saving model for episode: 4070\n",
      "episode 4071, reward 1699, memory_length 2000, epsilon 0.02563, total time 724, loss [4.077903747558594], compute time 12.03\n",
      "episode 4072, reward 1591, memory_length 2000, epsilon 0.02561, total time 721, loss -, compute time 12.14\n",
      "episode 4073, reward 1571, memory_length 2000, epsilon 0.02559, total time 730, loss -, compute time 13.13\n",
      "episode 4074, reward 1548, memory_length 2000, epsilon 0.02556, total time 732, loss -, compute time 13.33\n",
      "episode 4075, reward 1603, memory_length 2000, epsilon 0.02554, total time 721, loss -, compute time 13.27\n",
      "episode 4076, reward 1324, memory_length 2000, epsilon 0.02552, total time 725, loss -, compute time 14.85\n",
      "episode 4077, reward 1397, memory_length 2000, epsilon 0.02549, total time 721, loss -, compute time 13.59\n",
      "episode 4078, reward 1844, memory_length 2000, epsilon 0.02547, total time 721, loss -, compute time 13.03\n",
      "episode 4079, reward 1648, memory_length 2000, epsilon 0.02545, total time 723, loss -, compute time 13.4\n",
      "episode 4080, reward 1391, memory_length 2000, epsilon 0.02543, total time 726, loss -, compute time 11.53\n",
      "Saving model for episode: 4080\n",
      "episode 4081, reward 1835, memory_length 2000, epsilon 0.0254, total time 724, loss -, compute time 11.33\n",
      "episode 4082, reward 1514, memory_length 2000, epsilon 0.02538, total time 726, loss -, compute time 13.46\n",
      "episode 4083, reward 1731, memory_length 2000, epsilon 0.02536, total time 723, loss -, compute time 12.39\n",
      "episode 4084, reward 1455, memory_length 2000, epsilon 0.02533, total time 725, loss -, compute time 13.11\n",
      "episode 4085, reward 1496, memory_length 2000, epsilon 0.02531, total time 725, loss -, compute time 14.11\n",
      "episode 4086, reward 1593, memory_length 2000, epsilon 0.02529, total time 722, loss -, compute time 11.98\n",
      "episode 4087, reward 1746, memory_length 2000, epsilon 0.02527, total time 723, loss -, compute time 13.59\n",
      "episode 4088, reward 1699, memory_length 2000, epsilon 0.02524, total time 722, loss -, compute time 12.79\n",
      "episode 4089, reward 1583, memory_length 2000, epsilon 0.02522, total time 722, loss -, compute time 14.03\n",
      "episode 4090, reward 1555, memory_length 2000, epsilon 0.0252, total time 726, loss -, compute time 14.4\n",
      "Saving model for episode: 4090\n",
      "episode 4091, reward 1755, memory_length 2000, epsilon 0.02518, total time 725, loss -, compute time 13.62\n",
      "episode 4092, reward 2025, memory_length 2000, epsilon 0.02515, total time 733, loss -, compute time 12.9\n",
      "episode 4093, reward 1630, memory_length 2000, epsilon 0.02513, total time 723, loss -, compute time 13.51\n",
      "episode 4094, reward 1652, memory_length 2000, epsilon 0.02511, total time 728, loss -, compute time 11.56\n",
      "episode 4095, reward 1820, memory_length 2000, epsilon 0.02508, total time 722, loss -, compute time 11.52\n",
      "episode 4096, reward 1738, memory_length 2000, epsilon 0.02506, total time 721, loss -, compute time 12.35\n",
      "episode 4097, reward 1690, memory_length 2000, epsilon 0.02504, total time 722, loss -, compute time 12.45\n",
      "episode 4098, reward 1883, memory_length 2000, epsilon 0.02502, total time 727, loss -, compute time 12.92\n",
      "episode 4099, reward 1927, memory_length 2000, epsilon 0.02499, total time 721, loss -, compute time 11.94\n",
      "episode 4100, reward 1818, memory_length 2000, epsilon 0.02497, total time 723, loss -, compute time 13.52\n",
      "Saving model for episode: 4100\n",
      "episode 4101, reward 1648, memory_length 2000, epsilon 0.02495, total time 727, loss -, compute time 11.51\n",
      "episode 4102, reward 1541, memory_length 2000, epsilon 0.02493, total time 723, loss -, compute time 12.89\n",
      "episode 4103, reward 1747, memory_length 2000, epsilon 0.0249, total time 728, loss -, compute time 12.8\n",
      "episode 4104, reward 1727, memory_length 2000, epsilon 0.02488, total time 722, loss -, compute time 13.34\n",
      "episode 4105, reward 1870, memory_length 2000, epsilon 0.02486, total time 722, loss -, compute time 13.66\n",
      "episode 4106, reward 1512, memory_length 2000, epsilon 0.02484, total time 726, loss -, compute time 13.16\n",
      "episode 4107, reward 1855, memory_length 2000, epsilon 0.02482, total time 725, loss -, compute time 14.5\n",
      "episode 4108, reward 1765, memory_length 2000, epsilon 0.02479, total time 725, loss -, compute time 14.5\n",
      "episode 4109, reward 1719, memory_length 2000, epsilon 0.02477, total time 721, loss -, compute time 12.79\n",
      "episode 4110, reward 1731, memory_length 2000, epsilon 0.02475, total time 721, loss -, compute time 13.05\n",
      "Saving model for episode: 4110\n",
      "episode 4111, reward 1922, memory_length 2000, epsilon 0.02473, total time 724, loss -, compute time 12.18\n",
      "episode 4112, reward 1809, memory_length 2000, epsilon 0.0247, total time 730, loss -, compute time 12.51\n",
      "episode 4113, reward 1669, memory_length 2000, epsilon 0.02468, total time 722, loss -, compute time 11.8\n",
      "episode 4114, reward 1511, memory_length 2000, epsilon 0.02466, total time 725, loss -, compute time 13.15\n",
      "episode 4115, reward 1548, memory_length 2000, epsilon 0.02464, total time 725, loss -, compute time 14.15\n",
      "episode 4116, reward 1594, memory_length 2000, epsilon 0.02461, total time 723, loss -, compute time 12.62\n",
      "episode 4117, reward 1640, memory_length 2000, epsilon 0.02459, total time 723, loss -, compute time 12.16\n",
      "episode 4118, reward 1770, memory_length 2000, epsilon 0.02457, total time 722, loss -, compute time 12.36\n",
      "episode 4119, reward 1582, memory_length 2000, epsilon 0.02455, total time 724, loss -, compute time 12.8\n",
      "episode 4120, reward 1499, memory_length 2000, epsilon 0.02453, total time 721, loss -, compute time 12.93\n",
      "Saving model for episode: 4120\n",
      "episode 4121, reward 1710, memory_length 2000, epsilon 0.0245, total time 728, loss -, compute time 13.07\n",
      "episode 4122, reward 1243, memory_length 2000, epsilon 0.02448, total time 725, loss -, compute time 12.77\n",
      "episode 4123, reward 1694, memory_length 2000, epsilon 0.02446, total time 722, loss -, compute time 12.28\n",
      "episode 4124, reward 1589, memory_length 2000, epsilon 0.02444, total time 724, loss -, compute time 12.3\n",
      "episode 4125, reward 1570, memory_length 2000, epsilon 0.02442, total time 722, loss -, compute time 12.02\n",
      "episode 4126, reward 1739, memory_length 2000, epsilon 0.02439, total time 727, loss -, compute time 11.92\n",
      "episode 4127, reward 1692, memory_length 2000, epsilon 0.02437, total time 722, loss -, compute time 11.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4128, reward 1469, memory_length 2000, epsilon 0.02435, total time 723, loss -, compute time 13.83\n",
      "episode 4129, reward 1674, memory_length 2000, epsilon 0.02433, total time 724, loss -, compute time 12.89\n",
      "episode 4130, reward 1781, memory_length 2000, epsilon 0.02431, total time 728, loss -, compute time 11.84\n",
      "Saving model for episode: 4130\n",
      "episode 4131, reward 1737, memory_length 2000, epsilon 0.02428, total time 723, loss -, compute time 13.33\n",
      "episode 4132, reward 1445, memory_length 2000, epsilon 0.02426, total time 724, loss -, compute time 12.75\n",
      "episode 4133, reward 1905, memory_length 2000, epsilon 0.02424, total time 730, loss -, compute time 12.27\n",
      "episode 4134, reward 1376, memory_length 2000, epsilon 0.02422, total time 721, loss -, compute time 12.65\n",
      "episode 4135, reward 1732, memory_length 2000, epsilon 0.0242, total time 723, loss -, compute time 13.59\n",
      "episode 4136, reward 2014, memory_length 2000, epsilon 0.02418, total time 722, loss -, compute time 12.7\n",
      "episode 4137, reward 1355, memory_length 2000, epsilon 0.02415, total time 730, loss -, compute time 12.71\n",
      "episode 4138, reward 1453, memory_length 2000, epsilon 0.02413, total time 723, loss -, compute time 13.69\n",
      "episode 4139, reward 1744, memory_length 2000, epsilon 0.02411, total time 722, loss -, compute time 12.84\n",
      "episode 4140, reward 1817, memory_length 2000, epsilon 0.02409, total time 725, loss -, compute time 13.36\n",
      "Saving model for episode: 4140\n",
      "episode 4141, reward 1508, memory_length 2000, epsilon 0.02407, total time 727, loss -, compute time 12.9\n",
      "episode 4142, reward 1719, memory_length 2000, epsilon 0.02405, total time 722, loss -, compute time 13.72\n",
      "episode 4143, reward 1646, memory_length 2000, epsilon 0.02402, total time 722, loss -, compute time 13.11\n",
      "episode 4144, reward 1654, memory_length 2000, epsilon 0.024, total time 723, loss -, compute time 12.94\n",
      "episode 4145, reward 1630, memory_length 2000, epsilon 0.02398, total time 722, loss -, compute time 12.49\n",
      "episode 4146, reward 1802, memory_length 2000, epsilon 0.02396, total time 725, loss -, compute time 13.22\n",
      "episode 4147, reward 1689, memory_length 2000, epsilon 0.02394, total time 723, loss -, compute time 15.0\n",
      "episode 4148, reward 1585, memory_length 2000, epsilon 0.02392, total time 725, loss -, compute time 12.97\n",
      "episode 4149, reward 1674, memory_length 2000, epsilon 0.02389, total time 721, loss -, compute time 13.99\n",
      "episode 4150, reward 1692, memory_length 2000, epsilon 0.02387, total time 727, loss -, compute time 12.68\n",
      "Saving model for episode: 4150\n",
      "episode 4151, reward 1667, memory_length 2000, epsilon 0.02385, total time 725, loss -, compute time 14.39\n",
      "episode 4152, reward 1734, memory_length 2000, epsilon 0.02383, total time 724, loss -, compute time 12.2\n",
      "episode 4153, reward 1608, memory_length 2000, epsilon 0.02381, total time 725, loss -, compute time 11.82\n",
      "episode 4154, reward 1505, memory_length 2000, epsilon 0.02379, total time 725, loss -, compute time 13.03\n",
      "episode 4155, reward 1362, memory_length 2000, epsilon 0.02377, total time 725, loss -, compute time 14.7\n",
      "episode 4156, reward 1594, memory_length 2000, epsilon 0.02374, total time 726, loss -, compute time 12.76\n",
      "episode 4157, reward 1953, memory_length 2000, epsilon 0.02372, total time 728, loss -, compute time 12.93\n",
      "episode 4158, reward 1838, memory_length 2000, epsilon 0.0237, total time 727, loss -, compute time 12.37\n",
      "episode 4159, reward 1562, memory_length 2000, epsilon 0.02368, total time 721, loss -, compute time 14.1\n",
      "episode 4160, reward 1632, memory_length 2000, epsilon 0.02366, total time 722, loss -, compute time 13.25\n",
      "Saving model for episode: 4160\n",
      "episode 4161, reward 1378, memory_length 2000, epsilon 0.02364, total time 721, loss -, compute time 12.02\n",
      "episode 4162, reward 1658, memory_length 2000, epsilon 0.02362, total time 725, loss -, compute time 12.11\n",
      "episode 4163, reward 1451, memory_length 2000, epsilon 0.0236, total time 728, loss -, compute time 13.66\n",
      "episode 4164, reward 1621, memory_length 2000, epsilon 0.02357, total time 725, loss -, compute time 11.55\n",
      "episode 4165, reward 1571, memory_length 2000, epsilon 0.02355, total time 726, loss -, compute time 12.23\n",
      "episode 4166, reward 1917, memory_length 2000, epsilon 0.02353, total time 726, loss -, compute time 14.04\n",
      "episode 4167, reward 1597, memory_length 2000, epsilon 0.02351, total time 721, loss -, compute time 13.18\n",
      "episode 4168, reward 1585, memory_length 2000, epsilon 0.02349, total time 730, loss -, compute time 12.89\n",
      "episode 4169, reward 1562, memory_length 2000, epsilon 0.02347, total time 726, loss -, compute time 13.86\n",
      "episode 4170, reward 1810, memory_length 2000, epsilon 0.02345, total time 721, loss -, compute time 12.22\n",
      "Saving model for episode: 4170\n",
      "episode 4171, reward 1855, memory_length 2000, epsilon 0.02343, total time 730, loss -, compute time 13.61\n",
      "episode 4172, reward 1535, memory_length 2000, epsilon 0.02341, total time 727, loss -, compute time 13.56\n",
      "episode 4173, reward 1530, memory_length 2000, epsilon 0.02338, total time 722, loss -, compute time 12.03\n",
      "episode 4174, reward 1852, memory_length 2000, epsilon 0.02336, total time 729, loss -, compute time 13.21\n",
      "episode 4175, reward 1762, memory_length 2000, epsilon 0.02334, total time 724, loss -, compute time 12.34\n",
      "episode 4176, reward 1726, memory_length 2000, epsilon 0.02332, total time 725, loss -, compute time 11.4\n",
      "episode 4177, reward 1521, memory_length 2000, epsilon 0.0233, total time 728, loss -, compute time 14.3\n",
      "episode 4178, reward 1616, memory_length 2000, epsilon 0.02328, total time 729, loss -, compute time 13.81\n",
      "episode 4179, reward 1728, memory_length 2000, epsilon 0.02326, total time 721, loss -, compute time 12.8\n",
      "episode 4180, reward 1541, memory_length 2000, epsilon 0.02324, total time 722, loss -, compute time 14.27\n",
      "Saving model for episode: 4180\n",
      "episode 4181, reward 1598, memory_length 2000, epsilon 0.02322, total time 732, loss -, compute time 12.79\n",
      "episode 4182, reward 1681, memory_length 2000, epsilon 0.0232, total time 725, loss -, compute time 12.63\n",
      "episode 4183, reward 1864, memory_length 2000, epsilon 0.02317, total time 727, loss -, compute time 12.63\n",
      "episode 4184, reward 1707, memory_length 2000, epsilon 0.02315, total time 726, loss -, compute time 13.41\n",
      "episode 4185, reward 1623, memory_length 2000, epsilon 0.02313, total time 725, loss -, compute time 14.36\n",
      "episode 4186, reward 1905, memory_length 2000, epsilon 0.02311, total time 722, loss -, compute time 13.91\n",
      "episode 4187, reward 1661, memory_length 2000, epsilon 0.02309, total time 730, loss -, compute time 13.24\n",
      "episode 4188, reward 1783, memory_length 2000, epsilon 0.02307, total time 726, loss -, compute time 13.47\n",
      "episode 4189, reward 1491, memory_length 2000, epsilon 0.02305, total time 725, loss -, compute time 13.29\n",
      "episode 4190, reward 1491, memory_length 2000, epsilon 0.02303, total time 728, loss -, compute time 12.69\n",
      "Saving model for episode: 4190\n",
      "episode 4191, reward 1860, memory_length 2000, epsilon 0.02301, total time 724, loss -, compute time 13.22\n",
      "episode 4192, reward 1599, memory_length 2000, epsilon 0.02299, total time 727, loss -, compute time 13.48\n",
      "episode 4193, reward 1607, memory_length 2000, epsilon 0.02297, total time 732, loss [2.0135457515716553], compute time 13.86\n",
      "episode 4194, reward 1750, memory_length 2000, epsilon 0.02295, total time 723, loss -, compute time 13.51\n",
      "episode 4195, reward 1656, memory_length 2000, epsilon 0.02293, total time 729, loss -, compute time 12.12\n",
      "episode 4196, reward 1922, memory_length 2000, epsilon 0.02291, total time 724, loss -, compute time 13.37\n",
      "episode 4197, reward 1658, memory_length 2000, epsilon 0.02288, total time 722, loss -, compute time 13.37\n",
      "episode 4198, reward 1677, memory_length 2000, epsilon 0.02286, total time 727, loss -, compute time 13.42\n",
      "episode 4199, reward 1809, memory_length 2000, epsilon 0.02284, total time 721, loss -, compute time 13.66\n",
      "episode 4200, reward 1535, memory_length 2000, epsilon 0.02282, total time 727, loss -, compute time 14.3\n",
      "Saving model for episode: 4200\n",
      "episode 4201, reward 1487, memory_length 2000, epsilon 0.0228, total time 723, loss -, compute time 13.96\n",
      "episode 4202, reward 1726, memory_length 2000, epsilon 0.02278, total time 721, loss -, compute time 12.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4203, reward 1508, memory_length 2000, epsilon 0.02276, total time 728, loss -, compute time 13.45\n",
      "episode 4204, reward 1323, memory_length 2000, epsilon 0.02274, total time 725, loss -, compute time 13.04\n",
      "episode 4205, reward 1370, memory_length 2000, epsilon 0.02272, total time 721, loss -, compute time 13.48\n",
      "episode 4206, reward 1823, memory_length 2000, epsilon 0.0227, total time 729, loss -, compute time 13.0\n",
      "episode 4207, reward 1604, memory_length 2000, epsilon 0.02268, total time 724, loss -, compute time 13.66\n",
      "episode 4208, reward 1510, memory_length 2000, epsilon 0.02266, total time 724, loss -, compute time 14.41\n",
      "episode 4209, reward 1599, memory_length 2000, epsilon 0.02264, total time 727, loss -, compute time 13.4\n",
      "episode 4210, reward 1812, memory_length 2000, epsilon 0.02262, total time 725, loss [1.4736673831939697], compute time 13.6\n",
      "Saving model for episode: 4210\n",
      "episode 4211, reward 1661, memory_length 2000, epsilon 0.0226, total time 729, loss -, compute time 13.3\n",
      "episode 4212, reward 1193, memory_length 2000, epsilon 0.02258, total time 727, loss -, compute time 15.34\n",
      "episode 4213, reward 1688, memory_length 2000, epsilon 0.02256, total time 727, loss -, compute time 14.08\n",
      "episode 4214, reward 1559, memory_length 2000, epsilon 0.02254, total time 721, loss -, compute time 12.51\n",
      "episode 4215, reward 1762, memory_length 2000, epsilon 0.02252, total time 725, loss -, compute time 15.58\n",
      "episode 4216, reward 1560, memory_length 2000, epsilon 0.0225, total time 722, loss -, compute time 14.29\n",
      "episode 4217, reward 1531, memory_length 2000, epsilon 0.02248, total time 721, loss -, compute time 13.96\n",
      "episode 4218, reward 1524, memory_length 2000, epsilon 0.02246, total time 723, loss -, compute time 12.9\n",
      "episode 4219, reward 1584, memory_length 2000, epsilon 0.02244, total time 729, loss -, compute time 14.31\n",
      "episode 4220, reward 1688, memory_length 2000, epsilon 0.02242, total time 724, loss -, compute time 13.31\n",
      "Saving model for episode: 4220\n",
      "episode 4221, reward 1590, memory_length 2000, epsilon 0.0224, total time 723, loss -, compute time 12.7\n",
      "episode 4222, reward 1712, memory_length 2000, epsilon 0.02238, total time 725, loss -, compute time 12.88\n",
      "episode 4223, reward 1780, memory_length 2000, epsilon 0.02236, total time 722, loss -, compute time 13.07\n",
      "episode 4224, reward 1868, memory_length 2000, epsilon 0.02234, total time 728, loss -, compute time 13.41\n",
      "episode 4225, reward 1786, memory_length 2000, epsilon 0.02231, total time 723, loss -, compute time 12.15\n",
      "episode 4226, reward 1557, memory_length 2000, epsilon 0.02229, total time 727, loss -, compute time 13.46\n",
      "episode 4227, reward 1922, memory_length 2000, epsilon 0.02227, total time 725, loss -, compute time 12.71\n",
      "episode 4228, reward 1713, memory_length 2000, epsilon 0.02225, total time 724, loss -, compute time 13.41\n",
      "episode 4229, reward 1521, memory_length 2000, epsilon 0.02223, total time 721, loss -, compute time 13.92\n",
      "episode 4230, reward 1807, memory_length 2000, epsilon 0.02221, total time 723, loss -, compute time 12.22\n",
      "Saving model for episode: 4230\n",
      "episode 4231, reward 1851, memory_length 2000, epsilon 0.02219, total time 724, loss -, compute time 13.5\n",
      "episode 4232, reward 1883, memory_length 2000, epsilon 0.02217, total time 722, loss -, compute time 12.94\n",
      "episode 4233, reward 1541, memory_length 2000, epsilon 0.02215, total time 726, loss -, compute time 13.95\n",
      "episode 4234, reward 1918, memory_length 2000, epsilon 0.02213, total time 726, loss -, compute time 11.78\n",
      "episode 4235, reward 1824, memory_length 2000, epsilon 0.02211, total time 730, loss -, compute time 13.91\n",
      "episode 4236, reward 1816, memory_length 2000, epsilon 0.0221, total time 722, loss -, compute time 11.81\n",
      "episode 4237, reward 1717, memory_length 2000, epsilon 0.02208, total time 727, loss -, compute time 13.5\n",
      "episode 4238, reward 1610, memory_length 2000, epsilon 0.02206, total time 722, loss -, compute time 11.34\n",
      "episode 4239, reward 1810, memory_length 2000, epsilon 0.02204, total time 723, loss -, compute time 13.4\n",
      "episode 4240, reward 1698, memory_length 2000, epsilon 0.02202, total time 728, loss -, compute time 12.33\n",
      "Saving model for episode: 4240\n",
      "episode 4241, reward 1958, memory_length 2000, epsilon 0.022, total time 727, loss -, compute time 12.03\n",
      "episode 4242, reward 1725, memory_length 2000, epsilon 0.02198, total time 731, loss -, compute time 13.53\n",
      "episode 4243, reward 1773, memory_length 2000, epsilon 0.02196, total time 729, loss -, compute time 12.77\n",
      "episode 4244, reward 1650, memory_length 2000, epsilon 0.02194, total time 729, loss -, compute time 12.92\n",
      "episode 4245, reward 1418, memory_length 2000, epsilon 0.02192, total time 727, loss -, compute time 14.63\n",
      "episode 4246, reward 1866, memory_length 2000, epsilon 0.0219, total time 724, loss -, compute time 11.21\n",
      "episode 4247, reward 1814, memory_length 2000, epsilon 0.02188, total time 728, loss -, compute time 12.17\n",
      "episode 4248, reward 1654, memory_length 2000, epsilon 0.02186, total time 721, loss -, compute time 13.34\n",
      "episode 4249, reward 1644, memory_length 2000, epsilon 0.02184, total time 731, loss -, compute time 12.82\n",
      "episode 4250, reward 1899, memory_length 2000, epsilon 0.02182, total time 721, loss -, compute time 13.6\n",
      "Saving model for episode: 4250\n",
      "episode 4251, reward 1599, memory_length 2000, epsilon 0.0218, total time 728, loss -, compute time 12.96\n",
      "episode 4252, reward 1873, memory_length 2000, epsilon 0.02178, total time 723, loss -, compute time 14.54\n",
      "episode 4253, reward 1752, memory_length 2000, epsilon 0.02176, total time 728, loss -, compute time 12.01\n",
      "episode 4254, reward 1709, memory_length 2000, epsilon 0.02174, total time 726, loss -, compute time 12.55\n",
      "episode 4255, reward 1629, memory_length 2000, epsilon 0.02172, total time 731, loss -, compute time 13.67\n",
      "episode 4256, reward 1526, memory_length 2000, epsilon 0.0217, total time 727, loss -, compute time 13.8\n",
      "episode 4257, reward 1782, memory_length 2000, epsilon 0.02168, total time 728, loss -, compute time 13.39\n",
      "episode 4258, reward 1792, memory_length 2000, epsilon 0.02166, total time 723, loss -, compute time 13.88\n",
      "episode 4259, reward 1744, memory_length 2000, epsilon 0.02164, total time 721, loss -, compute time 12.1\n",
      "episode 4260, reward 1671, memory_length 2000, epsilon 0.02162, total time 726, loss -, compute time 14.3\n",
      "Saving model for episode: 4260\n",
      "episode 4261, reward 1848, memory_length 2000, epsilon 0.0216, total time 727, loss -, compute time 12.48\n",
      "episode 4262, reward 1441, memory_length 2000, epsilon 0.02158, total time 728, loss -, compute time 13.53\n",
      "episode 4263, reward 1890, memory_length 2000, epsilon 0.02156, total time 721, loss -, compute time 14.5\n",
      "episode 4264, reward 1746, memory_length 2000, epsilon 0.02155, total time 721, loss -, compute time 13.86\n",
      "episode 4265, reward 1465, memory_length 2000, epsilon 0.02153, total time 726, loss -, compute time 14.55\n",
      "episode 4266, reward 1850, memory_length 2000, epsilon 0.02151, total time 730, loss -, compute time 13.79\n",
      "episode 4267, reward 1760, memory_length 2000, epsilon 0.02149, total time 729, loss -, compute time 14.37\n",
      "episode 4268, reward 1737, memory_length 2000, epsilon 0.02147, total time 724, loss -, compute time 13.84\n",
      "episode 4269, reward 1767, memory_length 2000, epsilon 0.02145, total time 725, loss -, compute time 13.07\n",
      "episode 4270, reward 1734, memory_length 2000, epsilon 0.02143, total time 725, loss -, compute time 14.21\n",
      "Saving model for episode: 4270\n",
      "episode 4271, reward 1792, memory_length 2000, epsilon 0.02141, total time 723, loss -, compute time 14.23\n",
      "episode 4272, reward 1523, memory_length 2000, epsilon 0.02139, total time 722, loss -, compute time 13.99\n",
      "episode 4273, reward 1517, memory_length 2000, epsilon 0.02137, total time 727, loss [2.081580638885498], compute time 13.4\n",
      "episode 4274, reward 1782, memory_length 2000, epsilon 0.02135, total time 723, loss -, compute time 15.05\n",
      "episode 4275, reward 1558, memory_length 2000, epsilon 0.02133, total time 728, loss -, compute time 15.92\n",
      "episode 4276, reward 1821, memory_length 2000, epsilon 0.02131, total time 724, loss -, compute time 13.93\n",
      "episode 4277, reward 1980, memory_length 2000, epsilon 0.02129, total time 728, loss -, compute time 13.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4278, reward 1710, memory_length 2000, epsilon 0.02128, total time 726, loss -, compute time 12.38\n",
      "episode 4279, reward 1706, memory_length 2000, epsilon 0.02126, total time 722, loss -, compute time 16.13\n",
      "episode 4280, reward 1652, memory_length 2000, epsilon 0.02124, total time 724, loss -, compute time 15.64\n",
      "Saving model for episode: 4280\n",
      "episode 4281, reward 1648, memory_length 2000, epsilon 0.02122, total time 724, loss -, compute time 12.68\n",
      "episode 4282, reward 1824, memory_length 2000, epsilon 0.0212, total time 728, loss -, compute time 14.48\n",
      "episode 4283, reward 1697, memory_length 2000, epsilon 0.02118, total time 730, loss -, compute time 13.82\n",
      "episode 4284, reward 1760, memory_length 2000, epsilon 0.02116, total time 727, loss -, compute time 12.12\n",
      "episode 4285, reward 1517, memory_length 2000, epsilon 0.02114, total time 722, loss -, compute time 13.22\n",
      "episode 4286, reward 1420, memory_length 2000, epsilon 0.02112, total time 725, loss -, compute time 13.81\n",
      "episode 4287, reward 1587, memory_length 2000, epsilon 0.0211, total time 721, loss -, compute time 15.53\n",
      "episode 4288, reward 1790, memory_length 2000, epsilon 0.02108, total time 724, loss -, compute time 15.31\n",
      "episode 4289, reward 1620, memory_length 2000, epsilon 0.02107, total time 723, loss -, compute time 15.01\n",
      "episode 4290, reward 1679, memory_length 2000, epsilon 0.02105, total time 726, loss -, compute time 14.24\n",
      "Saving model for episode: 4290\n",
      "episode 4291, reward 1841, memory_length 2000, epsilon 0.02103, total time 728, loss -, compute time 15.43\n",
      "episode 4292, reward 1688, memory_length 2000, epsilon 0.02101, total time 723, loss -, compute time 13.59\n",
      "episode 4293, reward 1509, memory_length 2000, epsilon 0.02099, total time 728, loss -, compute time 14.04\n",
      "episode 4294, reward 1834, memory_length 2000, epsilon 0.02097, total time 721, loss -, compute time 14.11\n",
      "episode 4295, reward 1634, memory_length 2000, epsilon 0.02095, total time 728, loss -, compute time 13.98\n",
      "episode 4296, reward 1765, memory_length 2000, epsilon 0.02093, total time 724, loss -, compute time 12.92\n",
      "episode 4297, reward 1828, memory_length 2000, epsilon 0.02091, total time 730, loss -, compute time 12.55\n",
      "episode 4298, reward 1654, memory_length 2000, epsilon 0.0209, total time 726, loss -, compute time 14.19\n",
      "episode 4299, reward 1388, memory_length 2000, epsilon 0.02088, total time 725, loss -, compute time 13.0\n",
      "episode 4300, reward 1737, memory_length 2000, epsilon 0.02086, total time 721, loss -, compute time 14.01\n",
      "Saving model for episode: 4300\n",
      "episode 4301, reward 1698, memory_length 2000, epsilon 0.02084, total time 725, loss -, compute time 12.41\n",
      "episode 4302, reward 1704, memory_length 2000, epsilon 0.02082, total time 727, loss -, compute time 14.71\n",
      "episode 4303, reward 2031, memory_length 2000, epsilon 0.0208, total time 728, loss -, compute time 12.02\n",
      "episode 4304, reward 1701, memory_length 2000, epsilon 0.02078, total time 721, loss -, compute time 14.84\n",
      "episode 4305, reward 1719, memory_length 2000, epsilon 0.02076, total time 721, loss -, compute time 13.16\n",
      "episode 4306, reward 1764, memory_length 2000, epsilon 0.02075, total time 721, loss -, compute time 14.23\n",
      "episode 4307, reward 1733, memory_length 2000, epsilon 0.02073, total time 724, loss [1.2609751224517822], compute time 12.32\n",
      "episode 4308, reward 1971, memory_length 2000, epsilon 0.02071, total time 722, loss -, compute time 13.97\n",
      "episode 4309, reward 1836, memory_length 2000, epsilon 0.02069, total time 728, loss -, compute time 14.27\n",
      "episode 4310, reward 1540, memory_length 2000, epsilon 0.02067, total time 722, loss -, compute time 12.54\n",
      "Saving model for episode: 4310\n",
      "episode 4311, reward 1805, memory_length 2000, epsilon 0.02065, total time 724, loss -, compute time 15.0\n",
      "episode 4312, reward 1575, memory_length 2000, epsilon 0.02063, total time 731, loss -, compute time 13.76\n",
      "episode 4313, reward 1575, memory_length 2000, epsilon 0.02062, total time 721, loss -, compute time 12.27\n",
      "episode 4314, reward 1604, memory_length 2000, epsilon 0.0206, total time 724, loss -, compute time 12.84\n",
      "episode 4315, reward 1605, memory_length 2000, epsilon 0.02058, total time 723, loss -, compute time 13.93\n",
      "episode 4316, reward 1612, memory_length 2000, epsilon 0.02056, total time 728, loss -, compute time 12.42\n",
      "episode 4317, reward 1582, memory_length 2000, epsilon 0.02054, total time 721, loss -, compute time 13.19\n",
      "episode 4318, reward 1842, memory_length 2000, epsilon 0.02052, total time 725, loss -, compute time 11.63\n",
      "episode 4319, reward 1716, memory_length 2000, epsilon 0.0205, total time 726, loss -, compute time 14.64\n",
      "episode 4320, reward 1482, memory_length 2000, epsilon 0.02049, total time 725, loss -, compute time 13.85\n",
      "Saving model for episode: 4320\n",
      "episode 4321, reward 1449, memory_length 2000, epsilon 0.02047, total time 729, loss -, compute time 14.17\n",
      "episode 4322, reward 1356, memory_length 2000, epsilon 0.02045, total time 721, loss -, compute time 14.27\n",
      "episode 4323, reward 1807, memory_length 2000, epsilon 0.02043, total time 725, loss -, compute time 12.76\n",
      "episode 4324, reward 1680, memory_length 2000, epsilon 0.02041, total time 723, loss -, compute time 15.83\n",
      "episode 4325, reward 1641, memory_length 2000, epsilon 0.02039, total time 722, loss -, compute time 13.32\n",
      "episode 4326, reward 1639, memory_length 2000, epsilon 0.02038, total time 730, loss -, compute time 12.59\n",
      "episode 4327, reward 1661, memory_length 2000, epsilon 0.02036, total time 725, loss -, compute time 14.68\n",
      "episode 4328, reward 1659, memory_length 2000, epsilon 0.02034, total time 721, loss -, compute time 13.92\n",
      "episode 4329, reward 1856, memory_length 2000, epsilon 0.02032, total time 724, loss -, compute time 12.34\n",
      "episode 4330, reward 2007, memory_length 2000, epsilon 0.0203, total time 730, loss -, compute time 12.9\n",
      "Saving model for episode: 4330\n",
      "episode 4331, reward 1393, memory_length 2000, epsilon 0.02028, total time 721, loss -, compute time 13.66\n",
      "episode 4332, reward 1629, memory_length 2000, epsilon 0.02027, total time 721, loss [1.322817087173462], compute time 12.97\n",
      "episode 4333, reward 1755, memory_length 2000, epsilon 0.02025, total time 721, loss -, compute time 12.02\n",
      "episode 4334, reward 1653, memory_length 2000, epsilon 0.02023, total time 725, loss -, compute time 13.82\n",
      "episode 4335, reward 1658, memory_length 2000, epsilon 0.02021, total time 725, loss -, compute time 12.39\n",
      "episode 4336, reward 1738, memory_length 2000, epsilon 0.02019, total time 728, loss -, compute time 12.59\n",
      "episode 4337, reward 1725, memory_length 2000, epsilon 0.02018, total time 721, loss -, compute time 14.82\n",
      "episode 4338, reward 1324, memory_length 2000, epsilon 0.02016, total time 721, loss -, compute time 15.45\n",
      "episode 4339, reward 1667, memory_length 2000, epsilon 0.02014, total time 723, loss -, compute time 12.92\n",
      "episode 4340, reward 1926, memory_length 2000, epsilon 0.02012, total time 728, loss -, compute time 14.41\n",
      "Saving model for episode: 4340\n",
      "episode 4341, reward 1743, memory_length 2000, epsilon 0.0201, total time 721, loss -, compute time 13.81\n",
      "episode 4342, reward 1675, memory_length 2000, epsilon 0.02008, total time 723, loss -, compute time 13.57\n",
      "episode 4343, reward 1801, memory_length 2000, epsilon 0.02007, total time 725, loss -, compute time 14.5\n",
      "episode 4344, reward 1828, memory_length 2000, epsilon 0.02005, total time 722, loss -, compute time 11.64\n",
      "episode 4345, reward 1763, memory_length 2000, epsilon 0.02003, total time 721, loss -, compute time 13.68\n",
      "episode 4346, reward 1446, memory_length 2000, epsilon 0.02001, total time 723, loss -, compute time 13.38\n",
      "episode 4347, reward 1659, memory_length 2000, epsilon 0.01999, total time 721, loss -, compute time 10.55\n",
      "episode 4348, reward 1756, memory_length 2000, epsilon 0.01998, total time 724, loss -, compute time 12.65\n",
      "episode 4349, reward 1572, memory_length 2000, epsilon 0.01996, total time 728, loss -, compute time 12.59\n",
      "episode 4350, reward 1541, memory_length 2000, epsilon 0.01994, total time 724, loss -, compute time 12.48\n",
      "Saving model for episode: 4350\n",
      "episode 4351, reward 1681, memory_length 2000, epsilon 0.01992, total time 726, loss -, compute time 12.37\n",
      "episode 4352, reward 1629, memory_length 2000, epsilon 0.0199, total time 731, loss -, compute time 12.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4353, reward 1710, memory_length 2000, epsilon 0.01989, total time 725, loss -, compute time 11.91\n",
      "episode 4354, reward 1787, memory_length 2000, epsilon 0.01987, total time 729, loss -, compute time 13.03\n",
      "episode 4355, reward 1670, memory_length 2000, epsilon 0.01985, total time 721, loss -, compute time 13.04\n",
      "episode 4356, reward 1656, memory_length 2000, epsilon 0.01983, total time 721, loss -, compute time 13.0\n",
      "episode 4357, reward 1392, memory_length 2000, epsilon 0.01982, total time 723, loss -, compute time 14.24\n",
      "episode 4358, reward 1782, memory_length 2000, epsilon 0.0198, total time 721, loss -, compute time 13.25\n",
      "episode 4359, reward 1851, memory_length 2000, epsilon 0.01978, total time 726, loss -, compute time 12.94\n",
      "episode 4360, reward 1764, memory_length 2000, epsilon 0.01976, total time 725, loss -, compute time 14.64\n",
      "Saving model for episode: 4360\n",
      "episode 4361, reward 1560, memory_length 2000, epsilon 0.01974, total time 721, loss -, compute time 14.48\n",
      "episode 4362, reward 1711, memory_length 2000, epsilon 0.01973, total time 725, loss -, compute time 14.03\n",
      "episode 4363, reward 1541, memory_length 2000, epsilon 0.01971, total time 722, loss -, compute time 12.88\n",
      "episode 4364, reward 1781, memory_length 2000, epsilon 0.01969, total time 724, loss -, compute time 11.8\n",
      "episode 4365, reward 1426, memory_length 2000, epsilon 0.01967, total time 721, loss -, compute time 11.96\n",
      "episode 4366, reward 1576, memory_length 2000, epsilon 0.01966, total time 730, loss -, compute time 11.21\n",
      "episode 4367, reward 1663, memory_length 2000, epsilon 0.01964, total time 725, loss -, compute time 12.12\n",
      "episode 4368, reward 1530, memory_length 2000, epsilon 0.01962, total time 728, loss -, compute time 13.08\n",
      "episode 4369, reward 1631, memory_length 2000, epsilon 0.0196, total time 729, loss -, compute time 11.65\n",
      "episode 4370, reward 1576, memory_length 2000, epsilon 0.01958, total time 726, loss -, compute time 13.78\n",
      "Saving model for episode: 4370\n",
      "episode 4371, reward 1544, memory_length 2000, epsilon 0.01957, total time 724, loss -, compute time 12.21\n",
      "episode 4372, reward 1440, memory_length 2000, epsilon 0.01955, total time 727, loss -, compute time 11.61\n",
      "episode 4373, reward 1728, memory_length 2000, epsilon 0.01953, total time 727, loss -, compute time 13.49\n",
      "episode 4374, reward 1697, memory_length 2000, epsilon 0.01951, total time 725, loss [1.5858790874481201], compute time 12.26\n",
      "episode 4375, reward 1402, memory_length 2000, epsilon 0.0195, total time 721, loss -, compute time 14.0\n",
      "episode 4376, reward 1401, memory_length 2000, epsilon 0.01948, total time 723, loss -, compute time 12.41\n",
      "episode 4377, reward 1680, memory_length 2000, epsilon 0.01946, total time 722, loss -, compute time 12.06\n",
      "episode 4378, reward 1744, memory_length 2000, epsilon 0.01944, total time 722, loss -, compute time 12.86\n",
      "episode 4379, reward 1667, memory_length 2000, epsilon 0.01943, total time 722, loss -, compute time 12.96\n",
      "episode 4380, reward 1670, memory_length 2000, epsilon 0.01941, total time 725, loss -, compute time 12.01\n",
      "Saving model for episode: 4380\n",
      "episode 4381, reward 1735, memory_length 2000, epsilon 0.01939, total time 722, loss -, compute time 12.99\n",
      "episode 4382, reward 1692, memory_length 2000, epsilon 0.01937, total time 726, loss -, compute time 13.55\n",
      "episode 4383, reward 1851, memory_length 2000, epsilon 0.01936, total time 726, loss -, compute time 12.21\n",
      "episode 4384, reward 1953, memory_length 2000, epsilon 0.01934, total time 735, loss -, compute time 12.24\n",
      "episode 4385, reward 1676, memory_length 2000, epsilon 0.01932, total time 724, loss -, compute time 13.66\n",
      "episode 4386, reward 1980, memory_length 2000, epsilon 0.0193, total time 723, loss -, compute time 11.52\n",
      "episode 4387, reward 1581, memory_length 2000, epsilon 0.01929, total time 722, loss -, compute time 13.56\n",
      "episode 4388, reward 1801, memory_length 2000, epsilon 0.01927, total time 725, loss -, compute time 13.79\n",
      "episode 4389, reward 1604, memory_length 2000, epsilon 0.01925, total time 731, loss -, compute time 13.07\n",
      "episode 4390, reward 1794, memory_length 2000, epsilon 0.01924, total time 723, loss -, compute time 12.41\n",
      "Saving model for episode: 4390\n",
      "episode 4391, reward 1662, memory_length 2000, epsilon 0.01922, total time 728, loss -, compute time 12.9\n",
      "episode 4392, reward 1638, memory_length 2000, epsilon 0.0192, total time 721, loss -, compute time 11.76\n",
      "episode 4393, reward 1378, memory_length 2000, epsilon 0.01918, total time 723, loss -, compute time 13.42\n",
      "episode 4394, reward 1854, memory_length 2000, epsilon 0.01917, total time 724, loss -, compute time 12.09\n",
      "episode 4395, reward 1703, memory_length 2000, epsilon 0.01915, total time 723, loss -, compute time 12.51\n",
      "episode 4396, reward 1851, memory_length 2000, epsilon 0.01913, total time 722, loss -, compute time 12.34\n",
      "episode 4397, reward 1796, memory_length 2000, epsilon 0.01911, total time 727, loss -, compute time 13.82\n",
      "episode 4398, reward 1402, memory_length 2000, epsilon 0.0191, total time 721, loss -, compute time 11.43\n",
      "episode 4399, reward 1819, memory_length 2000, epsilon 0.01908, total time 722, loss -, compute time 12.51\n",
      "episode 4400, reward 1787, memory_length 2000, epsilon 0.01906, total time 731, loss -, compute time 12.74\n",
      "Saving model for episode: 4400\n",
      "episode 4401, reward 1719, memory_length 2000, epsilon 0.01905, total time 725, loss -, compute time 13.51\n",
      "episode 4402, reward 1743, memory_length 2000, epsilon 0.01903, total time 729, loss -, compute time 11.95\n",
      "episode 4403, reward 1771, memory_length 2000, epsilon 0.01901, total time 723, loss -, compute time 12.83\n",
      "episode 4404, reward 1594, memory_length 2000, epsilon 0.01899, total time 730, loss -, compute time 13.67\n",
      "episode 4405, reward 1724, memory_length 2000, epsilon 0.01898, total time 726, loss -, compute time 12.86\n",
      "episode 4406, reward 1827, memory_length 2000, epsilon 0.01896, total time 726, loss -, compute time 12.07\n",
      "episode 4407, reward 1851, memory_length 2000, epsilon 0.01894, total time 722, loss -, compute time 12.55\n",
      "episode 4408, reward 1877, memory_length 2000, epsilon 0.01893, total time 721, loss -, compute time 13.12\n",
      "episode 4409, reward 1557, memory_length 2000, epsilon 0.01891, total time 729, loss -, compute time 11.88\n",
      "episode 4410, reward 1782, memory_length 2000, epsilon 0.01889, total time 726, loss -, compute time 11.81\n",
      "Saving model for episode: 4410\n",
      "episode 4411, reward 1710, memory_length 2000, epsilon 0.01888, total time 730, loss -, compute time 12.36\n",
      "episode 4412, reward 1751, memory_length 2000, epsilon 0.01886, total time 725, loss -, compute time 14.16\n",
      "episode 4413, reward 1747, memory_length 2000, epsilon 0.01884, total time 732, loss -, compute time 13.74\n",
      "episode 4414, reward 1670, memory_length 2000, epsilon 0.01882, total time 726, loss -, compute time 12.88\n",
      "episode 4415, reward 1765, memory_length 2000, epsilon 0.01881, total time 727, loss -, compute time 13.99\n",
      "episode 4416, reward 1483, memory_length 2000, epsilon 0.01879, total time 728, loss -, compute time 14.25\n",
      "episode 4417, reward 1630, memory_length 2000, epsilon 0.01877, total time 727, loss -, compute time 14.49\n",
      "episode 4418, reward 1514, memory_length 2000, epsilon 0.01876, total time 721, loss -, compute time 14.71\n",
      "episode 4419, reward 1904, memory_length 2000, epsilon 0.01874, total time 726, loss -, compute time 13.57\n",
      "episode 4420, reward 1586, memory_length 2000, epsilon 0.01872, total time 724, loss -, compute time 13.64\n",
      "Saving model for episode: 4420\n",
      "episode 4421, reward 1683, memory_length 2000, epsilon 0.01871, total time 721, loss -, compute time 13.3\n",
      "episode 4422, reward 1553, memory_length 2000, epsilon 0.01869, total time 725, loss -, compute time 10.9\n",
      "episode 4423, reward 1682, memory_length 2000, epsilon 0.01867, total time 722, loss -, compute time 12.53\n",
      "episode 4424, reward 1681, memory_length 2000, epsilon 0.01866, total time 722, loss -, compute time 13.02\n",
      "episode 4425, reward 1643, memory_length 2000, epsilon 0.01864, total time 722, loss -, compute time 13.85\n",
      "episode 4426, reward 1776, memory_length 2000, epsilon 0.01862, total time 725, loss -, compute time 13.32\n",
      "episode 4427, reward 1755, memory_length 2000, epsilon 0.01861, total time 723, loss -, compute time 12.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4428, reward 1597, memory_length 2000, epsilon 0.01859, total time 722, loss -, compute time 12.45\n",
      "episode 4429, reward 1587, memory_length 2000, epsilon 0.01857, total time 722, loss -, compute time 11.66\n",
      "episode 4430, reward 1791, memory_length 2000, epsilon 0.01856, total time 726, loss -, compute time 12.73\n",
      "Saving model for episode: 4430\n",
      "episode 4431, reward 1760, memory_length 2000, epsilon 0.01854, total time 728, loss -, compute time 14.35\n",
      "episode 4432, reward 1788, memory_length 2000, epsilon 0.01852, total time 725, loss -, compute time 11.99\n",
      "episode 4433, reward 1633, memory_length 2000, epsilon 0.01851, total time 723, loss -, compute time 14.34\n",
      "episode 4434, reward 1874, memory_length 2000, epsilon 0.01849, total time 724, loss -, compute time 12.57\n",
      "episode 4435, reward 1733, memory_length 2000, epsilon 0.01847, total time 725, loss -, compute time 13.13\n",
      "episode 4436, reward 1629, memory_length 2000, epsilon 0.01846, total time 723, loss [0.9479431509971619], compute time 12.78\n",
      "episode 4437, reward 1828, memory_length 2000, epsilon 0.01844, total time 726, loss -, compute time 11.54\n",
      "episode 4438, reward 1585, memory_length 2000, epsilon 0.01842, total time 722, loss -, compute time 12.9\n",
      "episode 4439, reward 1724, memory_length 2000, epsilon 0.01841, total time 730, loss -, compute time 12.73\n",
      "episode 4440, reward 1597, memory_length 2000, epsilon 0.01839, total time 724, loss -, compute time 12.09\n",
      "Saving model for episode: 4440\n",
      "episode 4441, reward 1612, memory_length 2000, epsilon 0.01837, total time 727, loss -, compute time 12.11\n",
      "episode 4442, reward 1522, memory_length 2000, epsilon 0.01836, total time 726, loss -, compute time 13.62\n",
      "episode 4443, reward 1474, memory_length 2000, epsilon 0.01834, total time 721, loss -, compute time 12.87\n",
      "episode 4444, reward 1572, memory_length 2000, epsilon 0.01832, total time 723, loss -, compute time 12.32\n",
      "episode 4445, reward 1696, memory_length 2000, epsilon 0.01831, total time 722, loss -, compute time 11.48\n",
      "episode 4446, reward 1562, memory_length 2000, epsilon 0.01829, total time 722, loss -, compute time 11.78\n",
      "episode 4447, reward 1629, memory_length 2000, epsilon 0.01827, total time 721, loss -, compute time 13.53\n",
      "episode 4448, reward 1882, memory_length 2000, epsilon 0.01826, total time 725, loss -, compute time 11.8\n",
      "episode 4449, reward 1872, memory_length 2000, epsilon 0.01824, total time 729, loss -, compute time 11.97\n",
      "episode 4450, reward 1780, memory_length 2000, epsilon 0.01822, total time 727, loss [2.2725577354431152], compute time 12.45\n",
      "Saving model for episode: 4450\n",
      "episode 4451, reward 1800, memory_length 2000, epsilon 0.01821, total time 721, loss -, compute time 13.4\n",
      "episode 4452, reward 1744, memory_length 2000, epsilon 0.01819, total time 730, loss -, compute time 11.74\n",
      "episode 4453, reward 1521, memory_length 2000, epsilon 0.01818, total time 721, loss -, compute time 13.46\n",
      "episode 4454, reward 1727, memory_length 2000, epsilon 0.01816, total time 721, loss -, compute time 13.6\n",
      "episode 4455, reward 1760, memory_length 2000, epsilon 0.01814, total time 727, loss -, compute time 12.99\n",
      "episode 4456, reward 1829, memory_length 2000, epsilon 0.01813, total time 727, loss -, compute time 13.15\n",
      "episode 4457, reward 1693, memory_length 2000, epsilon 0.01811, total time 728, loss -, compute time 11.73\n",
      "episode 4458, reward 1809, memory_length 2000, epsilon 0.01809, total time 729, loss -, compute time 13.4\n",
      "episode 4459, reward 1544, memory_length 2000, epsilon 0.01808, total time 727, loss -, compute time 13.63\n",
      "episode 4460, reward 1671, memory_length 2000, epsilon 0.01806, total time 721, loss -, compute time 12.13\n",
      "Saving model for episode: 4460\n",
      "episode 4461, reward 1458, memory_length 2000, epsilon 0.01804, total time 724, loss -, compute time 13.26\n",
      "episode 4462, reward 1709, memory_length 2000, epsilon 0.01803, total time 723, loss -, compute time 14.02\n",
      "episode 4463, reward 1661, memory_length 2000, epsilon 0.01801, total time 723, loss -, compute time 13.51\n",
      "episode 4464, reward 1707, memory_length 2000, epsilon 0.018, total time 724, loss -, compute time 13.68\n",
      "episode 4465, reward 1674, memory_length 2000, epsilon 0.01798, total time 726, loss -, compute time 12.08\n",
      "episode 4466, reward 1551, memory_length 2000, epsilon 0.01796, total time 724, loss -, compute time 13.36\n",
      "episode 4467, reward 1769, memory_length 2000, epsilon 0.01795, total time 726, loss -, compute time 13.05\n",
      "episode 4468, reward 1827, memory_length 2000, epsilon 0.01793, total time 726, loss -, compute time 11.65\n",
      "episode 4469, reward 1571, memory_length 2000, epsilon 0.01792, total time 721, loss -, compute time 13.07\n",
      "episode 4470, reward 1792, memory_length 2000, epsilon 0.0179, total time 729, loss -, compute time 13.08\n",
      "Saving model for episode: 4470\n",
      "episode 4471, reward 1474, memory_length 2000, epsilon 0.01788, total time 723, loss -, compute time 14.4\n",
      "episode 4472, reward 1791, memory_length 2000, epsilon 0.01787, total time 723, loss -, compute time 13.66\n",
      "episode 4473, reward 1521, memory_length 2000, epsilon 0.01785, total time 733, loss -, compute time 13.71\n",
      "episode 4474, reward 1282, memory_length 2000, epsilon 0.01783, total time 722, loss -, compute time 12.18\n",
      "episode 4475, reward 1409, memory_length 2000, epsilon 0.01782, total time 725, loss -, compute time 14.42\n",
      "episode 4476, reward 1630, memory_length 2000, epsilon 0.0178, total time 724, loss -, compute time 13.06\n",
      "episode 4477, reward 1895, memory_length 2000, epsilon 0.01779, total time 721, loss -, compute time 11.85\n",
      "episode 4478, reward 1706, memory_length 2000, epsilon 0.01777, total time 722, loss -, compute time 11.69\n",
      "episode 4479, reward 1641, memory_length 2000, epsilon 0.01775, total time 722, loss -, compute time 12.88\n",
      "episode 4480, reward 1760, memory_length 2000, epsilon 0.01774, total time 732, loss -, compute time 12.4\n",
      "Saving model for episode: 4480\n",
      "episode 4481, reward 1809, memory_length 2000, epsilon 0.01772, total time 726, loss -, compute time 12.73\n",
      "episode 4482, reward 1631, memory_length 2000, epsilon 0.01771, total time 721, loss -, compute time 13.02\n",
      "episode 4483, reward 1663, memory_length 2000, epsilon 0.01769, total time 722, loss -, compute time 13.81\n",
      "episode 4484, reward 1659, memory_length 2000, epsilon 0.01768, total time 721, loss -, compute time 12.33\n",
      "episode 4485, reward 1905, memory_length 2000, epsilon 0.01766, total time 723, loss -, compute time 12.98\n",
      "episode 4486, reward 1770, memory_length 2000, epsilon 0.01764, total time 725, loss -, compute time 12.31\n",
      "episode 4487, reward 1847, memory_length 2000, epsilon 0.01763, total time 724, loss -, compute time 13.32\n",
      "episode 4488, reward 1735, memory_length 2000, epsilon 0.01761, total time 727, loss -, compute time 12.49\n",
      "episode 4489, reward 1957, memory_length 2000, epsilon 0.0176, total time 726, loss -, compute time 11.75\n",
      "episode 4490, reward 1482, memory_length 2000, epsilon 0.01758, total time 721, loss -, compute time 13.76\n",
      "Saving model for episode: 4490\n",
      "episode 4491, reward 1616, memory_length 2000, epsilon 0.01756, total time 723, loss -, compute time 13.1\n",
      "episode 4492, reward 1412, memory_length 2000, epsilon 0.01755, total time 725, loss -, compute time 13.23\n",
      "episode 4493, reward 1851, memory_length 2000, epsilon 0.01753, total time 724, loss -, compute time 13.9\n",
      "episode 4494, reward 1782, memory_length 2000, epsilon 0.01752, total time 725, loss -, compute time 12.43\n",
      "episode 4495, reward 1815, memory_length 2000, epsilon 0.0175, total time 723, loss -, compute time 12.58\n",
      "episode 4496, reward 1670, memory_length 2000, epsilon 0.01749, total time 726, loss -, compute time 12.52\n",
      "episode 4497, reward 1827, memory_length 2000, epsilon 0.01747, total time 726, loss -, compute time 11.53\n",
      "episode 4498, reward 1827, memory_length 2000, epsilon 0.01745, total time 721, loss -, compute time 13.25\n",
      "episode 4499, reward 1611, memory_length 2000, epsilon 0.01744, total time 723, loss -, compute time 13.32\n",
      "episode 4500, reward 1551, memory_length 2000, epsilon 0.01742, total time 722, loss -, compute time 11.54\n",
      "Saving model for episode: 4500\n",
      "episode 4501, reward 1666, memory_length 2000, epsilon 0.01741, total time 722, loss -, compute time 13.13\n",
      "episode 4502, reward 1756, memory_length 2000, epsilon 0.01739, total time 724, loss -, compute time 12.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4503, reward 2061, memory_length 2000, epsilon 0.01738, total time 731, loss -, compute time 13.35\n",
      "episode 4504, reward 1584, memory_length 2000, epsilon 0.01736, total time 728, loss -, compute time 12.0\n",
      "episode 4505, reward 1646, memory_length 2000, epsilon 0.01734, total time 723, loss -, compute time 14.12\n",
      "episode 4506, reward 1676, memory_length 2000, epsilon 0.01733, total time 726, loss -, compute time 12.76\n",
      "episode 4507, reward 1894, memory_length 2000, epsilon 0.01731, total time 733, loss -, compute time 12.8\n",
      "episode 4508, reward 1748, memory_length 2000, epsilon 0.0173, total time 721, loss -, compute time 13.14\n",
      "episode 4509, reward 1881, memory_length 2000, epsilon 0.01728, total time 731, loss -, compute time 12.75\n",
      "episode 4510, reward 1851, memory_length 2000, epsilon 0.01727, total time 726, loss -, compute time 13.8\n",
      "Saving model for episode: 4510\n",
      "episode 4511, reward 1878, memory_length 2000, epsilon 0.01725, total time 725, loss -, compute time 12.0\n",
      "episode 4512, reward 1697, memory_length 2000, epsilon 0.01724, total time 723, loss -, compute time 12.79\n",
      "episode 4513, reward 1742, memory_length 2000, epsilon 0.01722, total time 728, loss -, compute time 12.35\n",
      "episode 4514, reward 1750, memory_length 2000, epsilon 0.0172, total time 721, loss -, compute time 11.88\n",
      "episode 4515, reward 1621, memory_length 2000, epsilon 0.01719, total time 725, loss -, compute time 12.37\n",
      "episode 4516, reward 1639, memory_length 2000, epsilon 0.01717, total time 728, loss -, compute time 11.41\n",
      "episode 4517, reward 1751, memory_length 2000, epsilon 0.01716, total time 721, loss -, compute time 12.64\n",
      "episode 4518, reward 1857, memory_length 2000, epsilon 0.01714, total time 721, loss -, compute time 12.22\n",
      "episode 4519, reward 1789, memory_length 2000, epsilon 0.01713, total time 725, loss -, compute time 11.83\n",
      "episode 4520, reward 1400, memory_length 2000, epsilon 0.01711, total time 731, loss -, compute time 14.6\n",
      "Saving model for episode: 4520\n",
      "episode 4521, reward 1779, memory_length 2000, epsilon 0.0171, total time 723, loss -, compute time 13.26\n",
      "episode 4522, reward 1784, memory_length 2000, epsilon 0.01708, total time 725, loss -, compute time 13.5\n",
      "episode 4523, reward 1213, memory_length 2000, epsilon 0.01707, total time 727, loss -, compute time 12.56\n",
      "episode 4524, reward 1847, memory_length 2000, epsilon 0.01705, total time 727, loss -, compute time 13.07\n",
      "episode 4525, reward 1715, memory_length 2000, epsilon 0.01703, total time 722, loss -, compute time 11.17\n",
      "episode 4526, reward 1811, memory_length 2000, epsilon 0.01702, total time 721, loss -, compute time 12.55\n",
      "episode 4527, reward 1635, memory_length 2000, epsilon 0.017, total time 722, loss -, compute time 12.57\n",
      "episode 4528, reward 1577, memory_length 2000, epsilon 0.01699, total time 722, loss -, compute time 13.2\n",
      "episode 4529, reward 1740, memory_length 2000, epsilon 0.01697, total time 722, loss -, compute time 13.0\n",
      "episode 4530, reward 1490, memory_length 2000, epsilon 0.01696, total time 730, loss -, compute time 11.93\n",
      "Saving model for episode: 4530\n",
      "episode 4531, reward 1188, memory_length 2000, epsilon 0.01694, total time 724, loss -, compute time 11.86\n",
      "episode 4532, reward 1757, memory_length 2000, epsilon 0.01693, total time 725, loss -, compute time 13.06\n",
      "episode 4533, reward 1674, memory_length 2000, epsilon 0.01691, total time 722, loss -, compute time 12.78\n",
      "episode 4534, reward 1599, memory_length 2000, epsilon 0.0169, total time 723, loss -, compute time 11.57\n",
      "episode 4535, reward 1541, memory_length 2000, epsilon 0.01688, total time 727, loss -, compute time 12.67\n",
      "episode 4536, reward 1536, memory_length 2000, epsilon 0.01687, total time 722, loss -, compute time 14.64\n",
      "episode 4537, reward 1423, memory_length 2000, epsilon 0.01685, total time 721, loss -, compute time 12.13\n",
      "episode 4538, reward 1671, memory_length 2000, epsilon 0.01684, total time 726, loss -, compute time 13.79\n",
      "episode 4539, reward 1330, memory_length 2000, epsilon 0.01682, total time 729, loss -, compute time 13.37\n",
      "episode 4540, reward 1511, memory_length 2000, epsilon 0.01681, total time 723, loss -, compute time 11.81\n",
      "Saving model for episode: 4540\n",
      "episode 4541, reward 1605, memory_length 2000, epsilon 0.01679, total time 722, loss -, compute time 12.71\n",
      "episode 4542, reward 1746, memory_length 2000, epsilon 0.01678, total time 721, loss -, compute time 11.19\n",
      "episode 4543, reward 1596, memory_length 2000, epsilon 0.01676, total time 721, loss -, compute time 12.07\n",
      "episode 4544, reward 1861, memory_length 2000, epsilon 0.01675, total time 721, loss -, compute time 13.81\n",
      "episode 4545, reward 1704, memory_length 2000, epsilon 0.01673, total time 722, loss -, compute time 11.95\n",
      "episode 4546, reward 1748, memory_length 2000, epsilon 0.01672, total time 725, loss -, compute time 11.2\n",
      "episode 4547, reward 1600, memory_length 2000, epsilon 0.0167, total time 725, loss -, compute time 13.78\n",
      "episode 4548, reward 1681, memory_length 2000, epsilon 0.01669, total time 721, loss -, compute time 12.25\n",
      "episode 4549, reward 1430, memory_length 2000, epsilon 0.01667, total time 721, loss -, compute time 14.34\n",
      "episode 4550, reward 1550, memory_length 2000, epsilon 0.01666, total time 722, loss -, compute time 12.49\n",
      "Saving model for episode: 4550\n",
      "episode 4551, reward 1949, memory_length 2000, epsilon 0.01664, total time 725, loss -, compute time 12.21\n",
      "episode 4552, reward 1797, memory_length 2000, epsilon 0.01663, total time 722, loss -, compute time 13.13\n",
      "episode 4553, reward 1717, memory_length 2000, epsilon 0.01661, total time 722, loss -, compute time 12.11\n",
      "episode 4554, reward 1757, memory_length 2000, epsilon 0.0166, total time 724, loss -, compute time 11.89\n",
      "episode 4555, reward 1634, memory_length 2000, epsilon 0.01658, total time 727, loss -, compute time 13.29\n",
      "episode 4556, reward 1662, memory_length 2000, epsilon 0.01657, total time 723, loss -, compute time 13.42\n",
      "episode 4557, reward 1905, memory_length 2000, epsilon 0.01655, total time 724, loss -, compute time 12.97\n",
      "episode 4558, reward 1755, memory_length 2000, epsilon 0.01654, total time 722, loss -, compute time 13.73\n",
      "episode 4559, reward 1571, memory_length 2000, epsilon 0.01652, total time 722, loss -, compute time 13.17\n",
      "episode 4560, reward 1709, memory_length 2000, epsilon 0.01651, total time 722, loss -, compute time 13.81\n",
      "Saving model for episode: 4560\n",
      "episode 4561, reward 1742, memory_length 2000, epsilon 0.01649, total time 725, loss -, compute time 12.42\n",
      "episode 4562, reward 1819, memory_length 2000, epsilon 0.01648, total time 729, loss -, compute time 13.01\n",
      "episode 4563, reward 1779, memory_length 2000, epsilon 0.01646, total time 723, loss -, compute time 13.43\n",
      "episode 4564, reward 1505, memory_length 2000, epsilon 0.01645, total time 722, loss -, compute time 13.97\n",
      "episode 4565, reward 1688, memory_length 2000, epsilon 0.01643, total time 723, loss -, compute time 11.97\n",
      "episode 4566, reward 1806, memory_length 2000, epsilon 0.01642, total time 723, loss -, compute time 13.71\n",
      "episode 4567, reward 1985, memory_length 2000, epsilon 0.0164, total time 722, loss -, compute time 14.59\n",
      "episode 4568, reward 1655, memory_length 2000, epsilon 0.01639, total time 721, loss -, compute time 14.06\n",
      "episode 4569, reward 1616, memory_length 2000, epsilon 0.01637, total time 730, loss -, compute time 13.67\n",
      "episode 4570, reward 1738, memory_length 2000, epsilon 0.01636, total time 729, loss -, compute time 12.73\n",
      "Saving model for episode: 4570\n",
      "episode 4571, reward 1905, memory_length 2000, epsilon 0.01634, total time 723, loss -, compute time 13.31\n",
      "episode 4572, reward 1695, memory_length 2000, epsilon 0.01633, total time 722, loss -, compute time 13.91\n",
      "episode 4573, reward 1294, memory_length 2000, epsilon 0.01631, total time 721, loss -, compute time 11.27\n",
      "episode 4574, reward 1946, memory_length 2000, epsilon 0.0163, total time 727, loss -, compute time 12.82\n",
      "episode 4575, reward 1716, memory_length 2000, epsilon 0.01629, total time 728, loss -, compute time 11.72\n",
      "episode 4576, reward 1674, memory_length 2000, epsilon 0.01627, total time 721, loss -, compute time 13.29\n",
      "episode 4577, reward 1320, memory_length 2000, epsilon 0.01626, total time 726, loss -, compute time 13.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4578, reward 1540, memory_length 2000, epsilon 0.01624, total time 723, loss -, compute time 11.84\n",
      "episode 4579, reward 1823, memory_length 2000, epsilon 0.01623, total time 726, loss -, compute time 13.02\n",
      "episode 4580, reward 1647, memory_length 2000, epsilon 0.01621, total time 733, loss -, compute time 14.18\n",
      "Saving model for episode: 4580\n",
      "episode 4581, reward 1613, memory_length 2000, epsilon 0.0162, total time 723, loss -, compute time 13.41\n",
      "episode 4582, reward 1528, memory_length 2000, epsilon 0.01618, total time 723, loss -, compute time 12.85\n",
      "episode 4583, reward 1747, memory_length 2000, epsilon 0.01617, total time 727, loss -, compute time 12.0\n",
      "episode 4584, reward 1820, memory_length 2000, epsilon 0.01615, total time 724, loss -, compute time 12.61\n",
      "episode 4585, reward 1629, memory_length 2000, epsilon 0.01614, total time 726, loss -, compute time 12.83\n",
      "episode 4586, reward 1686, memory_length 2000, epsilon 0.01612, total time 724, loss -, compute time 13.14\n",
      "episode 4587, reward 1686, memory_length 2000, epsilon 0.01611, total time 726, loss -, compute time 12.51\n",
      "episode 4588, reward 1839, memory_length 2000, epsilon 0.0161, total time 721, loss -, compute time 13.65\n",
      "episode 4589, reward 1659, memory_length 2000, epsilon 0.01608, total time 727, loss -, compute time 12.92\n",
      "episode 4590, reward 1818, memory_length 2000, epsilon 0.01607, total time 728, loss -, compute time 13.19\n",
      "Saving model for episode: 4590\n",
      "episode 4591, reward 1783, memory_length 2000, epsilon 0.01605, total time 723, loss -, compute time 11.5\n",
      "episode 4592, reward 1577, memory_length 2000, epsilon 0.01604, total time 721, loss -, compute time 11.67\n",
      "episode 4593, reward 1683, memory_length 2000, epsilon 0.01602, total time 728, loss -, compute time 15.59\n",
      "episode 4594, reward 1890, memory_length 2000, epsilon 0.01601, total time 723, loss -, compute time 13.22\n",
      "episode 4595, reward 1964, memory_length 2000, epsilon 0.01599, total time 722, loss -, compute time 12.45\n",
      "episode 4596, reward 1792, memory_length 2000, epsilon 0.01598, total time 721, loss -, compute time 14.0\n",
      "episode 4597, reward 1595, memory_length 2000, epsilon 0.01597, total time 730, loss -, compute time 13.64\n",
      "episode 4598, reward 1847, memory_length 2000, epsilon 0.01595, total time 726, loss -, compute time 13.66\n",
      "episode 4599, reward 1702, memory_length 2000, epsilon 0.01594, total time 722, loss -, compute time 13.64\n",
      "episode 4600, reward 1666, memory_length 2000, epsilon 0.01592, total time 726, loss -, compute time 13.99\n",
      "Saving model for episode: 4600\n",
      "episode 4601, reward 1499, memory_length 2000, epsilon 0.01591, total time 724, loss -, compute time 12.16\n",
      "episode 4602, reward 1522, memory_length 2000, epsilon 0.01589, total time 725, loss -, compute time 15.95\n",
      "episode 4603, reward 1829, memory_length 2000, epsilon 0.01588, total time 727, loss -, compute time 14.24\n",
      "episode 4604, reward 1721, memory_length 2000, epsilon 0.01587, total time 724, loss -, compute time 14.44\n",
      "episode 4605, reward 1801, memory_length 2000, epsilon 0.01585, total time 721, loss -, compute time 13.12\n",
      "episode 4606, reward 1926, memory_length 2000, epsilon 0.01584, total time 722, loss -, compute time 15.12\n",
      "episode 4607, reward 1614, memory_length 2000, epsilon 0.01582, total time 723, loss -, compute time 14.64\n",
      "episode 4608, reward 1256, memory_length 2000, epsilon 0.01581, total time 724, loss -, compute time 13.2\n",
      "episode 4609, reward 1823, memory_length 2000, epsilon 0.01579, total time 722, loss -, compute time 12.84\n",
      "episode 4610, reward 1455, memory_length 2000, epsilon 0.01578, total time 723, loss -, compute time 12.91\n",
      "Saving model for episode: 4610\n",
      "episode 4611, reward 1848, memory_length 2000, epsilon 0.01577, total time 727, loss -, compute time 14.23\n",
      "episode 4612, reward 1611, memory_length 2000, epsilon 0.01575, total time 728, loss -, compute time 15.07\n",
      "episode 4613, reward 1859, memory_length 2000, epsilon 0.01574, total time 726, loss -, compute time 15.2\n",
      "episode 4614, reward 1774, memory_length 2000, epsilon 0.01572, total time 730, loss -, compute time 13.96\n",
      "episode 4615, reward 1897, memory_length 2000, epsilon 0.01571, total time 722, loss -, compute time 13.67\n",
      "episode 4616, reward 1719, memory_length 2000, epsilon 0.0157, total time 726, loss -, compute time 15.18\n",
      "episode 4617, reward 1696, memory_length 2000, epsilon 0.01568, total time 727, loss -, compute time 16.12\n",
      "episode 4618, reward 1620, memory_length 2000, epsilon 0.01567, total time 721, loss -, compute time 13.96\n",
      "episode 4619, reward 1623, memory_length 2000, epsilon 0.01565, total time 724, loss -, compute time 15.37\n",
      "episode 4620, reward 1633, memory_length 2000, epsilon 0.01564, total time 724, loss -, compute time 15.5\n",
      "Saving model for episode: 4620\n",
      "episode 4621, reward 1635, memory_length 2000, epsilon 0.01562, total time 727, loss -, compute time 13.56\n",
      "episode 4622, reward 1530, memory_length 2000, epsilon 0.01561, total time 723, loss -, compute time 14.08\n",
      "episode 4623, reward 1764, memory_length 2000, epsilon 0.0156, total time 727, loss -, compute time 14.37\n",
      "episode 4624, reward 1719, memory_length 2000, epsilon 0.01558, total time 729, loss -, compute time 15.64\n",
      "episode 4625, reward 1514, memory_length 2000, epsilon 0.01557, total time 724, loss -, compute time 14.8\n",
      "episode 4626, reward 1512, memory_length 2000, epsilon 0.01555, total time 721, loss -, compute time 14.93\n",
      "episode 4627, reward 1598, memory_length 2000, epsilon 0.01554, total time 723, loss -, compute time 15.16\n",
      "episode 4628, reward 1823, memory_length 2000, epsilon 0.01553, total time 723, loss -, compute time 15.02\n",
      "episode 4629, reward 1712, memory_length 2000, epsilon 0.01551, total time 722, loss -, compute time 15.12\n",
      "episode 4630, reward 1998, memory_length 2000, epsilon 0.0155, total time 731, loss -, compute time 14.38\n",
      "Saving model for episode: 4630\n",
      "episode 4631, reward 1652, memory_length 2000, epsilon 0.01548, total time 725, loss -, compute time 14.26\n",
      "episode 4632, reward 1724, memory_length 2000, epsilon 0.01547, total time 723, loss -, compute time 14.46\n",
      "episode 4633, reward 1517, memory_length 2000, epsilon 0.01546, total time 732, loss -, compute time 15.0\n",
      "episode 4634, reward 1886, memory_length 2000, epsilon 0.01544, total time 728, loss -, compute time 16.47\n",
      "episode 4635, reward 1542, memory_length 2000, epsilon 0.01543, total time 725, loss -, compute time 14.59\n",
      "episode 4636, reward 1623, memory_length 2000, epsilon 0.01542, total time 721, loss -, compute time 14.01\n",
      "episode 4637, reward 1458, memory_length 2000, epsilon 0.0154, total time 725, loss -, compute time 12.94\n",
      "episode 4638, reward 1763, memory_length 2000, epsilon 0.01539, total time 722, loss -, compute time 16.35\n",
      "episode 4639, reward 1903, memory_length 2000, epsilon 0.01537, total time 724, loss -, compute time 15.58\n",
      "episode 4640, reward 1733, memory_length 2000, epsilon 0.01536, total time 727, loss -, compute time 15.09\n",
      "Saving model for episode: 4640\n",
      "episode 4641, reward 1531, memory_length 2000, epsilon 0.01535, total time 727, loss -, compute time 15.79\n",
      "episode 4642, reward 1701, memory_length 2000, epsilon 0.01533, total time 728, loss -, compute time 14.55\n",
      "episode 4643, reward 1638, memory_length 2000, epsilon 0.01532, total time 722, loss -, compute time 14.95\n",
      "episode 4644, reward 1616, memory_length 2000, epsilon 0.0153, total time 725, loss -, compute time 13.48\n",
      "episode 4645, reward 1823, memory_length 2000, epsilon 0.01529, total time 725, loss -, compute time 14.63\n",
      "episode 4646, reward 1697, memory_length 2000, epsilon 0.01528, total time 723, loss -, compute time 14.65\n",
      "episode 4647, reward 1630, memory_length 2000, epsilon 0.01526, total time 724, loss -, compute time 16.31\n",
      "episode 4648, reward 1796, memory_length 2000, epsilon 0.01525, total time 728, loss -, compute time 16.1\n",
      "episode 4649, reward 1604, memory_length 2000, epsilon 0.01524, total time 723, loss -, compute time 15.27\n",
      "episode 4650, reward 1701, memory_length 2000, epsilon 0.01522, total time 731, loss -, compute time 17.09\n",
      "Saving model for episode: 4650\n",
      "episode 4651, reward 1814, memory_length 2000, epsilon 0.01521, total time 728, loss -, compute time 15.03\n",
      "episode 4652, reward 1815, memory_length 2000, epsilon 0.01519, total time 726, loss -, compute time 14.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4653, reward 1681, memory_length 2000, epsilon 0.01518, total time 723, loss -, compute time 14.73\n",
      "episode 4654, reward 1715, memory_length 2000, epsilon 0.01517, total time 725, loss -, compute time 15.28\n",
      "episode 4655, reward 1684, memory_length 2000, epsilon 0.01515, total time 727, loss -, compute time 16.0\n",
      "episode 4656, reward 1962, memory_length 2000, epsilon 0.01514, total time 721, loss -, compute time 15.4\n",
      "episode 4657, reward 1656, memory_length 2000, epsilon 0.01513, total time 722, loss -, compute time 14.95\n",
      "episode 4658, reward 1709, memory_length 2000, epsilon 0.01511, total time 722, loss -, compute time 13.66\n",
      "episode 4659, reward 1835, memory_length 2000, epsilon 0.0151, total time 721, loss -, compute time 13.31\n",
      "episode 4660, reward 1766, memory_length 2000, epsilon 0.01509, total time 725, loss -, compute time 14.62\n",
      "Saving model for episode: 4660\n",
      "episode 4661, reward 1589, memory_length 2000, epsilon 0.01507, total time 731, loss -, compute time 15.01\n",
      "episode 4662, reward 1854, memory_length 2000, epsilon 0.01506, total time 728, loss -, compute time 14.63\n",
      "episode 4663, reward 1634, memory_length 2000, epsilon 0.01505, total time 734, loss -, compute time 13.48\n",
      "episode 4664, reward 1670, memory_length 2000, epsilon 0.01503, total time 721, loss -, compute time 14.61\n",
      "episode 4665, reward 1736, memory_length 2000, epsilon 0.01502, total time 723, loss -, compute time 15.59\n",
      "episode 4666, reward 1863, memory_length 2000, epsilon 0.015, total time 723, loss -, compute time 14.81\n",
      "episode 4667, reward 1778, memory_length 2000, epsilon 0.01499, total time 727, loss -, compute time 15.27\n",
      "episode 4668, reward 1589, memory_length 2000, epsilon 0.01498, total time 725, loss -, compute time 14.42\n",
      "episode 4669, reward 1779, memory_length 2000, epsilon 0.01496, total time 721, loss -, compute time 15.22\n",
      "episode 4670, reward 1800, memory_length 2000, epsilon 0.01495, total time 726, loss -, compute time 16.56\n",
      "Saving model for episode: 4670\n",
      "episode 4671, reward 1698, memory_length 2000, epsilon 0.01494, total time 729, loss -, compute time 15.05\n",
      "episode 4672, reward 1594, memory_length 2000, epsilon 0.01492, total time 726, loss -, compute time 13.56\n",
      "episode 4673, reward 1802, memory_length 2000, epsilon 0.01491, total time 729, loss -, compute time 16.12\n",
      "episode 4674, reward 1554, memory_length 2000, epsilon 0.0149, total time 729, loss -, compute time 14.76\n",
      "episode 4675, reward 1401, memory_length 2000, epsilon 0.01488, total time 728, loss -, compute time 15.81\n",
      "episode 4676, reward 2102, memory_length 2000, epsilon 0.01487, total time 726, loss -, compute time 14.31\n",
      "episode 4677, reward 1513, memory_length 2000, epsilon 0.01486, total time 725, loss -, compute time 16.36\n",
      "episode 4678, reward 1566, memory_length 2000, epsilon 0.01484, total time 723, loss -, compute time 14.57\n",
      "episode 4679, reward 1681, memory_length 2000, epsilon 0.01483, total time 721, loss -, compute time 15.27\n",
      "episode 4680, reward 1578, memory_length 2000, epsilon 0.01482, total time 726, loss -, compute time 15.46\n",
      "Saving model for episode: 4680\n",
      "episode 4681, reward 1801, memory_length 2000, epsilon 0.0148, total time 728, loss -, compute time 16.39\n",
      "episode 4682, reward 1647, memory_length 2000, epsilon 0.01479, total time 726, loss -, compute time 15.69\n",
      "episode 4683, reward 1653, memory_length 2000, epsilon 0.01478, total time 723, loss -, compute time 14.76\n",
      "episode 4684, reward 1755, memory_length 2000, epsilon 0.01476, total time 725, loss -, compute time 14.76\n",
      "episode 4685, reward 1697, memory_length 2000, epsilon 0.01475, total time 729, loss -, compute time 13.43\n",
      "episode 4686, reward 1581, memory_length 2000, epsilon 0.01474, total time 726, loss -, compute time 15.3\n",
      "episode 4687, reward 1733, memory_length 2000, epsilon 0.01472, total time 726, loss -, compute time 14.48\n",
      "episode 4688, reward 1680, memory_length 2000, epsilon 0.01471, total time 721, loss -, compute time 14.54\n",
      "episode 4689, reward 1668, memory_length 2000, epsilon 0.0147, total time 725, loss -, compute time 14.65\n",
      "episode 4690, reward 1913, memory_length 2000, epsilon 0.01468, total time 721, loss -, compute time 15.18\n",
      "Saving model for episode: 4690\n",
      "episode 4691, reward 1870, memory_length 2000, epsilon 0.01467, total time 723, loss -, compute time 13.42\n",
      "episode 4692, reward 1677, memory_length 2000, epsilon 0.01466, total time 724, loss -, compute time 16.47\n",
      "episode 4693, reward 1669, memory_length 2000, epsilon 0.01464, total time 723, loss -, compute time 14.61\n",
      "episode 4694, reward 1890, memory_length 2000, epsilon 0.01463, total time 724, loss -, compute time 14.54\n",
      "episode 4695, reward 1571, memory_length 2000, epsilon 0.01462, total time 725, loss -, compute time 13.64\n",
      "episode 4696, reward 1731, memory_length 2000, epsilon 0.0146, total time 725, loss -, compute time 14.5\n",
      "episode 4697, reward 1730, memory_length 2000, epsilon 0.01459, total time 723, loss -, compute time 14.79\n",
      "episode 4698, reward 1447, memory_length 2000, epsilon 0.01458, total time 721, loss -, compute time 14.76\n",
      "episode 4699, reward 1378, memory_length 2000, epsilon 0.01457, total time 727, loss -, compute time 15.25\n",
      "episode 4700, reward 1742, memory_length 2000, epsilon 0.01455, total time 722, loss -, compute time 15.48\n",
      "Saving model for episode: 4700\n",
      "episode 4701, reward 1673, memory_length 2000, epsilon 0.01454, total time 722, loss -, compute time 14.01\n",
      "episode 4702, reward 1545, memory_length 2000, epsilon 0.01453, total time 721, loss -, compute time 14.18\n",
      "episode 4703, reward 1602, memory_length 2000, epsilon 0.01451, total time 722, loss -, compute time 14.35\n",
      "episode 4704, reward 1736, memory_length 2000, epsilon 0.0145, total time 722, loss -, compute time 14.35\n",
      "episode 4705, reward 1629, memory_length 2000, epsilon 0.01449, total time 728, loss -, compute time 14.82\n",
      "episode 4706, reward 1447, memory_length 2000, epsilon 0.01447, total time 726, loss -, compute time 17.69\n",
      "episode 4707, reward 1374, memory_length 2000, epsilon 0.01446, total time 721, loss -, compute time 15.77\n",
      "episode 4708, reward 1805, memory_length 2000, epsilon 0.01445, total time 725, loss -, compute time 12.94\n",
      "episode 4709, reward 1548, memory_length 2000, epsilon 0.01443, total time 726, loss -, compute time 15.66\n",
      "episode 4710, reward 2124, memory_length 2000, epsilon 0.01442, total time 726, loss -, compute time 15.8\n",
      "Saving model for episode: 4710\n",
      "episode 4711, reward 1837, memory_length 2000, epsilon 0.01441, total time 722, loss -, compute time 15.03\n",
      "episode 4712, reward 1861, memory_length 2000, epsilon 0.0144, total time 723, loss -, compute time 13.71\n",
      "episode 4713, reward 1702, memory_length 2000, epsilon 0.01438, total time 724, loss -, compute time 15.04\n",
      "episode 4714, reward 1413, memory_length 2000, epsilon 0.01437, total time 725, loss -, compute time 14.93\n",
      "episode 4715, reward 1854, memory_length 2000, epsilon 0.01436, total time 726, loss -, compute time 15.58\n",
      "episode 4716, reward 1688, memory_length 2000, epsilon 0.01434, total time 723, loss -, compute time 14.88\n",
      "episode 4717, reward 1764, memory_length 2000, epsilon 0.01433, total time 728, loss -, compute time 13.51\n",
      "episode 4718, reward 1468, memory_length 2000, epsilon 0.01432, total time 726, loss -, compute time 14.25\n",
      "episode 4719, reward 1755, memory_length 2000, epsilon 0.01431, total time 721, loss -, compute time 16.81\n",
      "episode 4720, reward 1683, memory_length 2000, epsilon 0.01429, total time 726, loss -, compute time 15.82\n",
      "Saving model for episode: 4720\n",
      "episode 4721, reward 1840, memory_length 2000, epsilon 0.01428, total time 723, loss -, compute time 14.82\n",
      "episode 4722, reward 1804, memory_length 2000, epsilon 0.01427, total time 724, loss -, compute time 14.92\n",
      "episode 4723, reward 1766, memory_length 2000, epsilon 0.01425, total time 724, loss -, compute time 14.36\n",
      "episode 4724, reward 1618, memory_length 2000, epsilon 0.01424, total time 727, loss -, compute time 16.07\n",
      "episode 4725, reward 1737, memory_length 2000, epsilon 0.01423, total time 730, loss -, compute time 14.63\n",
      "episode 4726, reward 1827, memory_length 2000, epsilon 0.01422, total time 721, loss -, compute time 13.68\n",
      "episode 4727, reward 1634, memory_length 2000, epsilon 0.0142, total time 730, loss -, compute time 13.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4728, reward 1548, memory_length 2000, epsilon 0.01419, total time 724, loss -, compute time 15.69\n",
      "episode 4729, reward 1833, memory_length 2000, epsilon 0.01418, total time 732, loss -, compute time 14.23\n",
      "episode 4730, reward 1877, memory_length 2000, epsilon 0.01416, total time 722, loss -, compute time 13.08\n",
      "Saving model for episode: 4730\n",
      "episode 4731, reward 1776, memory_length 2000, epsilon 0.01415, total time 722, loss -, compute time 12.29\n",
      "episode 4732, reward 1804, memory_length 2000, epsilon 0.01414, total time 721, loss -, compute time 12.08\n",
      "episode 4733, reward 1595, memory_length 2000, epsilon 0.01413, total time 722, loss -, compute time 13.06\n",
      "episode 4734, reward 1683, memory_length 2000, epsilon 0.01411, total time 725, loss [1.7060751914978027], compute time 12.81\n",
      "episode 4735, reward 1344, memory_length 2000, epsilon 0.0141, total time 723, loss -, compute time 13.0\n",
      "episode 4736, reward 1579, memory_length 2000, epsilon 0.01409, total time 725, loss -, compute time 11.86\n",
      "episode 4737, reward 1722, memory_length 2000, epsilon 0.01408, total time 722, loss -, compute time 14.63\n",
      "episode 4738, reward 1680, memory_length 2000, epsilon 0.01406, total time 727, loss -, compute time 11.98\n",
      "episode 4739, reward 1545, memory_length 2000, epsilon 0.01405, total time 727, loss -, compute time 13.6\n",
      "episode 4740, reward 1746, memory_length 2000, epsilon 0.01404, total time 724, loss -, compute time 12.16\n",
      "Saving model for episode: 4740\n",
      "episode 4741, reward 1836, memory_length 2000, epsilon 0.01403, total time 726, loss -, compute time 12.66\n",
      "episode 4742, reward 1683, memory_length 2000, epsilon 0.01401, total time 726, loss -, compute time 14.17\n",
      "episode 4743, reward 2093, memory_length 2000, epsilon 0.014, total time 726, loss -, compute time 11.81\n",
      "episode 4744, reward 1684, memory_length 2000, epsilon 0.01399, total time 722, loss -, compute time 12.58\n",
      "episode 4745, reward 1633, memory_length 2000, epsilon 0.01397, total time 729, loss -, compute time 12.75\n",
      "episode 4746, reward 1653, memory_length 2000, epsilon 0.01396, total time 724, loss -, compute time 13.02\n",
      "episode 4747, reward 1668, memory_length 2000, epsilon 0.01395, total time 724, loss -, compute time 12.92\n",
      "episode 4748, reward 1645, memory_length 2000, epsilon 0.01394, total time 721, loss -, compute time 11.31\n",
      "episode 4749, reward 1693, memory_length 2000, epsilon 0.01392, total time 725, loss -, compute time 13.6\n",
      "episode 4750, reward 1562, memory_length 2000, epsilon 0.01391, total time 725, loss -, compute time 13.05\n",
      "Saving model for episode: 4750\n",
      "episode 4751, reward 1863, memory_length 2000, epsilon 0.0139, total time 724, loss -, compute time 13.46\n",
      "episode 4752, reward 1954, memory_length 2000, epsilon 0.01389, total time 729, loss -, compute time 11.8\n",
      "episode 4753, reward 1695, memory_length 2000, epsilon 0.01387, total time 725, loss -, compute time 14.26\n",
      "episode 4754, reward 1696, memory_length 2000, epsilon 0.01386, total time 724, loss -, compute time 12.51\n",
      "episode 4755, reward 1536, memory_length 2000, epsilon 0.01385, total time 722, loss -, compute time 13.83\n",
      "episode 4756, reward 1737, memory_length 2000, epsilon 0.01384, total time 726, loss -, compute time 14.3\n",
      "episode 4757, reward 1498, memory_length 2000, epsilon 0.01382, total time 723, loss -, compute time 12.73\n",
      "episode 4758, reward 1495, memory_length 2000, epsilon 0.01381, total time 733, loss -, compute time 13.24\n",
      "episode 4759, reward 1785, memory_length 2000, epsilon 0.0138, total time 724, loss -, compute time 12.99\n",
      "episode 4760, reward 1822, memory_length 2000, epsilon 0.01379, total time 721, loss -, compute time 11.18\n",
      "Saving model for episode: 4760\n",
      "episode 4761, reward 1748, memory_length 2000, epsilon 0.01377, total time 724, loss -, compute time 12.85\n",
      "episode 4762, reward 1683, memory_length 2000, epsilon 0.01376, total time 721, loss -, compute time 14.57\n",
      "episode 4763, reward 1761, memory_length 2000, epsilon 0.01375, total time 721, loss -, compute time 13.07\n",
      "episode 4764, reward 1816, memory_length 2000, epsilon 0.01374, total time 725, loss -, compute time 12.23\n",
      "episode 4765, reward 1786, memory_length 2000, epsilon 0.01373, total time 721, loss -, compute time 11.91\n",
      "episode 4766, reward 1805, memory_length 2000, epsilon 0.01371, total time 727, loss -, compute time 13.96\n",
      "episode 4767, reward 1681, memory_length 2000, epsilon 0.0137, total time 725, loss -, compute time 12.59\n",
      "episode 4768, reward 1702, memory_length 2000, epsilon 0.01369, total time 728, loss -, compute time 11.7\n",
      "episode 4769, reward 1656, memory_length 2000, epsilon 0.01368, total time 727, loss -, compute time 12.57\n",
      "episode 4770, reward 1724, memory_length 2000, epsilon 0.01366, total time 721, loss -, compute time 14.22\n",
      "Saving model for episode: 4770\n",
      "episode 4771, reward 1647, memory_length 2000, epsilon 0.01365, total time 729, loss -, compute time 12.1\n",
      "episode 4772, reward 1566, memory_length 2000, epsilon 0.01364, total time 726, loss -, compute time 11.39\n",
      "episode 4773, reward 1639, memory_length 2000, epsilon 0.01363, total time 724, loss -, compute time 12.41\n",
      "episode 4774, reward 1749, memory_length 2000, epsilon 0.01361, total time 722, loss [1.9152278900146484], compute time 12.1\n",
      "episode 4775, reward 1707, memory_length 2000, epsilon 0.0136, total time 725, loss -, compute time 11.49\n",
      "episode 4776, reward 1384, memory_length 2000, epsilon 0.01359, total time 721, loss -, compute time 12.95\n",
      "episode 4777, reward 1479, memory_length 2000, epsilon 0.01358, total time 723, loss -, compute time 13.87\n",
      "episode 4778, reward 1813, memory_length 2000, epsilon 0.01357, total time 724, loss -, compute time 13.02\n",
      "episode 4779, reward 1216, memory_length 2000, epsilon 0.01355, total time 724, loss -, compute time 13.38\n",
      "episode 4780, reward 1618, memory_length 2000, epsilon 0.01354, total time 725, loss -, compute time 11.3\n",
      "Saving model for episode: 4780\n",
      "episode 4781, reward 1548, memory_length 2000, epsilon 0.01353, total time 724, loss -, compute time 11.87\n",
      "episode 4782, reward 1729, memory_length 2000, epsilon 0.01352, total time 729, loss -, compute time 13.15\n",
      "episode 4783, reward 1622, memory_length 2000, epsilon 0.0135, total time 722, loss -, compute time 13.15\n",
      "episode 4784, reward 1594, memory_length 2000, epsilon 0.01349, total time 722, loss -, compute time 14.49\n",
      "episode 4785, reward 1638, memory_length 2000, epsilon 0.01348, total time 731, loss -, compute time 12.93\n",
      "episode 4786, reward 1612, memory_length 2000, epsilon 0.01347, total time 721, loss -, compute time 12.77\n",
      "episode 4787, reward 1598, memory_length 2000, epsilon 0.01346, total time 721, loss -, compute time 13.54\n",
      "episode 4788, reward 1478, memory_length 2000, epsilon 0.01344, total time 722, loss -, compute time 13.45\n",
      "episode 4789, reward 1582, memory_length 2000, epsilon 0.01343, total time 724, loss -, compute time 13.4\n",
      "episode 4790, reward 1413, memory_length 2000, epsilon 0.01342, total time 722, loss -, compute time 13.22\n",
      "Saving model for episode: 4790\n",
      "episode 4791, reward 1524, memory_length 2000, epsilon 0.01341, total time 725, loss -, compute time 13.21\n",
      "episode 4792, reward 1573, memory_length 2000, epsilon 0.0134, total time 723, loss -, compute time 11.95\n",
      "episode 4793, reward 1977, memory_length 2000, epsilon 0.01338, total time 725, loss -, compute time 11.92\n",
      "episode 4794, reward 1368, memory_length 2000, epsilon 0.01337, total time 731, loss -, compute time 12.73\n",
      "episode 4795, reward 1778, memory_length 2000, epsilon 0.01336, total time 732, loss -, compute time 13.96\n",
      "episode 4796, reward 1809, memory_length 2000, epsilon 0.01335, total time 730, loss -, compute time 13.55\n",
      "episode 4797, reward 1671, memory_length 2000, epsilon 0.01334, total time 721, loss -, compute time 12.46\n",
      "episode 4798, reward 1676, memory_length 2000, epsilon 0.01332, total time 724, loss -, compute time 12.06\n",
      "episode 4799, reward 1701, memory_length 2000, epsilon 0.01331, total time 728, loss -, compute time 12.74\n",
      "episode 4800, reward 1940, memory_length 2000, epsilon 0.0133, total time 728, loss -, compute time 12.36\n",
      "Saving model for episode: 4800\n",
      "episode 4801, reward 1597, memory_length 2000, epsilon 0.01329, total time 721, loss -, compute time 12.62\n",
      "episode 4802, reward 1800, memory_length 2000, epsilon 0.01328, total time 731, loss -, compute time 12.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4803, reward 1308, memory_length 2000, epsilon 0.01326, total time 721, loss -, compute time 12.95\n",
      "episode 4804, reward 1693, memory_length 2000, epsilon 0.01325, total time 724, loss -, compute time 12.47\n",
      "episode 4805, reward 1793, memory_length 2000, epsilon 0.01324, total time 723, loss -, compute time 12.35\n",
      "episode 4806, reward 1835, memory_length 2000, epsilon 0.01323, total time 723, loss -, compute time 13.6\n",
      "episode 4807, reward 1671, memory_length 2000, epsilon 0.01322, total time 728, loss -, compute time 13.16\n",
      "episode 4808, reward 1878, memory_length 2000, epsilon 0.0132, total time 727, loss -, compute time 13.75\n",
      "episode 4809, reward 1428, memory_length 2000, epsilon 0.01319, total time 723, loss -, compute time 13.66\n",
      "episode 4810, reward 1517, memory_length 2000, epsilon 0.01318, total time 721, loss -, compute time 12.48\n",
      "Saving model for episode: 4810\n",
      "episode 4811, reward 1669, memory_length 2000, epsilon 0.01317, total time 724, loss -, compute time 12.71\n",
      "episode 4812, reward 1553, memory_length 2000, epsilon 0.01316, total time 727, loss -, compute time 13.19\n",
      "episode 4813, reward 1575, memory_length 2000, epsilon 0.01315, total time 724, loss -, compute time 12.39\n",
      "episode 4814, reward 1734, memory_length 2000, epsilon 0.01313, total time 721, loss -, compute time 13.02\n",
      "episode 4815, reward 1729, memory_length 2000, epsilon 0.01312, total time 721, loss -, compute time 12.17\n",
      "episode 4816, reward 1890, memory_length 2000, epsilon 0.01311, total time 721, loss -, compute time 12.09\n",
      "episode 4817, reward 1699, memory_length 2000, epsilon 0.0131, total time 729, loss -, compute time 14.6\n",
      "episode 4818, reward 1703, memory_length 2000, epsilon 0.01309, total time 724, loss -, compute time 13.34\n",
      "episode 4819, reward 1440, memory_length 2000, epsilon 0.01307, total time 728, loss -, compute time 12.89\n",
      "episode 4820, reward 1985, memory_length 2000, epsilon 0.01306, total time 726, loss -, compute time 12.57\n",
      "Saving model for episode: 4820\n",
      "episode 4821, reward 1589, memory_length 2000, epsilon 0.01305, total time 721, loss -, compute time 13.27\n",
      "episode 4822, reward 1634, memory_length 2000, epsilon 0.01304, total time 723, loss -, compute time 13.53\n",
      "episode 4823, reward 1689, memory_length 2000, epsilon 0.01303, total time 722, loss -, compute time 12.41\n",
      "episode 4824, reward 1769, memory_length 2000, epsilon 0.01302, total time 726, loss -, compute time 12.53\n",
      "episode 4825, reward 1735, memory_length 2000, epsilon 0.013, total time 723, loss -, compute time 12.94\n",
      "episode 4826, reward 1364, memory_length 2000, epsilon 0.01299, total time 727, loss -, compute time 13.64\n",
      "episode 4827, reward 1868, memory_length 2000, epsilon 0.01298, total time 727, loss -, compute time 12.88\n",
      "episode 4828, reward 1608, memory_length 2000, epsilon 0.01297, total time 722, loss -, compute time 12.88\n",
      "episode 4829, reward 1448, memory_length 2000, epsilon 0.01296, total time 725, loss -, compute time 14.36\n",
      "episode 4830, reward 1647, memory_length 2000, epsilon 0.01295, total time 726, loss -, compute time 13.86\n",
      "Saving model for episode: 4830\n",
      "episode 4831, reward 1542, memory_length 2000, epsilon 0.01293, total time 723, loss -, compute time 13.39\n",
      "episode 4832, reward 1655, memory_length 2000, epsilon 0.01292, total time 729, loss -, compute time 14.39\n",
      "episode 4833, reward 1593, memory_length 2000, epsilon 0.01291, total time 721, loss -, compute time 14.58\n",
      "episode 4834, reward 1607, memory_length 2000, epsilon 0.0129, total time 730, loss -, compute time 13.23\n",
      "episode 4835, reward 1721, memory_length 2000, epsilon 0.01289, total time 722, loss -, compute time 13.31\n",
      "episode 4836, reward 1281, memory_length 2000, epsilon 0.01288, total time 721, loss -, compute time 14.58\n",
      "episode 4837, reward 1675, memory_length 2000, epsilon 0.01286, total time 725, loss -, compute time 11.83\n",
      "episode 4838, reward 1816, memory_length 2000, epsilon 0.01285, total time 724, loss -, compute time 13.29\n",
      "episode 4839, reward 1557, memory_length 2000, epsilon 0.01284, total time 721, loss -, compute time 12.71\n",
      "episode 4840, reward 1476, memory_length 2000, epsilon 0.01283, total time 726, loss -, compute time 13.3\n",
      "Saving model for episode: 4840\n",
      "episode 4841, reward 1557, memory_length 2000, epsilon 0.01282, total time 722, loss -, compute time 13.63\n",
      "episode 4842, reward 1685, memory_length 2000, epsilon 0.01281, total time 721, loss -, compute time 13.78\n",
      "episode 4843, reward 1875, memory_length 2000, epsilon 0.0128, total time 724, loss -, compute time 12.96\n",
      "episode 4844, reward 1556, memory_length 2000, epsilon 0.01278, total time 721, loss -, compute time 13.39\n",
      "episode 4845, reward 1700, memory_length 2000, epsilon 0.01277, total time 724, loss -, compute time 13.56\n",
      "episode 4846, reward 1423, memory_length 2000, epsilon 0.01276, total time 725, loss -, compute time 13.08\n",
      "episode 4847, reward 1607, memory_length 2000, epsilon 0.01275, total time 727, loss -, compute time 13.28\n",
      "episode 4848, reward 1725, memory_length 2000, epsilon 0.01274, total time 724, loss -, compute time 13.17\n",
      "episode 4849, reward 1646, memory_length 2000, epsilon 0.01273, total time 723, loss -, compute time 13.96\n",
      "episode 4850, reward 1612, memory_length 2000, epsilon 0.01271, total time 727, loss -, compute time 12.2\n",
      "Saving model for episode: 4850\n",
      "episode 4851, reward 1689, memory_length 2000, epsilon 0.0127, total time 725, loss -, compute time 12.98\n",
      "episode 4852, reward 1711, memory_length 2000, epsilon 0.01269, total time 727, loss -, compute time 12.54\n",
      "episode 4853, reward 1643, memory_length 2000, epsilon 0.01268, total time 729, loss -, compute time 12.22\n",
      "episode 4854, reward 1656, memory_length 2000, epsilon 0.01267, total time 728, loss -, compute time 13.67\n",
      "episode 4855, reward 1830, memory_length 2000, epsilon 0.01266, total time 724, loss -, compute time 12.76\n",
      "episode 4856, reward 1508, memory_length 2000, epsilon 0.01265, total time 722, loss -, compute time 13.6\n",
      "episode 4857, reward 1467, memory_length 2000, epsilon 0.01263, total time 728, loss -, compute time 12.63\n",
      "episode 4858, reward 1630, memory_length 2000, epsilon 0.01262, total time 734, loss -, compute time 13.18\n",
      "episode 4859, reward 1449, memory_length 2000, epsilon 0.01261, total time 727, loss -, compute time 13.76\n",
      "episode 4860, reward 1458, memory_length 2000, epsilon 0.0126, total time 721, loss -, compute time 11.71\n",
      "Saving model for episode: 4860\n",
      "episode 4861, reward 1902, memory_length 2000, epsilon 0.01259, total time 721, loss -, compute time 12.35\n",
      "episode 4862, reward 1519, memory_length 2000, epsilon 0.01258, total time 722, loss -, compute time 13.14\n",
      "episode 4863, reward 1769, memory_length 2000, epsilon 0.01257, total time 726, loss -, compute time 13.79\n",
      "episode 4864, reward 1308, memory_length 2000, epsilon 0.01256, total time 724, loss -, compute time 13.26\n",
      "episode 4865, reward 1513, memory_length 2000, epsilon 0.01254, total time 725, loss -, compute time 14.11\n",
      "episode 4866, reward 1662, memory_length 2000, epsilon 0.01253, total time 724, loss -, compute time 12.52\n",
      "episode 4867, reward 1848, memory_length 2000, epsilon 0.01252, total time 725, loss -, compute time 11.59\n",
      "episode 4868, reward 1566, memory_length 2000, epsilon 0.01251, total time 730, loss -, compute time 13.75\n",
      "episode 4869, reward 1542, memory_length 2000, epsilon 0.0125, total time 721, loss -, compute time 11.48\n",
      "episode 4870, reward 1692, memory_length 2000, epsilon 0.01249, total time 726, loss -, compute time 12.72\n",
      "Saving model for episode: 4870\n",
      "episode 4871, reward 1827, memory_length 2000, epsilon 0.01248, total time 721, loss -, compute time 12.81\n",
      "episode 4872, reward 1697, memory_length 2000, epsilon 0.01247, total time 727, loss -, compute time 13.33\n",
      "episode 4873, reward 1753, memory_length 2000, epsilon 0.01245, total time 725, loss -, compute time 12.36\n",
      "episode 4874, reward 1675, memory_length 2000, epsilon 0.01244, total time 722, loss -, compute time 13.82\n",
      "episode 4875, reward 1608, memory_length 2000, epsilon 0.01243, total time 723, loss -, compute time 12.97\n",
      "episode 4876, reward 1827, memory_length 2000, epsilon 0.01242, total time 728, loss -, compute time 13.78\n",
      "episode 4877, reward 1636, memory_length 2000, epsilon 0.01241, total time 722, loss -, compute time 14.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4878, reward 1832, memory_length 2000, epsilon 0.0124, total time 729, loss -, compute time 14.65\n",
      "episode 4879, reward 1651, memory_length 2000, epsilon 0.01239, total time 721, loss -, compute time 11.27\n",
      "episode 4880, reward 1918, memory_length 2000, epsilon 0.01238, total time 726, loss -, compute time 13.94\n",
      "Saving model for episode: 4880\n",
      "episode 4881, reward 1773, memory_length 2000, epsilon 0.01236, total time 728, loss -, compute time 13.74\n",
      "episode 4882, reward 1773, memory_length 2000, epsilon 0.01235, total time 723, loss -, compute time 14.17\n",
      "episode 4883, reward 1655, memory_length 2000, epsilon 0.01234, total time 721, loss -, compute time 12.68\n",
      "episode 4884, reward 1985, memory_length 2000, epsilon 0.01233, total time 727, loss -, compute time 14.24\n",
      "episode 4885, reward 1951, memory_length 2000, epsilon 0.01232, total time 724, loss -, compute time 12.6\n",
      "episode 4886, reward 1672, memory_length 2000, epsilon 0.01231, total time 723, loss -, compute time 13.37\n",
      "episode 4887, reward 1673, memory_length 2000, epsilon 0.0123, total time 725, loss -, compute time 11.6\n",
      "episode 4888, reward 1706, memory_length 2000, epsilon 0.01229, total time 724, loss -, compute time 13.67\n",
      "episode 4889, reward 1707, memory_length 2000, epsilon 0.01228, total time 729, loss -, compute time 12.56\n",
      "episode 4890, reward 1580, memory_length 2000, epsilon 0.01227, total time 727, loss -, compute time 13.39\n",
      "Saving model for episode: 4890\n",
      "episode 4891, reward 1739, memory_length 2000, epsilon 0.01225, total time 723, loss -, compute time 13.39\n",
      "episode 4892, reward 1719, memory_length 2000, epsilon 0.01224, total time 721, loss -, compute time 13.43\n",
      "episode 4893, reward 1763, memory_length 2000, epsilon 0.01223, total time 725, loss -, compute time 12.38\n",
      "episode 4894, reward 1562, memory_length 2000, epsilon 0.01222, total time 726, loss -, compute time 14.62\n",
      "episode 4895, reward 1718, memory_length 2000, epsilon 0.01221, total time 723, loss -, compute time 12.94\n",
      "episode 4896, reward 1389, memory_length 2000, epsilon 0.0122, total time 723, loss -, compute time 12.78\n",
      "episode 4897, reward 1773, memory_length 2000, epsilon 0.01219, total time 722, loss -, compute time 11.73\n",
      "episode 4898, reward 1727, memory_length 2000, epsilon 0.01218, total time 721, loss -, compute time 12.66\n",
      "episode 4899, reward 1647, memory_length 2000, epsilon 0.01217, total time 725, loss -, compute time 13.44\n",
      "episode 4900, reward 1692, memory_length 2000, epsilon 0.01216, total time 721, loss -, compute time 13.53\n",
      "Saving model for episode: 4900\n",
      "episode 4901, reward 1721, memory_length 2000, epsilon 0.01214, total time 723, loss -, compute time 13.02\n",
      "episode 4902, reward 1657, memory_length 2000, epsilon 0.01213, total time 723, loss -, compute time 12.15\n",
      "episode 4903, reward 1412, memory_length 2000, epsilon 0.01212, total time 722, loss -, compute time 12.98\n",
      "episode 4904, reward 2122, memory_length 2000, epsilon 0.01211, total time 721, loss -, compute time 12.2\n",
      "episode 4905, reward 1873, memory_length 2000, epsilon 0.0121, total time 730, loss -, compute time 12.9\n",
      "episode 4906, reward 1563, memory_length 2000, epsilon 0.01209, total time 723, loss -, compute time 12.4\n",
      "episode 4907, reward 1629, memory_length 2000, epsilon 0.01208, total time 727, loss -, compute time 13.24\n",
      "episode 4908, reward 1746, memory_length 2000, epsilon 0.01207, total time 726, loss -, compute time 12.92\n",
      "episode 4909, reward 1574, memory_length 2000, epsilon 0.01206, total time 721, loss -, compute time 14.91\n",
      "episode 4910, reward 1524, memory_length 2000, epsilon 0.01205, total time 722, loss -, compute time 13.09\n",
      "Saving model for episode: 4910\n",
      "episode 4911, reward 1460, memory_length 2000, epsilon 0.01204, total time 723, loss -, compute time 13.29\n",
      "episode 4912, reward 1746, memory_length 2000, epsilon 0.01202, total time 724, loss -, compute time 15.12\n",
      "episode 4913, reward 1663, memory_length 2000, epsilon 0.01201, total time 721, loss -, compute time 12.24\n",
      "episode 4914, reward 1423, memory_length 2000, epsilon 0.012, total time 724, loss -, compute time 14.35\n",
      "episode 4915, reward 1810, memory_length 2000, epsilon 0.01199, total time 727, loss -, compute time 13.17\n",
      "episode 4916, reward 1796, memory_length 2000, epsilon 0.01198, total time 727, loss -, compute time 12.63\n",
      "episode 4917, reward 1854, memory_length 2000, epsilon 0.01197, total time 727, loss -, compute time 12.33\n",
      "episode 4918, reward 1544, memory_length 2000, epsilon 0.01196, total time 729, loss -, compute time 13.2\n",
      "episode 4919, reward 1490, memory_length 2000, epsilon 0.01195, total time 724, loss -, compute time 13.82\n",
      "episode 4920, reward 1656, memory_length 2000, epsilon 0.01194, total time 734, loss -, compute time 15.18\n",
      "Saving model for episode: 4920\n",
      "episode 4921, reward 1734, memory_length 2000, epsilon 0.01193, total time 724, loss -, compute time 13.31\n",
      "episode 4922, reward 1622, memory_length 2000, epsilon 0.01192, total time 727, loss -, compute time 13.15\n",
      "episode 4923, reward 1645, memory_length 2000, epsilon 0.01191, total time 722, loss -, compute time 12.36\n",
      "episode 4924, reward 1576, memory_length 2000, epsilon 0.0119, total time 727, loss -, compute time 13.19\n",
      "episode 4925, reward 1743, memory_length 2000, epsilon 0.01188, total time 724, loss -, compute time 13.3\n",
      "episode 4926, reward 1539, memory_length 2000, epsilon 0.01187, total time 721, loss -, compute time 12.32\n",
      "episode 4927, reward 1680, memory_length 2000, epsilon 0.01186, total time 726, loss -, compute time 14.99\n",
      "episode 4928, reward 1499, memory_length 2000, epsilon 0.01185, total time 725, loss -, compute time 14.4\n",
      "episode 4929, reward 1314, memory_length 2000, epsilon 0.01184, total time 725, loss -, compute time 12.13\n",
      "episode 4930, reward 1541, memory_length 2000, epsilon 0.01183, total time 723, loss -, compute time 13.76\n",
      "Saving model for episode: 4930\n",
      "episode 4931, reward 1743, memory_length 2000, epsilon 0.01182, total time 723, loss -, compute time 13.09\n",
      "episode 4932, reward 1653, memory_length 2000, epsilon 0.01181, total time 721, loss -, compute time 13.14\n",
      "episode 4933, reward 1517, memory_length 2000, epsilon 0.0118, total time 727, loss -, compute time 13.66\n",
      "episode 4934, reward 1623, memory_length 2000, epsilon 0.01179, total time 723, loss -, compute time 13.51\n",
      "episode 4935, reward 1725, memory_length 2000, epsilon 0.01178, total time 726, loss -, compute time 12.12\n",
      "episode 4936, reward 1785, memory_length 2000, epsilon 0.01177, total time 722, loss -, compute time 12.06\n",
      "episode 4937, reward 1695, memory_length 2000, epsilon 0.01176, total time 721, loss -, compute time 12.27\n",
      "episode 4938, reward 2150, memory_length 2000, epsilon 0.01175, total time 730, loss -, compute time 13.7\n",
      "episode 4939, reward 1768, memory_length 2000, epsilon 0.01174, total time 724, loss -, compute time 13.56\n",
      "episode 4940, reward 1773, memory_length 2000, epsilon 0.01173, total time 721, loss -, compute time 12.45\n",
      "Saving model for episode: 4940\n",
      "episode 4941, reward 1782, memory_length 2000, epsilon 0.01171, total time 721, loss -, compute time 13.37\n",
      "episode 4942, reward 1844, memory_length 2000, epsilon 0.0117, total time 724, loss -, compute time 11.63\n",
      "episode 4943, reward 1684, memory_length 2000, epsilon 0.01169, total time 722, loss -, compute time 12.54\n",
      "episode 4944, reward 1643, memory_length 2000, epsilon 0.01168, total time 728, loss -, compute time 11.94\n",
      "episode 4945, reward 1941, memory_length 2000, epsilon 0.01167, total time 727, loss -, compute time 13.88\n",
      "episode 4946, reward 1780, memory_length 2000, epsilon 0.01166, total time 725, loss -, compute time 13.62\n",
      "episode 4947, reward 1432, memory_length 2000, epsilon 0.01165, total time 732, loss -, compute time 13.75\n",
      "episode 4948, reward 1695, memory_length 2000, epsilon 0.01164, total time 721, loss -, compute time 13.56\n",
      "episode 4949, reward 1532, memory_length 2000, epsilon 0.01163, total time 725, loss -, compute time 11.36\n",
      "episode 4950, reward 1773, memory_length 2000, epsilon 0.01162, total time 725, loss -, compute time 13.01\n",
      "Saving model for episode: 4950\n",
      "episode 4951, reward 1719, memory_length 2000, epsilon 0.01161, total time 721, loss -, compute time 12.85\n",
      "episode 4952, reward 1748, memory_length 2000, epsilon 0.0116, total time 725, loss -, compute time 13.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4953, reward 1478, memory_length 2000, epsilon 0.01159, total time 722, loss -, compute time 13.5\n",
      "episode 4954, reward 1640, memory_length 2000, epsilon 0.01158, total time 722, loss -, compute time 14.02\n",
      "episode 4955, reward 1669, memory_length 2000, epsilon 0.01157, total time 723, loss -, compute time 13.57\n",
      "episode 4956, reward 1645, memory_length 2000, epsilon 0.01156, total time 723, loss [2.5436809062957764], compute time 14.21\n",
      "episode 4957, reward 1618, memory_length 2000, epsilon 0.01155, total time 722, loss -, compute time 13.7\n",
      "episode 4958, reward 1459, memory_length 2000, epsilon 0.01154, total time 726, loss -, compute time 12.54\n",
      "episode 4959, reward 1611, memory_length 2000, epsilon 0.01153, total time 729, loss -, compute time 13.24\n",
      "episode 4960, reward 1659, memory_length 2000, epsilon 0.01152, total time 723, loss -, compute time 12.06\n",
      "Saving model for episode: 4960\n",
      "episode 4961, reward 1764, memory_length 2000, epsilon 0.01151, total time 730, loss -, compute time 13.8\n",
      "episode 4962, reward 1831, memory_length 2000, epsilon 0.0115, total time 721, loss -, compute time 12.8\n",
      "episode 4963, reward 1575, memory_length 2000, epsilon 0.01149, total time 728, loss -, compute time 13.45\n",
      "episode 4964, reward 1712, memory_length 2000, epsilon 0.01147, total time 722, loss -, compute time 14.94\n",
      "episode 4965, reward 1582, memory_length 2000, epsilon 0.01146, total time 721, loss -, compute time 14.45\n",
      "episode 4966, reward 1559, memory_length 2000, epsilon 0.01145, total time 721, loss [1.741590976715088], compute time 13.1\n",
      "episode 4967, reward 1540, memory_length 2000, epsilon 0.01144, total time 726, loss -, compute time 12.97\n",
      "episode 4968, reward 1469, memory_length 2000, epsilon 0.01143, total time 722, loss -, compute time 13.59\n",
      "episode 4969, reward 1718, memory_length 2000, epsilon 0.01142, total time 721, loss [2.1594014167785645], compute time 11.71\n",
      "episode 4970, reward 1559, memory_length 2000, epsilon 0.01141, total time 723, loss -, compute time 13.3\n",
      "Saving model for episode: 4970\n",
      "episode 4971, reward 1597, memory_length 2000, epsilon 0.0114, total time 722, loss -, compute time 13.15\n",
      "episode 4972, reward 1832, memory_length 2000, epsilon 0.01139, total time 727, loss -, compute time 11.76\n",
      "episode 4973, reward 1564, memory_length 2000, epsilon 0.01138, total time 721, loss -, compute time 15.45\n",
      "episode 4974, reward 1850, memory_length 2000, epsilon 0.01137, total time 721, loss -, compute time 12.48\n",
      "episode 4975, reward 1793, memory_length 2000, epsilon 0.01136, total time 722, loss -, compute time 11.83\n",
      "episode 4976, reward 1598, memory_length 2000, epsilon 0.01135, total time 728, loss -, compute time 13.15\n",
      "episode 4977, reward 1842, memory_length 2000, epsilon 0.01134, total time 726, loss -, compute time 12.25\n",
      "episode 4978, reward 1657, memory_length 2000, epsilon 0.01133, total time 730, loss -, compute time 13.77\n",
      "episode 4979, reward 1937, memory_length 2000, epsilon 0.01132, total time 724, loss -, compute time 13.18\n",
      "episode 4980, reward 1434, memory_length 2000, epsilon 0.01131, total time 725, loss -, compute time 14.83\n",
      "Saving model for episode: 4980\n",
      "episode 4981, reward 1845, memory_length 2000, epsilon 0.0113, total time 725, loss -, compute time 12.31\n",
      "episode 4982, reward 2066, memory_length 2000, epsilon 0.01129, total time 731, loss -, compute time 13.62\n",
      "episode 4983, reward 1707, memory_length 2000, epsilon 0.01128, total time 723, loss -, compute time 14.61\n",
      "episode 4984, reward 1572, memory_length 2000, epsilon 0.01127, total time 727, loss -, compute time 16.26\n",
      "episode 4985, reward 1642, memory_length 2000, epsilon 0.01126, total time 721, loss -, compute time 13.36\n",
      "episode 4986, reward 1544, memory_length 2000, epsilon 0.01125, total time 725, loss -, compute time 12.98\n",
      "episode 4987, reward 1827, memory_length 2000, epsilon 0.01124, total time 727, loss -, compute time 13.78\n",
      "episode 4988, reward 1499, memory_length 2000, epsilon 0.01123, total time 729, loss -, compute time 13.41\n",
      "episode 4989, reward 1557, memory_length 2000, epsilon 0.01122, total time 721, loss -, compute time 14.76\n",
      "episode 4990, reward 1760, memory_length 2000, epsilon 0.01121, total time 734, loss -, compute time 14.29\n",
      "Saving model for episode: 4990\n",
      "episode 4991, reward 1564, memory_length 2000, epsilon 0.0112, total time 723, loss -, compute time 12.66\n",
      "episode 4992, reward 1721, memory_length 2000, epsilon 0.01119, total time 721, loss -, compute time 12.8\n",
      "episode 4993, reward 1791, memory_length 2000, epsilon 0.01118, total time 726, loss -, compute time 12.23\n",
      "episode 4994, reward 1684, memory_length 2000, epsilon 0.01117, total time 721, loss -, compute time 13.0\n",
      "episode 4995, reward 1545, memory_length 2000, epsilon 0.01116, total time 729, loss -, compute time 15.25\n",
      "episode 4996, reward 1819, memory_length 2000, epsilon 0.01115, total time 721, loss -, compute time 12.78\n",
      "episode 4997, reward 1838, memory_length 2000, epsilon 0.01114, total time 729, loss -, compute time 13.73\n",
      "episode 4998, reward 1753, memory_length 2000, epsilon 0.01113, total time 723, loss -, compute time 13.06\n",
      "episode 4999, reward 1700, memory_length 2000, epsilon 0.01112, total time 722, loss -, compute time 11.91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(Episodes):\n",
    "    start_time = time.time() #Measuring compute time per episode\n",
    "    done = False  #Variable used to terminate an episode\n",
    "    total_time = 0 #Number of hours for which the rewards are measured out of 730 (episode length)\n",
    "    total_reward = 0 #Total reward per episode\n",
    "    env = CabDriver() #Creating the object for CabDriver class\n",
    "    action_space, state_space, state = env.reset() #Obtaining all possible actions, states and initial state\n",
    "    state_size = m+t+d #State size\n",
    "    action_size = len(action_space) #Action size\n",
    "     \n",
    "    #Run until the episode is not complete\n",
    "    while not done:\n",
    "        \n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        possible_actions_index, actions = env.requests(state)\n",
    "        action_idx, epsilon = agent.get_action(state, episode, possible_actions_index, actions)\n",
    "        # 2. Evaluate the reward and next state\n",
    "        action = action_space[action_idx] #Action from index\n",
    "        reward, time_spent = env.reward_and_time_spent_func(state, action, Time_matrix) #Reward as well as duration between current and drop time\n",
    "        next_state = env.next_state_func(state, action, Time_matrix)\n",
    "        total_time += time_spent #Updating total time\n",
    "        if total_time > episode_len:\n",
    "            done = True\n",
    "        else:\n",
    "            # 3. Append the experience to the memory\n",
    "            agent.append_sample(state, action_idx, reward, next_state, done)\n",
    "            # 4. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 5. Keep a track of rewards and making next state as current state for next time step\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    end_time = time.time()\n",
    "    comp_time = round(end_time-start_time,2) #Time per episode\n",
    "    #obtaining loss if any\n",
    "    if \"loss\" in agent.model.history.history.keys(): \n",
    "        loss = agent.model.history.history[\"loss\"]\n",
    "    else:\n",
    "        loss = \"-\"\n",
    "    # every episode:\n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, total time {4}, loss {5}, compute time {6}\".format(episode,total_reward, \n",
    "                                                                                                     len(agent.memory),\n",
    "                                                                                                     round(epsilon,5), total_time, \n",
    "                                                                                                     loss,\n",
    "                                                                                                     comp_time))\n",
    "    #every few episodes:\n",
    "    if episode % 10 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "        agent.save_tracking_states()\n",
    "        # save model weights\n",
    "        print(f\"Saving model for episode: {episode}\")\n",
    "        agent.save(name=\"model_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0, 0, 0), (1, 0, 0), (2, 0, 0), (3, 0, 0)])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_track.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(states_to_track[(0,0,0)]) #Tracked state\n",
    "plt.title(\"Q-value for state (0,0,0) per visit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards_per_episode)\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,5000)\n",
    "epsilon = []\n",
    "for i in range(0,5000):\n",
    "    epsilon.append(np.exp(-0.0009*i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
